{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://pydata.org/nyc2014/schedule/\" width=100% height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"http://pydata.org/nyc2014/schedule/\" width=100% height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "# past events\n",
    "# TODO: fetch this data from past events page\n",
    "conferences = ['nyc2014', 'berlin2014', 'sv2014', 'ldn2014', 'nyc2013']\n",
    "\n",
    "abstract_url = \"http://pydata.org/%s/abstracts/\"\n",
    "\n",
    "conf_data = {}\n",
    "\n",
    "# Collecting data about abstracts\n",
    "for conference in conferences:\n",
    "    raw = urllib2.urlopen(abstract_url % conference).read()\n",
    "    soup = BeautifulSoup(raw)\n",
    "    abstracts = [abstract.get_text() for abstract in soup.find_all(class_=\"accordion-inner\")]\n",
    "    conf_data[conference] = abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'berlin2014': [u\"\\n                    ABBY is a Django app that helps you manage your A/B tests. The main objective is to document all tests happening in your company, in order to better understand which measures work and which don't. Thereby leading to a better understanding of your product and your customer. \\r\\n\\r\\nABBY offers a front-end that makes it easy to edit, delete or create tests and to add evaluation results. Further, it provides a RESTful API to integrate directly with our platform to easily handle A/B tests without touching the front-end. Another notable feature is the possibility to upload a CSV file and have the A/B test auto-evaluated, although this feature is considered highly experimental.\\r\\n\\r\\nAt Jimdo, a do-it-yourself website builder, we have a team of about 180 people from different countries and with professional backgrounds just as diverse. Therefore it is crucial to have tools that allow having a common perspective on the tests. This facilitates having data informed discussions and to deduce effective solutions. In our opinion tools like ABBY are cornerstones to achieve the ultimate goal of being a data-driven company. It enables all our co-workers to review past and plan future tests to further improve our product and to raise the happiness of our customers.\\r\\n\\r\\nThe proposed talk will give a detailed overview of ABBY, which eventually will be open-sourced, and its capabilities. I will further discuss the motivation behind the app and the influence it has on the way our company is becoming increasingly data driven.\\n                \",\n",
       "  u'\\n                    Python is quickly becoming the glue language which holds together data science and related fields like quantitative finance. Zipline is a BSD-licensed quantitative trading system which allows easy backtesting of investment algorithms on historical data. The system is fundamentally event-driven and a close approximation of how live-trading systems operate. Moreover, Zipline comes \"batteries included\" as many common statistics like moving average and linear regression can be readily accessed from within a user-written algorithm. Input of historical data and output of performance statistics is based on Pandas DataFrames to integrate nicely into the existing Python eco-system. Furthermore, statistic and machine learning libraries like matplotlib, scipy, statsmodels, and sklearn integrate nicely to support development, analysis and visualization of state-of-the-art trading systems.\\r\\n\\r\\nZipline is currently used in production as the backtesting engine powering Quantopian.com -- a free, community-centered platform that allows development and real-time backtesting of trading algorithms in the web browser.\\n                ',\n",
       "  u'\\n',\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u'\\n                    Learn how to program and utilize the parallel computing power of the Graphics Processing Unit (GPU) using NVIDIA\\u2019s CUDA programming framework. We will pay particular attention to the new CUDA 6 features like Unified Memory, which simplifies memory management by automatically migrating data between the CPU and GPU, as well as the new cuBLAS-XT library for multi-GPU blas as well as NVBLAS for drop in replacement of blas libraries.\\r\\n\\r\\nYou will get insight into development of CUDA and how it will take advantage of current and future GPUs in Python libraries for data analysis.\\n                ',\n",
       "  u'\\n                    Our eyes naturally and instantly extract information from  complex images with a minimum of thought.  In this tutorial, we will get our feet wet with analysis of images using Scikit-Image.  Images are a great example of data sets, since they have thousands of pixels to work with at a time.  \\r\\n\\r\\nIn this tutorial, geared towards beginners, how Sci-Kit Image stores images and how to transform them in RGB space.  Afterwards, we use some matrix math and k-means clustering to extract the significant colors in the image.  In fact, we will write the k-means ourselves in just a few lines with NumPy.\\r\\n\\r\\nImage examples will come from my own photographs of graffiti in San Juan, Puerto Rico.  However, you may use your own images or download some from Instagram.  The result of this tutorial should be some rather pleasant color schemes which can be incorporated into art or your web site.\\n                ',\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u'\\n',\n",
       "  u'\\nConda is an open source package manager, which can be used to manage binary packages and virtual environments on any platform. It is the package manager of the Anaconda Python distribution, although it can be used independently of Anaconda. We will look at how conda solves many of the problems that have plagued Python packaging in the past, followed by a demonstration of its features.\\nWe will look at the issues that have plagued packaging in the Python ecosystem in the past, and discuss how Conda solves these problems. We will show how to use conda to manage multiple environments. Finally, we will look at how to build your own conda packages.\\n',\n",
       "  u\"\\n                    Computers have traditionally been thought as tools for performing computations with numbers.  Of course, its name in English has a lot to do with this conception, but in other languages, like the french 'ordinateur' (which express concepts more like sorting or classifying), one can clearly see the other side of the coin: computers can also be used to extract (usually new) information from data.\\n\\nStorage, reduction, classification, selection, sorting, grouping, among others, are typical operations in this 'alternate' goal of computers, and although carrying out all these tasks does imply doing a lot of computations, it also requires thinking about the computer as a different entity than the view offered by the traditional von Neumann architecture (basically a CPU with memory).  In fact, when it is about programming the data handling efficiently, the most interesting part of a computer is the so-called hierarchical storage, where the different levels of caches in CPUs, the RAM memory, the SSD layers (there are several in the market already), the mechanical disks and finally, the network, are pretty much more important than the ALUs (arithmetic and logical units) in CPUs.\\n\\nIn data handling, techniques like data deduplication and compression become critical when speaking about dealing with extremely large datasets.  Moreover, distributed environments are useful mainly because of its increased storage capacities and I/O bandwidth, rather than for their aggregated computing throughput.\\n\\nDuring my talk I will describe several programming paradigms that should be taken in account when programming data oriented applications and that are usually different than those required for achieving pure computational throughput.  But specially, and in a surprising turnaround, how the amazing amount of computational power in modern CPUs can also be useful for data handling as well.\\n                \",\n",
       "  u\"\\n                    Just because data is open doesn't mean it is accessible to the wider public. This tutorial will take government data, clean it, and visualize it. If time permits, participants will also have a chance to put their visualization into a flask application.\\r\\n\\r\\nOnly basic knowledge of Python is required for this tutorial. We will go over IPython, and use Pandas to process, and clean our data. If you are a beginner Pythonista interested in making sense of open data, here is your chance to use your programming skills to get involved in the field of civic hacking.\\n                \",\n",
       "  u\"\\n                    Just because data is open doesn't mean it is accessible to the wider public. This tutorial will take government data, clean it, and visualize it. If time permits, participants will also have a chance to put their visualization into a flask application.\\n\\nOnly basic knowledge of Python is required for this tutorial. We will go over IPython, and use Pandas to process, and clean our data. If you are a beginner Pythonista interested in making sense of open data, here is your chance to use your programming skills to get involved in the field of civic hacking.\\n                \",\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u'\\n                    People talk about a Moore\\'s Law for gene sequencing, a Moore\\'s Law for software, etc. This is talk is about *the* Moore\\'s Law, the bull that the other \"Laws\" ride; and how Python-powered ML helps drive it. How do we keep making ever-smaller devices? How do we harness atomic-scale physics? Large-scale machine learning is key. The computation drives new chip designs, and those new chip designs are used for new computations, ad infinitum. High-dimensional regression, classification, active learning, optimization, ranking, clustering, density estimation, scientific visualization, massively parallel processing -- it all comes into play, and Python is powering it all.  \\n                ',\n",
       "  u'\\n                    What questions arise during a quick model assessment? In this hands-on-tutorial we want to cover the whole chain from preparing data to choosing and fitting a model to properly assessing the quality of a predictive model. Our dataset in this tutorial are the numbers of people entering and exiting New York subway stations. Among other ways of building a predictive model, we introduce the python package pydse ( http://pydse.readthedocs.org/ ) and apply it to the dataset in order to derive the parameters of an ARMA-model (autoregressive moving average). At the end of the tutorial we evaluate the models and examine the strengths and weaknesses of various ways to measure the accuracy and quality of a predictive model.\\n                ',\n",
       "  u'\\n                    Experiences from building a recommendation engine for patent search\\r\\nusing pythonic NLP and topic modeling tools such as Gensim.\\r\\n\\n                ',\n",
       "  u'\\n                    mETL is an ETL package written in Python which was developed to load  elective data for Central European University. Program can be used in a more general way, it can be used to load practically any kind of data to any target. Code is open source and available for anyone who want to use it. The main advantage to configurable via Yaml files and You have the possibility to write any transformation in Python and You can use it natively from any framework as well. \\r\\n\\r\\nWe  are using this tool in production for many of our clients and It is really stable and reliable. The project has a few contributors all around the world right now and I hope many developer will join soon. \\r\\n\\r\\nI really want to show you how you can use it in your daily work. In this tutorial We will see the most common situations:\\r\\n\\r\\n- Installation\\r\\n- Write simple Yaml configration files to load CSV, JSON, XML into MySQL or PostgreSQL Database, or convert CSV to JSON, etc.\\r\\n- Add tranformations on your fields\\r\\n- Filter records based on condition\\r\\n- Walk through a directory to feed the tool\\r\\n- How the mapping works\\r\\n- Generate Yaml configurations automatically from data source\\r\\n- Migrate a database to another database\\r\\n\\n                ',\n",
       "  u'\\n                    mETL is an ETL package written in Python which was developed to load elective data for Central European University. Program can be used in a more general way, it can be used to load practically any kind of data to any target. Code is open source and available for anyone who want to use it. The main advantage to configurable via Yaml files and You have the possibility to write any transformation in Python and You can use it natively from any framework as well.\\n\\nWe are using this tool in production for many of our clients and It is really stable and reliable. The project has a few contributors all around the world right now and I hope many developer will join soon.\\n\\nI really want to show you how you can use it in your daily work. In this tutorial We will see the most common situations:\\n\\n- Installation\\n- Write simple Yaml configration files to load CSV, JSON, XML into MySQL or PostgreSQL Database, or convert CSV to JSON, etc.\\n- Add tranformations on your fields\\n- Filter records based on condition\\n- Walk through a directory to feed the tool\\n- How the mapping works\\n- Generate Yaml configurations automatically from data source\\n- Migrate a database to another database\\n                ',\n",
       "  u\"\\n                    Bloscpack [1] is a reference implementation and file-format for fast serialization\\r\\nof numerical data. It features lightweight, chunked and compressed storage,\\r\\nbased on the extremely fast Blosc [2] metacodec and supports serialization of\\r\\nNumpy arrays out-of-the-box. Recently, Blosc -- being the metacodec that it is\\r\\n-- has received support for using the popular and widely used Snappy [3], LZ4\\r\\n[4], and ZLib [5] codecs, and so, now Bloscpack supports serializing Numpy arrays\\r\\neasily with those codecs!\\r\\n\\r\\nIn this talk I will present recent benchmarks of Bloscpack performance on a\\r\\nvariety of artificial and real-world datasets with a special focus on the newly\\r\\navailable codecs. In these benchmarks I will compare Bloscpack, both\\r\\nperformance and usability wise, to alternatives such as Numpy's native offerings\\r\\n(NPZ and NPY), HDF5/PyTables [6], and if time permits, to novel bleeding edge\\r\\nsolutions.\\r\\n\\r\\nLastly I will argue that compressed and chunked storage format such as\\r\\nBloscpack can be and somewhat already is a useful substrate on which to build more\\r\\npowerful applications such as online analytical processing engines and\\r\\ndistributed computing frameworks.\\r\\n\\r\\n[1]: https://github.com/Blosc/bloscpack\\r\\n[2]: https://github.com/Blosc/c-blosc/\\r\\n[3]: http://code.google.com/p/snappy/\\r\\n[4]: http://code.google.com/p/lz4/\\r\\n[5]: http://www.zlib.net/\\r\\n[6]: http://www.pytables.org/moin\\r\\n\\n                \",\n",
       "  u\"\\n                    Lessons from translating Google's deep learning algorithm into Python. Can a Python port compete with Google's tightly optimized C code?\\r\\n\\r\\nSpoiler: making use of Python and its vibrant ecosystem (generators, NumPy, Cython...), the optimized Python port is cleaner, more readable and clocks in\\u2014somewhat astonishingly\\u20144x faster than Google's C. This is 12,000x faster than a naive, pure Python implementation and 100x faster than an optimized NumPy implementation.\\r\\n\\r\\nThe talk will go over what went well (data streaming to process humongous datasets, parallelization and avoiding GIL with Cython, plugging into BLAS) as well as trouble along the way (BLAS idiosyncrasies, Cython issues, dead ends).\\r\\n\\r\\nThe quest is also documented on my blog.\\n                \",\n",
       "  u\"\\n                    What are generators and coroutines in Python? What additional conceptualisations do they offer, and how can we use them to better model problems? This is a talk I've given at PyCon Canada, PyData Boston, and PyTexas. It's an intermediate-level talk around the core concept of generators with a lot of examples of not only neat things you can do with generators but also new ways to model and conceptualise problems.\\n                \",\n",
       "  u'\\n',\n",
       "  u'\\n                    This talk will walk through what the US government has done in terms of spying on US citizens and foreigners with their PRISM program, then walk through how to do exactly that with Python.\\n                ',\n",
       "  u'\\n                    The best filter algorithm to fuse multiple sensor informations is the Kalman filter. To implement it for non-linear dynamic models (e.g. a car), analytic calculations for the matrices are necessary. In this talk, one can see, how the IPython Notebook and Sympy helps to develop an optimal filter to fuse sensor information from different sources (e.g. acceleration, speed and GPS position) to get an optimal estimate.\\r\\n\\r\\nmore: http://balzer82.github.io/Kalman/\\n                ',\n",
       "  u\"\\n                    Blender for Scientific Visualization\\r\\n\\r\\nSometimes, static plots of data aren't enough. Sometimes you need animations of data for presentations or\\r\\nvideos. There are several methods to create movie clips from single frame images, but there is plenty more\\r\\nyou can do with the right tools.\\r\\n\\r\\nBlender is an Open Source software package which can serve as a swiss army knife in computer graphics. \\r\\nUsing examples from finance, biology and physics I will demonstrate how you can use Blender\\r\\nto improve your visualizations and presentations.\\r\\n\\r\\nWith blender you can turn rendered graphs into video presentations of your data. We can move around\\r\\n2d animations, we can use motion blur and variable speed, pan, zoom, and much more.\\r\\nOn top of that, Blender has a fully fledged 3D animation engine and lets you use any 3D model or 2D\\r\\nimage to improve the look of the video or to make your point more convincingly.\\r\\n\\r\\nFinally, Blender can edit video sequences, achieve special effects with the node-based compositor,\\r\\nand render the final result for upload to your favourite streaming platform.\\r\\n\\r\\nAnd to supersede this awesomeness: It can be scripted with Python!\\r\\n\\n                \",\n",
       "  u'\\n',\n",
       "  u'\\n                    Bokeh is a Python interactive visualization library for large datasets that natively uses the latest web technologies. Its goal is to provide elegant, concise construction of novel graphics in the style of Protovis/D3, while delivering high-performance interactivity over large data to thin clients. This tutorial will walk users through the steps to create different kinds of interactive plots using Bokeh. We will cover using Bokeh for static HTML output, the IPython notebook, and plot hosting and embedding using the Bokeh server.\\n                ',\n",
       "  u'\\n                    We will give an introduction to the recent development of Deep Neural Networks and focus in particular on Convolution Networks which are well suited to image classification problems. We will also provide you with the practical knowledge of how to get started with using ConvNets via the cuda-convnet python library.\\n                ',\n",
       "  u'\\n                    The talk aims to provide a practical introduction to natural language processing (NLP) for a working programmer. The talk assumes no prior exposure to NLP, and will cover sufficient detail so that you know what modern NLP has to offer, and where to look for more details when you need it. We will cover the following topics:\\r\\n\\r\\n-Language modelling and vector space representation \\r\\n-Text classification and named entity recognition\\r\\n-Part-of-speech tagging\\r\\n-Question answering and text summarization\\r\\n\\r\\nThe overarching goal of the talk is that you walk away with a mental framework that allows you to systematically think about problems in natural language processing.\\r\\n\\r\\nNote: I gave a similar introductory talk on machine learning at EuroPython 2013, and i got excellent feedback on it. Some people told me that it was the best talk they heard at the conference: http://www.youtube.com/watch?v=n-_o5Vd9ceM\\r\\n\\r\\n\\n                ',\n",
       "  u'\\nThis talk will give a basic introduction to the pySPACE framework and its current applications.\\npySPACE (Signal Processing And Classification Environment) is a modular software for the processing of large data streams that has been specifically designed to enable distributed execution and empirical evaluation of signal processing chains. Various signal processing algorithms (so called nodes) are available within the software, from finite impulse response filters over data-dependent spatial filters (e.g., PCA, CSP, xDAWN) to established classifiers (e.g., SVM, LDA). pySPACE incorporates the concept of node and node chains of the Modular Toolkit for Data Processing (MDP) framework. Due to its modular architecture, the software can easily be extended with new processing nodes and more general operations. Large scale empirical investigations can be configured using simple text-configuration files in the YAML format, executed on different (distributed) computing modalities, and evaluated using an interactive graphical user interface.\\npySPACE allows the user to connect nodes modularly and automatically benchmark the respective chains for different parameter settings and compare these with other node chains, e.g., by automatic evaluation of classification performances provided within the software. In addition, the pySPACElive mode of execution can be used for online processing of streamed data. The software specifically supports but is not limited to EEG data. Any kind of time series or feature vector data can be processed and analyzed.\\npySPACE additionally provides interfaces to specialized signal processing libraries such as SciPy, scikit-learn, LIBSVM, the WEKA Machine Learning Framework, and the Maja Machine Learning Framework (MMLF).\\nWeb page: http://pyspace.github.io/pyspace/\\n',\n",
       "  u'\\n                    Low-rank approximations of data matrices have become an important tool in machine learning and data mining. They allow for embedding high dimensional data in lower dimensional spaces and can therefore mitigate effects due to noise, uncover latent relations, or facilitate further processing. These properties have been proven successful in many application areas such as bio-informatics, computer vision, text processing, recommender systems, social network analysis, among others. Present day technologies are characterized by exponentially growing amounts of data. Recent advances in sensor technology, internet applications, and communication networks call for methods that scale to very large and/or growing data matrices. In this talk, we will describe how to efficiently analyze data by means of matrix factorization using the Python Matrix Factorization Toolbox (PyMF) and HDF5. We will briefly cover common methods such as k-means clustering, PCA, or Archetypal Analysis which can be easily cast as a matrix decomposition, and explain their usefulness for everyday data analysis tasks.\\r\\n\\n                ',\n",
       "  u'\\n                    In this talk I would like to show you a few real-life use-cases where\\r\\nElasticsearch can help you make sense of your data. We will start with the most\\r\\nbasic use case of searching your unstructured data and move on to more advanced\\r\\ntopics such as faceting, aggregations and structured search.\\r\\n\\r\\nI would like to demonstrate that the very same tool and dataset can be used for\\r\\nreal-time analytics as well as the basis for your more advanced data processing\\r\\njobs. All in a distributed environment capable of handling terabyte-sized\\r\\ndatasets.\\r\\n\\r\\nAll examples will be shown with real data and python code demoing the new\\r\\nlibraries we have been working on to make this process easier.\\r\\n\\r\\n\\n                ',\n",
       "  u\"\\n                    This talk will be about my latest project in mall analytics, where we estimated visitor trends in malls around the globe using telco data as a basis, and employed map reduce technologies and data science to extrapolate from this basis to  reality and correct for biases.  We succeeded in extracting valuable information such as count of visitors per hour, demographics breakdown, competitor analysis and popularity of the mall among different parts of the surrounding areas, all the while preserving user privacy and working only with aggregated data. I will show an overview of our system's modules, how we got a first raw estimation of the visitors and their behaviours, and how we refined and evaluated this estimation using pandas, matplotlib, scikit-learn and other python libraries.\\n                \",\n",
       "  u\"\\n                    In 2004, at the Sixth Symposium on Operating System Design and Implementation, Jeffrey Dean and Sanjay Ghemawat, a couple of engineers working for Google, published a paper titled \\u201cMapReduce: Simplified Data Processing on Large Clusters\\u201d that introduced the world to a simple, yet powerful heuristic for processing large amounts of data at previously unheard of scales. Though the concepts were not new---map and reduce had existed for quite some time in functional programming languages---the observation that they could be used as a general programming paradigm for solving large data processing problems changed the current state of the art.\\r\\n\\r\\nIf you find yourself working with data nowadays, you\\u2019re bound to find yourself at some point with a need to process \\u201cBig Data\\u201d. Big Data can be a troublesome phrase, arguably more hype than anything at this point, it has many different meanings, but for the purpose of this tutorial we\\u2019ll consider it to be any data that is too large to fit into the main memory of a single machine. With that in mind, if you\\u2019ve ever found yourself with the need to process an amount of data that stretched the boundaries of your own personal laptop, and you wanted to apply the ideas expressed in Dean's and Ghemawat's seminal paper but had no idea what to do, or even where to start, then this tutorial is for you.\\r\\n\\r\\nThe goal of the tutorial is to give attendees a basic working knowledge of what MapReduce is, and how it can be used to process massive sets of data relatively quickly. We will walk through the basics of what MapReduce is and how it works. Though there are a handful of MapReduce implementations out there to choose from, Hadoop is without a doubt the most well known and, as such, we will take a look at how to use it to run our MapReduce jobs. With that in mind, we will discuss what you need to know to use Hadoop and take a look at how to write our own Hadoop jobs in Python using the Hadoop Streaming utility. Finally, we\\u2019ll look at a library created at Yelp called MRJob that can make writing Hadoop jobs in Python much easier. By the end of the tutorial an attendee with little to no knowledge of MapReduce, but a working knowledge of Python, should be able to write their own basic MapReduce tasks for Hadoop and run them on a cluster of machines using Amazon\\u2019s Elastic MapReduce service.\\r\\n\\n                \",\n",
       "  u\"\\n                    In 2004, at the Sixth Symposium on Operating System Design and Implementation, Jeffrey Dean and Sanjay Ghemawat, a couple of engineers working for Google, published a paper titled \\u201cMapReduce: Simplified Data Processing on Large Clusters\\u201d that introduced the world to a simple, yet powerful heuristic for processing large amounts of data at previously unheard of scales. Though the concepts were not new---map and reduce had existed for quite some time in functional programming languages---the observation that they could be used as a general programming paradigm for solving large data processing problems changed the current state of the art.\\r\\n\\r\\nIf you find yourself working with data nowadays, you\\u2019re bound to find yourself at some point with a need to process \\u201cBig Data\\u201d. Big Data can be a troublesome phrase, arguably more hype than anything at this point, it has many different meanings, but for the purpose of this tutorial we\\u2019ll consider it to be any data that is too large to fit into the main memory of a single machine. With that in mind, if you\\u2019ve ever found yourself with the need to process an amount of data that stretched the boundaries of your own personal laptop, and you wanted to apply the ideas expressed in Dean's and Ghemawat's seminal paper but had no idea what to do, or even where to start, then this tutorial is for you.\\r\\n\\r\\nThe goal of the tutorial is to give attendees a basic working knowledge of what MapReduce is, and how it can be used to process massive sets of data relatively quickly. We will walk through the basics of what MapReduce is and how it works. Though there are a handful of MapReduce implementations out there to choose from, Hadoop is without a doubt the most well known and, as such, we will take a look at how to use it to run our MapReduce jobs. With that in mind, we will discuss what you need to know to use Hadoop and take a look at how to write our own Hadoop jobs in Python using the Hadoop Streaming utility. Finally, we\\u2019ll look at a library created at Yelp called MRJob that can make writing Hadoop jobs in Python much easier. By the end of the tutorial an attendee with little to no knowledge of MapReduce, but a working knowledge of Python, should be able to write their own basic MapReduce tasks for Hadoop and run them on a cluster of machines using Amazon\\u2019s Elastic MapReduce service.\\r\\n\\n                \",\n",
       "  u'\\n                    The Python data ecosystem has grown beyond the confines of single machines to embrace scalability. Here we describe one of our approaches to scaling, which is already being used in production systems. The goal of in-database analytics is to bring the calculations to the data, reducing transport costs and I/O bottlenecks. Using PL/Python we can run parallel queries across terabytes of data using not only pure SQL but also familiar PyData packages such as scikit-learn and nltk. This approach can also be used with PL/R to make use of a wide variety of R packages. We look at examples on Postgres compatible systems such as the Greenplum Database and on Hadoop through Pivotal HAWQ. We will also introduce MADlib, Pivotal\\u2019s open source library for scalable in-database machine learning, which uses Python to glue SQL queries to low level C++ functions and is also usable through the PyMADlib package.\\n                ',\n",
       "  u'\\nIn the course of the 2008 Lehman and the subsequent European debt crisis, it became clear that both industry and regulators had underestimated the degree of interconnectedness and interdependency across financial assets and institutions. This type of information is especially well represented by network models, which had first gained popularity in other areas, such as computer science, biology and social sciences.\\nAlthough in its early stages, the study of network models in finance is gaining momentum and could be key to building the next generation of risk management tools and averting future financial crises. After a short overview of some of the most relevant work in the field, I will walk through (real data) examples using the pydata toolset.\\n',\n",
       "  u'\\n',\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u'\\n                    Lawyers are not famed for their mathematical ability.  On the contary - the law almost self-selects as a career choice for the numerically-challenged.\\r\\n\\r\\nSo when the one UK tax that property lawyers generally felt comfortable dealing with (lease duty) was replaced with a  new tax (stamp duty land tax) that was both arithmetically demanding and conceptually complex, it was inevitable that significant frustrations would arise.  Suddenly, lawyers had to deal with concepts such as net present valuations, aggregation of several streams of fluctuating figures, and constant integration of a complex suite of credits and disregards.\\r\\n\\r\\nThis talk is a description of how - against a backdrop of data-drunk tax authorities, legal pressures on businesses to have appropriate compliance systems in place, and the constant pressure on their law firms to commoditise compliance services, Pandas may be about to make a foray from its venerable financial origins into a brave new fiscal world - and can revolutionise an industry by doing so.\\r\\n \\r\\nA case study covering the author\\'s development of a Pandas-based stamp duty land tax engine (\"ORVILLE\") is discussed, and the inherent usefulness of Pandas in the world of tax analysis is explored.\\n                ',\n",
       "  u'\\n                    When talking of parallel processing, some task requires a substantial set-up time. This is the case of Natural Language Processing (NLP) tasks such as classification, where models need to be loaded into memory. In these situations, we can not start a new process for every data set to be handled, but the system needs to be ready to process new incoming data. \\r\\nThis talk will look at job queue systems, with particular focus on gearman. We will see how we are using it at Synthesio for NLP tasks; how to set up workers and clients, make it redundant and robust, monitor its activity and adapt to demand.\\n                ',\n",
       "  u'\\n                    The Python community has developed a powerful and convenient data analysis software stack. But although the PyData tools and libraries become more and more popular, they are often still associated with small to medium-scale data analysis. On the other side \"Big Data\" is sometimes treated as a Trademark-feature of Hadoop & Co.\\r\\n\\r\\nFortunately, another strength of the Python community is its openness and pragmatic culture which not only entails many excellent tools written in Python with rich interfaces to software without Python-core but more general makes Python a popular glue language. Not only, but particularly for data analysis.\\r\\n\\r\\nIn this spirit we show how to develop data-heavy software in Python while using cluster-computing technology around the Hadoop ecosystem in the backend. Particularly, we focus on recent frameworks such as Spark or Stratosphere and interactive data analysis use cases.\\n                ',\n",
       "  u'\\n                    For data, and data science, to be the fuel of the 21th century, data driven\\r\\napplications should not be confined to dashboards and static analyses.  Instead\\r\\nthey should be the driver of the organizations that own or generates the data.\\r\\nMost of these applications are web-based and require real-time access to the\\r\\ndata. However, many Big Data analyses and tools are inherently batch-driven and\\r\\nnot well suited for real-time and performance-critical connections with\\r\\napplications. Trade-offs become often inevitable, especially when mixing\\r\\nmultiple tools and data sources.  \\xa0\\r\\n\\r\\nIn this talk we will describe our journey to build a data driven application at\\r\\na large Dutch financial institution. We will dive into the issues we faced, why\\r\\nwe chose Python and pandas and what that meant for real-time data analysis (and\\r\\nagile development).  \\xa0\\r\\n\\r\\nImportant points in the talk will be, among others, the handling of\\r\\ngeographical data, the access to hundreds of millions of records as well as the\\r\\nreal time analysis of millions of data points.\\r\\n\\n                ',\n",
       "  u\"\\n                    Applications for self tracking that collect, analyze, or publish personal and medical data are getting more popular. This includes either a broad variety of medical and healthcare apps in the fields of telemedicine, remote care, treatment, or interaction with patients, and a huge increasing number of self tracking apps that aims to acquire data form from people\\u2019s daily life. The Quantified Self movement goes far beyond collecting or generating medical data. It aims in gathering data of all kinds of activities, habits, or relations that could help to understand and improve one\\u2019s behavior, health, or well-being. \\r\\n\\r\\nBoth, health apps as well as Quantified Self apps use either just the smartphone as data source (e.g., questionnaires, manual data input, smartphone sensors) or external devices and sensors such as \\u2018classical\\u2019 medical devices (e.g,. blood pressure meters) or wearable devices (e.g., wristbands or eye glasses). The data can be used to get insights into the medical condition or one\\u2019s personal life and behavior. \\r\\n\\r\\nThis talk will provide an overview of the various data sources and data formats that are relevant for self tracking as well as strategies and examples for analyzing that data with Python. The talk will cover:  \\r\\n\\nAccessing local and distributed sources for the heterogeneous Quantified Self data. That includes local data files generated by smartphone apps and web applications as well as data stored on cloud resources via APIs (e.g., data that is stored by vendors of self tracking hardware or data of social media channels, weather data, traffic data etc.)\\nHomogenizing the data. Especially, covering typical problems of heterogeneous Quantified Self data, such as missing data or different and non-standard data formatting.\\nAnalyzing and visualizing the data. Depending on the questions one has, the data can be analyzed with statistical methods or correlations. For example, to get insight into one's personal physical activities, steps data form activity trackers can be correlated to location data and weather information. The talk covers how to conduct this and other data analysis tasks with tools such as pandas and how to visualize the results.\\n\\r\\n\\r\\nThe examples in this talk will be shown as interactive IPython sessions.\\r\\n\\n                \",\n",
       "  u'\\nTim Berners-Lee defined the \\r\\nSemantic Web as a web of data that can be processed directly and \\r\\nindirectly by machines.\\nMore precisely, the Semantic Web can be defined as a set of standards and \\r\\nbest practices for sharing data and the semantics of that data over the Web to \\r\\nbe used by applications [DuCharme, 2013].\\nIn particular, the Semantic Web is built on top of three main pillars: \\r\\nthe RDF (i.e., Resource Description Framework) data model, the \\r\\nSPARQL query language, and the OWL standard for storing vocabularies and\\r\\nontologies.\\r\\nThese standards allows the huge amount of data on the Web to be available in a \\r\\nunique and unified standard format, contributing to the definition of the \\r\\nWeb of Data (WoD) [1].\\nThe WoD makes the web data to be reachable and easily manageable by Semantic \\r\\nWeb tools, providing also the relationships among these data (thus practically \\r\\nsetting up the \\u201cWeb\\u201d). This collection of interrelated datasets on the Web can \\r\\nalso be referred to as Linked Data [1].\\nTwo typical examples of large Linked Dataset are \\r\\nFreeBase, and DBPedia, \\r\\nwhich essentially provides the so called Common sense Knowledge in \\r\\nRDF format. \\nPython offers a very powerful and easy to use library to work with \\r\\nLinked Data: rdflib.\\nRDFLib is a lightweight and functionally complete RDF library, allowing\\r\\napplications to access, create and manage RDF graphs in a very Pythonic \\r\\nfashion.\\nIn this talk, a general overview of the main features provided by the \\r\\nrdflib package will be presented.\\r\\nTo this end, several code examples will be discussed, along with a case study \\r\\nconcerning the analysis of a (semantic) social graph.\\r\\nThis case study will be focused on the integration between the \\r\\nnetworkx module and the rdflib library \\r\\nin order to crawl, access (via SPARQL), and analyze a \\r\\nSocial Linked Data Graph represented using the FOAF (Friend of a Friend) \\r\\nschema.\\nThis talk is intended for an Novice level audience, assuming a \\r\\ngood knowledge of the Python language. \\n',\n",
       "  u\"\\n                    Speed without drag: making code faster when there's no time to waste\\r\\n\\r\\nA practical walkthrough over the state-of-the-art of low-friction numerical Python enhancing solutions, covering: exhausting CPython, NumPy, Numba, Parakeet, Cython, Theano, Pyston, PyPy/NumPyPy and Blaze.\\n                \",\n",
       "  u'\\n                    This talk presents a very hands-on approach for identifying research and technology trends in various industries with a little bit of Pandas here, NTLK there and all cooked up in an IPython Notebook. Three examples featured in this talk are:\\r\\n\\nHow to find out the most interesting research topics cutting edge companies are after right now?\\r\\nHow to pick sessions from a large conference program (think PyCon, PyData or Strata) that are presenting something really novel?\\r\\nHow to automagically identify trends in industries such as computer vision or telecommunications?\\r\\n\\r\\nThe talk will show how to tackle common tasks in applied trend research and technology foresight from identifying a data-source, getting the data and data cleaning to presenting the insights in meaningful visualizations.\\n                ',\n",
       "  u\"\\n                    This tutorial will introduce how to use the popular data science library Pandas on PySpark to enable solving big data tasks with pandas. Spark is a top level Apache project for lightning fast large-scale data processing.\\r\\n\\r\\nThe basic unit of data in Spark is an RDD (resilient distributed data set), which has a simple functional API. By having panda frames stored inside a Spark RDD and using its basic API we perform many parallel operations, but the  resulting syntax, rdd.map(lambda x: x.map(lambda y: z)), leaves something to be desired.\\r\\n\\r\\nWe will extend the basic Spark RDD to be aware of the underlying Panda data frames. Using this extended RDD we will examine how to implement simple operations on panda data frames.\\r\\n\\r\\nFrom there we will look at how to implement some of the panda 2-d operations over an RDD of 1-d panda data frames and incorporate this in our extended RDD.\\r\\n\\r\\nWe will cover how to load data effectively into Pandas using both SparkSQL and using Spark's file load mechanism combined with Pandas native parsing. This gives us the ability to load data from CSV files, paquet files, and from Apache Hive.\\n                \",\n",
       "  u'\\n                    Python has been taking over R and SPSS in the last couple of years. Python has many tools to offer like \\u200bNumPy and  scikit-learn  that make any data scientist happy but pandas has been the main reason for many analysts to switch. It is fast, flexible, simple to learn, well documented, has a substantial community and has been accepted in many businesses as an every day tool.\\r\\n\\r\\nOne reason why people still prefer R is the ggplotmodule. It offers a non-verbose yet flexible way to visualise information. In this session we will show you that a proper workflow with ipython-notebook/pandas still allows the use of this library ggplot. With this workflow you have the best of both worlds with no compromises.\\r\\n\\r\\nAt the end of this session you have been show how to load data, aggregate data and analyse data with pandas and how to then visualise it with ggplot.\\n                ',\n",
       "  u'\\n                    This will be an interactive tutorial using IPython notebook, where explanatory slides are interleaved with time for data exploration and trying out.\\r\\nWe will start from basics of data loading and preparation with pandas. Then we will discuss basics of machine learning, followed by algorithms for visualization and supervised learning. In the end, we will talk about model selection and the importance of the bias variance tradeoff. Depending on the audience we will finish with an end-to-end process for a text classification task. The goal of the tutorial is that the participants have a good idea how to attack a machine learning task, and what kind of tools are offered by scikit-learn to help.\\n                '],\n",
       " 'ldn2014': [u'\\n                    The Python data ecosystem has grown beyond the confines of single machines to embrace scalability. Here we describe one of our approaches to scaling, which is already being used in production systems. The goal of in-database analytics is to bring the calculations to the data, reducing transport costs and I/O bottlenecks. Using PL/Python we can run parallel queries across terabytes of data using not only pure SQL but also familiar PyData packages such as scikit-learn and nltk. This approach can also be used with PL/R to make use of a wide variety of R packages. We look at examples on Postgres compatible systems such as the Greenplum Database and on Hadoop through Pivotal HAWQ. We will also introduce MADlib, Pivotal\\u2019s open source library for scalable in-database machine learning, which uses Python to glue SQL queries to low level C++ functions and is also usable through the PyMADlib package.\\n                ',\n",
       "  u\"\\n                    In this talk I will give an overview of Random Forests and will show their versatility when attempting to predict song ratings using the EMI music data set. I will present results using the randomForest library in R and the scikit learn implementation in Python and discuss their performance. I will also comment on their ease of use from a beginner's point of view.\\r\\n\\n                \",\n",
       "  u'\\n                    Clustering data is a fundamental technique in data mining and machine learning. The basic problem can be specified as follows: \\u201cGiven a set of data, partition the data into a set of groups so that each member of a given group is as similar as possible to the other members of that group and as dissimilar as possible to members of other groups\\u201d. In this talk I will try to unpack some of the complexities inherent in this seemingly straightforward description. Specifically, I will discuss some of the issues involved in measuring similarity and try to provide some intuitions into the decisions that need to be made when using such metrics to cluster data.\\n                ',\n",
       "  u\"\\n                    At Conversocial we use machine learning to filter through our customer's social data. We get the relevant customer service content to them, minimizing their response time and limiting the number of agents needing to sift through all of their tweets, facebook and google plus data. This is a story about how to tackle this problem using NLP and Python. What works well and what doesn't. This talk will cover both the machine learning models and the infrastructure used to scale the approach in production. \\r\\n\\r\\n\\n                \",\n",
       "  u'\\n                    At WIDE IO, we are specialists in image processing and video analytics; we have individual experience using Python, Numpy and Scipy for Computer Vision applications since 2007. Now, the environment has become much mature. Our goal with this talk is to share our enthusiasm and to present the basic steps required to perform image and video pattern analysis with Python. In our tutorial, we\\u2019ll investigate how to build an action recognition framework and how to do video-tracking with traditional vision models based on a bag-of-keypoints.  By going through examples, we\\u2019ll discuss how in practice computer vision for real-applications involve a trade-off between esthetical theories and utilitarianism. We will explore the various tricks that allow engineers to boost global performances, methods for running experiments and a mechanism for how to prepare the data... All these points are just a nice pretext to discuss our favorite tools: Numpy and Scipy of course, but also more exotic ones such as MediaLovinToolkit, PyCUDA, Bob and PyCVF... At the end of the talk, we\\u2019ll conclude by briefly discussing future imperatives, especially with respect to mobility and cloud computing.\\n                ',\n",
       "  u'\\n                    The aim of this talk is to demonstrate methods and applications of Authorship Attribution and related Forensic Linguistics  techniques, using Python.\\r\\n \\r\\nhttp://en.wikipedia.org/wiki/Stylometry\\r\\nhttp://en.wikipedia.org/wiki/Forensic_linguistics#Author_identification\\n                ',\n",
       "  u\"\\n                    Blosc is a high performance compressor optimized for binary data. It has been designed to transmit data to the processor cache faster than the traditional, non-compressed, direct memory fetch approach via a memcpy() OS call. Blosc is the first compressor (that I'm aware of) that is meant not only to reduce the size of large datasets on-disk or in-memory, but also to accelerate memory-bound computations (which is typical in vector-vector operations).\\r\\n\\r\\nIt uses the blocking technique (as described in this talk: http://www.pytables.org/docs/StarvingCPUs.pdf) to reduce activity on the memory bus as much as possible. In short, the blocking technique works by dividing datasets in blocks that are small enough to fit in L1 cache of modern processor and perform compression/decompression there. It also leverages SIMD and multi-threading capabilities present in nowadays multi-core processors so as to accelerate the compression/decompression process to a maximum.\\r\\n\\r\\nRecently, in Blosc 1.3, support for different ultra-fast compressors like LZ4 and Snappy have been added, as well as LZ4HC and Zlib, which are more meant for achieving high compression ratios.  This adds a lot of flexibility to Blosc, so that it can be used in more scenarios.\\r\\n\\r\\nDuring my talk I will explain why I created Blosc, and its ultimate goal: reducing memory I/O and hence, making computations more efficient.\\n                \",\n",
       "  u'\\n                    Bokeh is a Python interactive visualization library for large datasets that natively uses the latest web technologies. Its goal is to provide elegant, concise construction of novel graphics in the style of Protovis/D3, while delivering high-performance interactivity over large data to thin clients. This tutorial will walk users through the steps to create different kinds of interactive plots using Bokeh. We will cover using Bokeh for static HTML output, the IPython notebook, and plot hosting and embedding using the Bokeh server.\\n                ',\n",
       "  u'\\nAs a penniless academic I wanted to do \"big data\" for science. Open\\r\\nsource, Python, and simple patterns were the way forward. Staying on top\\r\\nof todays growing datasets is an arm race. Data analytics machinery\\r\\n\\u2014clusters, NOSQL, visualization, Hadoop, machine learning, ...\\u2014 can\\r\\nspread a team\\'s resources thin. Focusing on simple patterns, lightweight\\r\\ntechnologies, and a good understanding of the applications gets us most\\r\\nof the way for a fraction of the cost.\\nI will present a personal perspective on ten years of scientific data\\r\\nprocessing with Python. What are the emerging patterns in data\\r\\nprocessing? How can modern data-mining ideas be used without a big\\r\\nengineering team? What constraints and design trade-offs govern software\\r\\nprojects like scikit-learn, Mayavi, or joblib? How can we make the most\\r\\nout of distributed hardware with simple framework-less code?\\n',\n",
       "  u'\\n',\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u'\\nDerivatives analytics is one of the most compute and data intensive areas in the financial industry. This mainly stems from the fact, that Monte Carlo simulation techniques have to be applied in general to value and risk manage single derivatives trades and whole books of derivatives.\\nDX Analytics is a derivatives analytics library that is completely build in Python and that has a rather Pythonic API. It allows the modeling and valuation of both single- and multi-risk factor derivatives with European and American exercise. It also allows the consistent valuation of complex portfolios of such derivatives, e.g. incorporating the correlation between single risk factors.\\nThe talk provides some theoretical and technical background information, discusses the basic architectural features of DX Analytics and illustrates its use by a number of simple and more complex examples.\\n',\n",
       "  u'\\n\"Much of what we want to do with data involves optimization: whether\\r\\nit\\'s to find a model that best fits the data, or to decide on the\\r\\noptimal action given some information.\\nWe\\'ll explore the embarrassment of riches Python offers to tackle custom\\r\\noptimization problems: the scipy.optimize package, Sympy for calculus\\r\\nand code generation, Cython for speedups and binding to external\\r\\nlibraries.\"\\n',\n",
       "  u'\\n',\n",
       "  u\"\\n                    There are now a wide variety of data visualisation libraries available for use to make your data beautiful. I'll do a review of the different options available, focusing on 3 that are specifically Python based (Matplotlib, Seaborn, and Bokeh) and one javascript library that is the reigning champion of dataviz for the web (d3.js). I'll highlight how each of them works, when they might be appropriate, and generally how to use them effectively.\\n                \",\n",
       "  u\"\\nNumpy and pandas are the corner stones of data analysis in python. They allow for efficient data access and manipulation. Yet, they are not always appropriate for more heterogeneous data usage, when access patterns are hard to predict, or when you need to support write parallelism. This is an area where traditional databases systems still shine compared to the traditional data scientist toolset.\\nThe goal of this tutorial is to give you an idea of how databases can help you dealing with data which are not just numerical, with minimal effort or knowledge. We will focus on Postgresql, an open source database that have powerful extensions to deal with heterogeneous (aka 'schemaless') data, while being simple to use from python.\\n\",\n",
       "  u'\\n                    How can Python be embedded into other applications (C/C++)? What bearing does this have on our conceptualisation of systems written in Python? This talk covers the very-high-level embedding and the pure-embedding and also includes two novel embeddings: a zero-interpreter embedding using Cython and a Python interpreter embedded as a extension model within another Python interpreter (the \"Xzibit\" embedding.) This is a talk I\\'ve given at PyData Boston and PyTexas. It\\'s an advanced level talk around the ways we can embed Python into other (C/C++) applications. It includes two fairly novel results of my own research.\\n                ',\n",
       "  u'\\nThis tutorial is free, but requires separate registration.\\nToday\\'s financial market environment demands for ever shorter times-to-insight when it comes to financial analytics tasks. For the analysis of financial times series or for typical tasks related to derivatives analytics and trading, Python has developed to the ideal technology platform.\\nNot only that Python provides powerful and efficient libraries for data analytics, such as NumPy or pandas. With IPython there is a tool and environment available that facilitates interactive, and even real-time, financial analytics tremendously.\\nThe tutorial introduces into IPython and shows, mainly on the basis of practical examples related to the VSTOXX volatility index, how Python and IPython might re-define interactive financial analytics.\\nQuants, traders, financial engineers, analysts, financial researchers, model validators and the like all benefit from the tutorial and the new technologies provided by the Python ecosystem.\\nBACKGROUND\\nFor background information see the Python-based \"VSTOXX Advanced Services\" and the related backtesting applications:\\nhttp://www.eurexchange.com/vstoxx/\\nhttp://www.eurexchange.com/vstoxx/app1/\\nhttp://www.eurexchange.com/vstoxx/app2/\\nTECHNICAL REQUIREMENTS\\nTo follow the tutorial, you should have installed the Anaconda Python distribution on your notebook. Download and follow the instructions here:\\nhttp://continuum.io/downloads\\nAfter installation, start IPython from the command line interface/shell as follows:\\n$ ipython notebook --pylab inline\\nIPython should then start in your default Web browser.\\n',\n",
       "  u'\\n                    Although Python programs may be slow for certain types of tasks, there are many\\r\\ndifferent ways to improve the performance. This tutorial will introduce\\r\\noptimization strategies and demonstrate techniques to implement them. Another\\r\\nobjective of this course is to help the participants to gain the ability to\\r\\ndecide what might be the optimal solution for a specific performance problem.\\r\\nAbstract\\r\\n\\r\\nThis is a hands-on course. Students are strongly encouraged to work along with\\r\\nthe trainer at the interactive prompt. There will be exercises for the students\\r\\nto do on their own. Experience shows that this active involvement is essential\\r\\nfor an effective learning.\\n                ',\n",
       "  u\"\\n                    What are generators and coroutines in Python? What additional conceptualisations do they offer, and how can we use them to better model problems? This is a talk I've given at PyCon Canada, PyData Boston, and PyTexas. It's an intermediate-level talk around the core concept of generators with a lot of examples of not only neat things you can do with generators but also new ways to model and conceptualise problems.\\n                \",\n",
       "  u'\\n                    Python makes hacking data into shape a far less painful process than it used to be, and has some fairly powerful visualization tools to boot. But what happens if you want to get those results onto the browser, something that is becoming increasingly imperative for much modern data-science? For the foreseeable future that means playing nicely with Javascript (JS) and, due to a recent arms-race among the JS engines, the good news is that JS is quite capable of handling the demands of cutting, splicing, filtering and visualizing etc. large data-sets interactively. This interativity, the ability to play with data in real-time, is day to the night of static, pre-rendered images. \\r\\n\\r\\nThis talk will focus on web-based data-visualizations, showing what JS can do on the client-side, how Python can provide the perfect back-end partner and some of the challenges and gotches experienced by the speaker. The key JS libraries discussed will be D3 (data driven documents) and Angular. Python old faithfuls Scipy, Numpy, and Panda will get the usual head-nod. \\n                ',\n",
       "  u'\\nThis talk describes Gradient Boosted Regression Trees (GBRT), a powerful statistical learning technique with applications in a variety of areas, ranging from web page ranking to environmental niche modeling. GBRT is a key ingredient of many winning solutions in data-mining competitions such as the Netflix Prize, the GE Flight Quest, or the Heritage Health Price.\\nI will give a brief introduction to the GBRT model and regression trees -- focusing on intuition rather than mathematical formulas. The majority of the talk will be dedicated to an in depth discussion how to apply GBRT in practice using scikit-learn. We will cover important topics such as regularization, model tuning and model interpretation that should significantly improve your score on Kaggle.\\n',\n",
       "  u\"\\n                    In this talk I will describe a system that we've built for doing hierarchical text classification. I will describe the logical setup of the various steps involved: data processing, feature selection, training, validation and labelling.\\r\\n\\r\\nTo make this all work in practice we've mapped the setup onto a Hadoop cluster. I'll discuss some of the pro's and con's that we've run into when working with Python and Hadoop.\\r\\n\\r\\nFinally, I'll discuss how we use crowdsourcing to continuously improve the quality of our hierarchical classifier.\\n                \",\n",
       "  u\"\\n                    The average ratio between house prices and rents is an indicator of housing market conditions. Since micro data on rents are difficult to find, little is known about this ratio at the individual-property level.\\r\\nIn this project, I analyse a real estate agency's proprietary dataset containing tens of thousands of rental transactions in Central London during the 2006-2012 period. I merge it with the Land Registry and isolate 1,922 properties which were both sold and rented out within a six-month period. I measure their price-rent ratios and show that price-rent ratios are higher for bigger and more central units.\\n                \",\n",
       "  u'\\n                    Modern programs used in quantum chemistry represent an enormous number of man years of research and development. Their use has resulted in significant contributions to both chemistry and solid state physics and they remain key to research today. On the practical side however many of these programs are firmly lodged in a paradigm from another age requiring construction of complex text based input files to direct calculations and producing large quantities of text based output files that are not machine readable and are barely human readable. Further, the manner in which quantum chemical research is reported in the literature often makes reproducing the work of other scientists an unnecessarily onerous task.\\r\\n\\r\\nIn this talk I will discuss how use of python wrappers and parsers together with the extended scientific python stack can change the way we construct and analyse calculations using Gaussian (the most popular quantum chemistry package) and how in combination with the IPython notebook we can transform the way quantum chemists share both the output of their research as well as the important lessons they learned along on the way.\\n                ',\n",
       "  u'\\n                    AvoPlot is a simple graphical plotting program created by scientists, for scientists. Born out of the frustrations of a multi-disciplinary research group working in the lab, the field and a desk, it aims to solve some of the greatest remaining problems in scientific data handling, such as:\\r\\n\\r\\n    * \"I hate programming! I wish I could create an interactive plot of this data file without having to write a script.\"\\r\\n    * \"This plot that matplotlib created is beautiful! But I wish I could just click and drag that title a bit to the left\".\\r\\n    * \"I have this great bit of data processing code and I\\'d like to make it available as a graphical application, but I have neither the time nor the programming ability to do so.\"\\r\\n    * \"Spreadsheets suck! I want to work with my data in a visual way, rather than having to deal with tables of numbers.\"\\r\\n\\r\\nBuilt on top of Python\\'s excellent matplotlib plotting library, AvoPlot is a graphical plotting tool designed for visualising and analysing scientific data. In addition to providing a graphical user interface to many of the capabilities of matplotlib, it also provides a plug-in framework, allowing users to extend its standard feature set to meet their specific requirements. Plug-ins can be written both to import different data sets (e.g. binary data), and to provide analysis tools for working with them (e.g. fitting routines, background subtraction etc.). \\r\\n\\r\\nThis talk will take a light-hearted look at some of the data handling problems encountered by scientists, and explain how we have brought together the capabilities of many well established Python packages into a single convenient application in an attempt to solve them.\\n                ',\n",
       "  u'\\n                    Understanding human mobility patterns is a significant research endeavor that has recently received considerable attention. Developing the science to describe and predict how people move from one place to another during their daily lives promises to address a wide range of societal challenges: from predicting the spread of infectious diseases, improving urban planning, to devising effective emergency response strategies. This presentation will discuss a Bayesian framework  to analyse an individual\\u2019s mobility patterns and identify departures from routine. It is able to detect both spatial and temporal departures from routine based on heterogeneous sensor data (GPS, Cell Tower, social media, ..) and outperforms existing state-of-the-art predictors.  Applications include mobile digital assistants (e.g., Google Now), mobile advertising (e.g., LivingSocial), and crowdsourcing physical tasks (e.g., TaskRabbit).\\n                ',\n",
       "  u'\\n                    The UK has one of strongest digital economies in the world, yet the size and scope of this space is poorly understood through conventional classification systems and datasets. This is not only important for economists but for the people affected by Government policy. In this talk, we will provide an overview of the work Growth Intelligence has done in generating our own classification system to describe businesses using web-based data and python tools and techniques \\u2013 including the challenges we still face. We will also look at some situations in which traditional methods fall short of a data driven approach.\\n                ',\n",
       "  u\"\\n                    Many people have started to suspect that their A/B testing results are not what they seem. A/B test reports an uplift of 20% and yet this increase never seems to translate into increased profits. So what\\u2019s going on? \\r\\n\\r\\nI'll use python simulations to show that A/B testing is often conducted in such a way that it virtually guarantees false positive results. I'll also mention some python functions that can be used to avoid these problems.\\r\\n\\n                \",\n",
       "  u'\\n                     Join us for a glass of vino, amazing tapas, and expertly selected craft beers at Levante Bar to discuss all things PyData! Invitation only so please RSVP via the link here.\\n                ',\n",
       "  u\"\\n                    A goal of this panel is to identify problems that companies in London have with data science in the hope that people get together to solve them. Who does what, what's wrong, what's missing, and how can we improve things?\\n                \",\n",
       "  u\"\\nWill your decisions change if you'll know that the audience of your website isn't 5M users, but rather 5'042'394'953? Unlikely, so why should we always calculate the exact solution at any cost? An approximate solution for this and many similar problems would take only a fraction of memory and runtime in comparison to calculating the exact solution.\\nThis tutorial is a practical survey of useful probabilistic data structures and algorithmic tricks for obtaining approximate solutions. When should we use them, and when we should not trade accuracy for scalability. In particular, we start with hashing and sampling; address the problems of comparing and filtering sets, counting the number of unique values and their occurrences; touch basic hashing tricks used in machine learning algorithms. Finally, we analyse some examples of their usage show the full power: how to organise an online analytics, or how to decode a DNA sequence by squeezing a large graph into a bloom filter.\\n\",\n",
       "  u'\\n                    As businesses search for diversification by trading new financial products, it is easy for market data infrastructure to become fragmented and inconsistent.  We describe how we have successfully used Python, Pandas and MongoDB to build a market data system that stores a variety of Timeseries-based financial data for research and live trading at a large systematic hedge fund.  Our system has a simple, high-performance schema, a consistent API for all data access, and built-in support for data versioning and deduplication.  We support fast interactive access to the data by quants, as well as clustered batch processing by running a dynamic data flow graph on a cluster.\\n                ',\n",
       "  u'\\nDiamond Light Source is the UK Synchrotron, a national facility containing over 20 experimental stations or beamlines, many of which are capable of generating Terra-bytes of raw data every day. In this data rich environment, many scientists that come to the facility can be daunted by the sheer quantity and complexity of the data on offer. The scientific software group is charged with assisting with this deluge of data and as a small team it is imperative that provides sustainable and rapid solutions to problems. Python has proved to be well suited to this and is now used heavily at the facility, from cutting edge research projects, through general pipe-lining and data management, to simple data manipulation scripts. And by a range of staff and facility users, from experienced software engineers and scientists, to support staff and PhD students simply wanting something to help make sense of the data or experimental set-up.\\nThis presentation focuses on the current state of the scientific management and data analysis within Diamond, and details the workhorses which are relied on, as well as what the future holds.\\n',\n",
       "  u'\\n                    In the context of a rapidly evolving financial industry, managing increasing amounts of data and coping with regulatory requirements, time-to market of services and cost efficiency along the value chain are key success drivers for any financial institution. Especially the shift from monolithic architectures (e.g. Open VMS/Cobol) to heterogeneous technology stacks and systems (e.g. Linux/Java/SQL) creates additional challenges for IT. In addition, the \\u201ctechnology empowerment of the business analysts\\u201d adds complexity to the implementation of IT systems if not managed properly. After the introduction of Python at Deutsche B\\xf6rse Group several years ago, the presentation today is a reflection about experiences in real world applications, the potential of Python as a universal tool for end-to-end development and an outlook to the future of this language framework in the financial industry.\\n                ',\n",
       "  u'\\nThe pharmaceutical industry is a \\xa3250 billion dollar a year industry and a third of the world\\u2019s R&D in pharmaceuticals occurs in the UK. Python is well used in high-throughput screening and target validation with a notable example at AstraZeneca displayed prominently on the python.org website but further along the drug development process Python and it\\u2019s scientific stack offers a compelling and comphrensive toolkit for use in preclinical and clinical drug development.\\nIn this talk, a demonstration of how Python/SciPy was used to calculate cardiac liability of a drug was assessed as part of routine preclinical screen, how Python was used to statistically analyse a Phase II clinical dataset and how Python was used organise and structure documentation about a new chemical entity according to regulated standards for submission to the European Medicines Agency. Lastly, the talk will conclude with the current barriers to progress for Python to be used more routinely for pharmaceutical problems and how the community might address Python being used in a heavily regulated environment.\\n',\n",
       "  u'\\nThanks to Python and R, data scientists and researchers have in hands highly powerful tools to program with data, simulate and publish reproducible computational results. Educators have access to free and open environments to teach efficiently statistics and numerical subjects. Thanks to cloud computing, anyone can work today on advanced high capacity technological infrastructures without having to build them or to comply with rigid and limiting access protocols. By combining the power of Python, R and public clouds such as Amazon EC2, it became possible to build a new generation of collaboration-centric platforms for virtual data science and virtual education of considerable power and flexibility.\\nThis tutorial aims to familiarise the attendees with what public clouds can do for e-Science and e-Learning, to present the challenges and opportunities raised by the use of Python and R on such infrastructures and to introduce Elastic-R, one of the first free Python/R-centric virtual data science platforms (www.elasticr.com).\\n',\n",
       "  u'\\n                    Recommender Systems are now ubiquitous in solving the problem of predicting latent behaviour from customer data. This workshop presents an introduction to this problem using a wide range of Python based libraries. We firstly review the general problem and simple collaborative filtering methods in numpy, before examining Matrix Factorisation using the SVD methods present in scipy and the sparsesvd. We then explore Non-negative Matrix Factorisation and binary problems using the nimfa library. To summarise, we end end with a comparative study on an example data-set, comparing efficacy and accuracy.\\r\\nThis tutorial is online at http://nbviewer.ipython.org/gist/jonnydedwards/8652546\\n                ',\n",
       "  u\"\\nPython threads cannot utilize the power of multiple CPUs.\\r\\nOther solutions such multiprocessing or MPI wrapper are based\\r\\non message passing, resulting substantial overhead for certain\\r\\ntypes of tasks.\\nWhile pure Python does not support shared memory calculations,\\r\\nCython combined with OpenMP can provide full access to this type\\r\\nof parallel data processing.\\nThis talk gives a whirlwind tour of Cython and introduces Cython's\\r\\nOpenMP abilities focusing on parallel loops over NumPy arrays. Source\\r\\ncode examples demonstrate how to use OpenMP from Python. Results for\\r\\nparallel algorithms with OpenMP show what speed-ups can be achieved\\r\\nfor different data sizes compared to other parallelizing strategies.\\n\",\n",
       "  u'\\n                    A Python programmer has many options to profile and optimize CPU-bound and data-bound systems, common solutions include Cython, numpy and PyPy. Increasingly we have single-core solutions that should take advantage of many cores and clusters. This talk reviews the current state of the art, looking at the compromises and outcomes of the current approaches and reviews upcoming solutions like Numba, Pythran and PyPy\\u2019s numpy. Thoughts will be shared on how current hindrances might be improved.\\r\\n\\n                ',\n",
       "  u'\\n                    Processing large-scale data set with Hadoop are commonplace nowadays. Python programmers can leverage a large set of frameworks like mrjob, dumbo, hadoopy, pydoop, and so forth to write sophisticated data processing jobs. However, these come with overhead and require some learning.\\r\\n\\r\\nApache Hive is a SQL engine on Hadoop, which can stream data through Python programs in a distributed fashion. The approach has multiple benefits. Hive and SQL do all the heavy lifting of selecting, joining and filtering the data, and programmers can focus their time and effort on the core logic of what they want to achieve. Nowadays, Hadoop cloud services make clusters available inexpensively, instantaneously, and without expert Hadoop knowledge.\\r\\n\\r\\nThis talk will discuss how Hive and Python (and Hadoop) work together, how to get started with it in the cloud, and will showcase some scalable Python examples.\\n                ',\n",
       "  u'\\n                    Practical Principles of Information Visualization Design\\r\\n\\r\\nData is meant to be seen. In an information economy, there is no shortage of information; only genuine understanding is in short supply. Knowledge workers are continually asked to make sense of more information than they could possibly have time to read and assimilate. Users have come to demand insight at a glance: the whole picture, not just an endless list of results. After all, as information becomes ever more abundant, attention remains as scarce as ever. Visualization, animation, and interaction can be gainfully employed to develop information systems that are both useful, enabling users to get the job done well, and usable, empowering users to do job with ease. Effective information visualization should be immediately appealing to the eye and directly relevant to the task, routinely enjoyable to the user and uniquely valuable to the business. By integrating the power of computational analysis with the expertise of human judgment, visualization serves to turn aggregated information into actionable insight, illustrating the way numbers can tell a story compelling enough for people to make decisions they can trust.\\r\\n\\r\\nThis presentation focuses on making the art and science of information visualization design more accessible and applicable to a wider audience, outlining a variety of ready-to-use techniques that cover business requirement validation, functional design analysis, visual perception considerations, robust descriptive statistics, and usability study evaluations. It features a brief guided tour of visualization resources on internet, including the available open source Python implementations. Time permitting, it may also include examples of innovative visualizations used in various applications, including a research project for Grapeshot and IBM to create an online news analysis service that provides objective metrics for the relative influence of different news sources on shaping how news coverage evolves over time.\\r\\n\\n                ',\n",
       "  u\"\\n                    Making product decisions is hard - often, we take one step forward, but two steps back. We need to become scientists about our own product - run an AB test comparing two versions and measure which works better. We'll talk through a simple but effective backend AB testing framework in Python (using Django as an example), along with some of the issues, gotchas and best practices of running AB tests in production.\\n                \",\n",
       "  u\"\\nData and algorithms are artistic materials just as much as paint and canvas.\\nA talk covering my recent work with The Tate's CC dataset, David Cameron's deleted speeches and the role of the artist in the world of Big Data.\\n\"],\n",
       " 'nyc2013': [u\"\\n                    I'll walk you through Python's best tools for getting your hands dirty with a new dataset: IPython Notebook and pandas. I'll show you how to read in data, clean it up, graph it, and draw some conclusions, using some open data about the number of cyclists on Montr\\xe9al's bike paths as an example.\\r\\n\\n                \",\n",
       "  u'\\nAn increasing amount of information is being conveyed via images, with mobile photographs being a particularly notable case. While image metadata, such as the text linking to an image in Google image search, is a common input to machine learning tasks, the content of the image itself is used far less frequently. Whether picking your best vacation photos, recommending similar-looking images to use in a talk or searching for hidden cats, learning tasks can do better if the actual pixels are used as input.\\nHowever, The choice of features determines the quality of result as much as the choice of machine learning algorithm and using the pixels directly often yields the poor results. Higher-level image features, such as face detection, histograms and color statistics like hue binning, provide significantly better performance. While advantageous, these features force the developer to choose from a vast number to accurately capture the details of their problem domain, a challenging task. This talk covers classes of simple image features and how to employ them in machine learning algorithms and focuses on providing basic domain knowledge in imaging/computer vision to developers already familiar with machine learning.\\nThe outline is as follows: We begin with an overview of common image features and discuss potential applications for each. Common features include examples from computer vision such as blob identification, face detection and edge statistics as well as from image statistics such as intensity histograms, Fourier properties and color statistics such as hue binning. Next, we present how to generate the features with python imaging libraries. Finally, we discuss approaches to converting complex image features into a series of scalars for the input vector of an ML algorithm that best represent the problem domain.\\n',\n",
       "  u'\\n                    Scikit-learn is one of the most well-known machine learning Python modules in existence. But how does it work, and what, for that matter, is machine learning? For those with programming experience but who are new to machine learning, this talk gives a beginner-level overview of how machine learning can be useful, important machine learning concepts, and how to implement them with scikit-learn. We\\u2019ll use real world data to look at supervised and unsupervised machine learning algorithms and why scikit-learn is useful for performing these tasks.\\r\\n\\n                ',\n",
       "  u'\\n                    Attendees will be given a practical introduction to Data analysis with Pandas. A sample dataset with minute-wise station populations will be the primary dataset that participants will analyze, explore, and find novel threads of insight about the Citibike bike sharing system.  At the end of the workshop students will be able to derive the total number of rides in a day, a prediction of total rides based on weather and number of users, an estimate number of daily passes purchased, comparisons of weekday vs weekend statistics, and a method of determining when rebalancing occurred.   The focus will be on exploratory play rather than advanced statistics.  A basic understanding of Python is necessary and prior NumPy/Pandas experience is helpful.  No math or statistics background is necessary to understand this tutorial.\\n                ',\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u'\\nProbabilistic Programming allows flexible specification of statistical models to gain insight from data. Estimation of best fitting parameter values, as well as uncertainty in these estimations, can be automated by sampling algorithms like Markov chain Monte Carlo (MCMC). The high interpretability and flexibility of this approach has lead to a huge paradigm shift in scientific fields ranging from Cognitive Science to Data Science and Quantitative Finance.\\nPyMC3 is a new Python module that features next generation sampling algorithms and an intuitive model specification syntax. The whole code base is written in pure Python and Just-in-time compiled via Theano for speed.\\nIn this talk I will provide an intuitive introduction to Bayesian statistics and how probabilistic models can be specified and estimated using PyMC3.\\n',\n",
       "  u'\\n                    There\\'s one real problem in data science: data, and the cleaning and slicing required before it can be fed into exciting machine learning algorithms. In this talk, I will present a survey of data management tools in Python, ranging from easy ways to handle CSV data at the small end (csv, sheets); using Python\\'s standard sqlite3 package as an in-memory RDBMS to get code \"beyond the dict\"; using PyTables as a highly CPU- and IO-efficient store for numerical and relational data; to ORM systems to scale to data beyond the needs of a script pipeline. After this talk, attendees will understand how to make their data-handling code more efficient and expressive with these major libraries.\\r\\n\\n                ',\n",
       "  u'\\n                    Sponsor Workshop\\n                ',\n",
       "  u'\\nErik will be talking about Luigi, a recently open-sourced Python framework that helps you build complex pipelines of batch jobs, handle dependency resolution, and create visualizations to help manage multiple workflows.\\nLuigi provides an infrastructure that powers several Spotify features including recommendations, top lists, A/B test analysis, external reports, internal dashboards, and many more. It also comes with Hadoop support built in (and that\\u2019s where really where its strength becomes clear). Spotify uses it to run 10,000+ of Hadoop jobs every day, but also other actions like training machine learning algorithm, sending out reports, loading data into databases, and much more.\\n',\n",
       "  u\"\\nPlayHaven is working to model and predict user behavior in games on mobile devices. We are integrated into 5,000 games, see around 130 million unique users monthly, and record events from around 2.5 billion game sessions. Given the right data science tools, we will be able to intelligently redirect that traffic amongst our client's games to both dramatically improve the game player's experience and, thus, to improve the quality of our client's business.\\nThe project chosen as PlayHaven's first run with predicting user behavior is churn prediction. Churn in our context is defined as a user that was previously active within a game decides to leave that game. The implications of successfully predicting include providing us with the chance to see why and solve a user's new lack of interest and, if that fails, provide them with content more in line with their interests.\\nThe method used is called Reconstructability Analysis (RA) - a graphical model framework with heavy overlap in Loglinear Models and Bayesian Networks. This is a conception-to-deliverable overview. We will start with a top level view of PlayHaven and what that means in terms of the data we collect and what we want from data science. We then discuss what data we pull and the resources needed to pull and pre-process it. With a data set built, we will discuss model construction and predictor performance in terms of predictive accuracy and computational resources needed. We will conclude with a top level discussion of a few special applications of RA.\\n\",\n",
       "  u'\\n                    Excel is the lingua franca of data - if you share data with non-programmers, you probably use Excel to do it. This talk will show you how to use Python to work with spreadsheets programatically, and what you can build once you start scripting Excel.\\r\\n\\n                ',\n",
       "  u'\\nAs the popularity of machine learning techniques spreads to new areas of industry and science, the number of potential machine learning users is growing rapidly. While the fantastic scikit-learn library is widely used in the Python community for tackling such tasks, there are two significant hurdles in place for people working on new machine learning problems:\\n1. Scikit-learn requires writing a fair amount of boilerplate code to run even simple experiments.\\n2. Obtaining good performance typically requires tuning various model parameters, which can be particularly challenging for beginners.\\nSciKit-Learn Laboratory (SKLL) is an open source Python package, originally developed by the NLP & Speech group at the Educational Testing Service (ETS), that addresses these issues by providing the ability to run scikit-learn experiments with tuned models without writing any code beyond what generates the features. This talk will provide an overview of performing common machine learning tasks with SKLL.\\n',\n",
       "  u'\\n                    Coming Soon.\\n                ',\n",
       "  u'\\nHow can Python be embedded into C/C++ applications?\\nThis talk covers the well-known very high-level and pure embeddings and includes two novel forms of embedding: a zero-interpreter embedding using Cython and Python running from within Python\\n',\n",
       "  u'\\n                    IPython is a powerful shell that enhances dynamic scripting in Python. Microsoft Excel is the default spreadsheet software that serves as a great user interface. Combining them together can greatly increase productivity of software developers, analysts and business users. In this talk I\\u2019m going to demonstrate some of the most valuable features of IPython in Excel. Live demos will be provided.\\r\\n\\n                ',\n",
       "  u'\\n                    Computing, and thus software, is one of the foundations of modern\\r\\ntechnical work across a broad range of fields. Like anything, all\\r\\nsoftware has attributes: slow, fast, buggy, robust, etc. However,\\r\\nthese attributes are not passive and neutral.  In this talk I will\\r\\ndescribe how the attributes of software have a profound affect on\\r\\nhuman behavior, attitudes and thought patterns. These attributes, for\\r\\nbetter or worse, infect all of the work that is done using the\\r\\nsoftware. To explore these ideas, I will provide an attribute based\\r\\ntour of the IPython Notebook. This tour will elucidate the overall\\r\\nvision for the project and cover our recent work on interactive\\r\\nwidgets and converting notebooks to different formats.\\n                ',\n",
       "  u'\\n                    How can we model problems using generators and coroutines? What additional conceptualizations do these modelings allow and what benefits can we derive from these approaches?\\r\\n\\n                ',\n",
       "  u'\\nThis is an introduction to data analysis in the context of chip design.\\nChip design complexity has grown exponentially in the last 5 years. The primary driver for this has been a demand for higher performance and lower power . While these requirements are not new, what is relatively new is an increased complexity in design styles and manufacturing processes - multiple process corners, multiple on chip voltages, voltage and frequency scaling, many operating modes etc. This has led to a huge increase in the amount of data generated in the process of designing current generation chips. This has made it harder to analyze using traditional methods used in the chip design space like reviewing waveforms, reports and ad-hoc scripting. All this makes this area attractive for rich data analysis.\\nThis talk will be an introduction to pandas and matplotlib with chip design data as the background.\\n',\n",
       "  u'\\n                    GeoPandas extends the pandas data analysis library to work with geographic objects. File I/O, geometric operations, map projection transformations and plotting are provided in a high level interface that makes use of other libraries including Shapely and Fiona. GeoPandas is ideal for interactive use with IPython, and provides easy geospatial analysis and manipulation tools without a need for complicated desktop GIS applications or spatial databases.\\r\\n\\n                ',\n",
       "  u'\\n                    You will use pre-existing code and write some or your own in order to classify emails from the Enron corpus.\\n                ',\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u\"\\nEnaml is an open source library for building rich user interfaces utilizing a declarative extension to the Python language grammar. Notable features of the framework include: automatic dependency analysis at the bytecode level, a constraints based layout system, support for multiple rendering engines, and the ability to interface with a variety of model notification systems. The Enaml DSL is a strict superset of Python which allows the developer to declaratively define dynamic, extensible, and reactive trees; such trees are particularly well suited for the definition of user interface hierarchies.\\nEnaml has been used for building production applications at multiple Fortune 500 companies and serves as the foundational UI framework at one of the world's leading investment banks. This talk by the author of Enaml will provide an introduction to the language and demonstrate some of the notable features of the framework.\\n\",\n",
       "  u\"\\n                    Scikit-learn is a popular Python machine learning library. In this tutorial, I'll give an introduction to the core concepts of machine learning, using scikit-learn to demonstrate applications of these concepts on real-world datasets. We'll cover some of the most powerful and popular supervised and unsupervised learning techniques, including classification and regression models like Support Vector Machines and Random Forests, clustering models like K Means and Gaussian Mixtures, and dimensionality reduction models like PCA and manifold learning. Throughout, I'll emphasize the key features of the scikit-learn API, so that participants will be well-poised to begin exploring their own datasets using the wide array of algorithms implemented in scikit-learn.\\r\\n\\n                \",\n",
       "  u'\\n                    What is numba, why is it different to other alternatives and why would you want to use it. Basic tutorial, modeling some simple interview question-like problems, optimizing using numba and analyzing performance from the perspective of a first-time user.\\r\\n\\n                ',\n",
       "  u'\\n\\nThis talk will use core functionality from the `PyToolz` projects. Students will leave both with a set of concrete tools and with an understanding of some of the more applicable lessons from the functional style.\\n',\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u\"\\n                    More and more applications are now dealing with massive data that need to be processed in realtime. While easing the development of realtime analytics applications, computing platforms like Storm increases the need for efficient algorithms that can run on a single pass on the data stream. In this talk, I'll give a brief overview of some interesting probabilistic data structures that can used in this context: Bloomfilter, Temporal Bloomfilter, Count-Min Sketch and HyperLogLog.\\r\\n\\n                \",\n",
       "  u'\\nDuring the fall of 2012, a heated technical discussion regarding asynchronous programming occurred on python-ideas. One of the outcomes of this discussion was Tulip, an asynchronous programming API for Python 3.3, spearheaded by Guido van Rossum. A lesser known outcome was PyParallel: a set of modifications to the CPython interpreter that allows Python code to execute concurrently across multiple cores.\\nTwisted, Tulip, Gevent, Stackless/greenlets and even node.js are all variations on the same pattern for achieving \"asynchronous I/O\": non-blocking I/O performed on a single thread. Each framework provides extensive mechanisms for encapsulating computational work via deferreds, coroutines, generators and yield from clauses that can be executed in the future when a file descriptor is ready for reading or writing.\\nWhat I found troubling with all these solutions is that so much effort was being invested to encapsulate future computation (to be executed when a file descriptor is ready for reading or writing), without consideration of the fact that execution is still limited to a single core.\\nPyParallel approaches the problem in a fundamentally different way. Developers will still write code in such a way that they\\'re encapsulating future computation via the provided APIs, however, thanks to some novel CPython interpreter modifications, such code can be run concurrently across all available cores.\\nThis talk will cover the history behind PyParallel, the numerous novel techniques invented to achieve concurrent interpreter execution, real-life examples of PyParallel in action (multi-threaded HTTP servers, parallel task decomposition). It will detail the approach PyParallel takes towards facilitating asynchronous I/O compared to competing libraries like Twisted and Tulip. It will also provide insight into the direction of PyParallel in the future, including things like LLVM integration via Numba/Blaze to further improve computational performance.\\nLink to the deck being presented: https://speakerdeck.com/trent/pyparallel-how-we-removed-the-gil-and-exploited-all-cores-1\\n',\n",
       "  u'\\nDatadog processes and reports on tens of billions of events a day and is used to manage infrastructures ranging from a couple of servers to many thousands of instances.\\nThis talk will explain why we chose python as the core ecosystem to build it on, how we scaled the system to date and what we learned in the process.\\nExpect to hear about numerical processing with numpy or pandas, memory management, concurrency, profiling and performance tuning, as well as examples of optimization using C or Cython.\\n',\n",
       "  u'\\nThe Python data ecosystem has grown beyond the confines of single machines to embrace scalability. Here we describe one of our approaches to scaling, which is already being used in production systems.\\nThe goal of in-database analytics is to bring the calculations to the data, reducing transport costs and I/O bottlenecks. The Greenplum Database is now part of the Pivotal Platform and provides super fast analytics capabilities through a shared-nothing architecture and SQL interface (based on Postgres). In addition to running parallel queries across terabytes of data using pure SQL it can also run procedural languages such as PL/Python. MADlib, Pivotal\\u2019s open source library for scalable in-database machine learning, uses Python to glue SQL queries to low level C++ functions and is also usable through the PyMADlib package. We will also show how we have used many of the standard tools in the Python data analysis toolkit in this framework, including Pandas, scikit-learn, nltk and of course NumPy and SciPy.\\nIn particular combining these tools has allowed us to perform sentiment analysis on large datasets and we will discuss the strategies and issues we have come across along the way.\\n',\n",
       "  u\"\\nSciDB-Py connects two of the most powerful open source tools whose shared mission is to change the way quants, data scientists, scientists and analysts work with big data: SciDB and python.\\nThe need to safely store and quickly manipulate vast datasets has made databases increasingly important to data scientists.  Unfortunately, the array-based datasets central to many applications do not fit neatly into classic relational or  key-value paradigm of popular database architectures. Enter SciDB, a new database platform which is built around large multi-dimensional arrays. It provides both efficient distributed storage and fast array-oriented computations, from simple slices and joins to more involved parallel linear algebraic operations. This talk will introduce SciDB-py, a Python package which wraps SciDB with a familiar, numpy-like syntax.  With SciDB-py, data scientists can effortlessly store and manipulate extremely large array-based data from the comfort of the Python interpreter.\\nWe'll demonstrate working with timeseries data in SciDB and present basic examples that illustrate SciDB's native analytics capabilities including aggregation and data decimation, regression and generalized linear models, covariance matrices, singular value decomposition, and extending SciDB with custom operations. The examples apply to a broad range of applications including quantitative finance, econometrics, and risk and credit analysis.\\nThe examples demonstrate how SciDB-Py\\u2019s scale out MPP architecture enables interactive exploratory analytics on large-scale data.\\n\",\n",
       "  u\"\\n                    Parakeet is a runtime compiler for numerical Python. It takes array-oriented computations, optimizes them, and compiles them to native code. Parakeet reimplements a subset of NumPy's library functions using data parallel operators, which are amenable to parallel execution. Until recently, however, this parallelism was wasted on a single-core LLVM backend. A new CUDA backend for Parakeet is under development and might prove to be the easiest to write GPU programs in Python.\\r\\n\\n                \",\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u'\\n                    Many data sets are beautiful in themselves, but how do we make their beauty obvious? Data visualization, of course. This hands-on tutorial will explore the craft, once described by industry expert Ben Fry as a clear multi-step process oriented around data: acquire, parse, filter, mine, represent, refine, & interact. Python -- especially with new analysis tools like Pandas -- excels at the first few steps. However, other tools beyond Python must be used in order to represent, refine, and interact with data. The best toolset for this lives in the modern browser. Many PyData attendees are familiar with IPython Notebook. It provides an ideal place for us to build out a \"read-evaluate-print loop\" (REPL) for the visual data exploration process. Join us as we unify Python with browser technologies like JavaScript, CSS, and SVG, in a single space. The tutorial will cover how to iteratively produce visualizations from a raw data set of online news articles & web traffic metrics. You will use IPython Notebook to discover hidden patterns in the data, then you will convert your own explorations into production visualizations ready for interaction (& publication!) on the web. The tools covered will include: Pandas; D3.js; NVD3.js; Vega / Vincent; and PhantomJS.\\n                ',\n",
       "  u'\\n                    This talk covers rapid prototyping of a high performance scalable text processing pipeline in Python.  We demonstrate how Python modules can be used to analyze, clean, perform feature extraction, and finally classify half a million documents.  Our style is to build small and simple modules (each with command line interfaces) that use very little memory and are parallelized with the multiprocessing module.  \\r\\n\\n                ',\n",
       "  u'\\nThis panel is a unique take on the question of Python Scalability.\\nThe first topic is the challenge of scaling Python across computational clusters and hardware like GPUs.  How do we take a language that prides itself on readability and ease-of-use, and apply it to cutting edge challenges like cluster computing, multi-core processors, and GPUs?  Are there fundamental limitations - do we have to move to a different language for these things?\\nThe second topic is the challenge of scaling the *usage* of Python across a team or an organization.  Oftentimes, Python is chosen by the early prototypers in a team, and as they succeessfuly deploy their code into production, more and more people - who do not necessarily possess the mindset of the early evangelists - start needing to use or adopt the language, without necessarily having \"organically\" grown an appreciation for its idioms.  Sometimes the code they produce more closely resembles Java or C++, but just spelled in Python.  How can experienced Python programmers help grow the effective usage of the language across the organization?\\nThese are two very different questions, but they are both rooted in a common theme: Python\\'s ease of use and its power as a scripting language is actually a double-edged sword.  Since people are used to things being much easier and more pleasant to code in Python, when they run into traditionally challenging problems like distributed computing or software management, there is a tendency to just assume that Python is \"really bad\" for that use case.  But Python can be successfully scaled across a cluster and across and organization.  In this panel discussion, we\\'ll talk about lessons learned in both of these situations.\\n',\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u\"\\n                    Are your data too complicated to visualize? Have you considered complimenting your visualizations with music? In this session, we'll analyze data from the American Community Survey by composing data-driven music with the ddpy package (https://github.com/csv/ddpy).\\r\\n\\n                \"],\n",
       " 'nyc2014': [u'\\n                    This talk covers rapid prototyping of a high performance scalable text processing pipeline development in Python. We demonstrate how Python modules, in particular from the Rosetta library, can be used to analyze, clean, extract features, and finally perform machine learning tasks such as classification or topic modeling on millions of documents. Our style is to build small and simple modules (each with command line interfaces) that use very little memory and are parallelized with the multiprocessing library.\\n                ',\n",
       "  u'\\n                    Scikit-Learn is one of the most popular machine learning library written in Python, it has quite active community and extensive coverage for a number of machine learning algorithms. It has feature extraction, feature and model selection algorithms, and validation methods as well to build a modern machine learning pipeline. \\r\\n\\r\\nThis tutorial introduces common recipes to build a modern machine learning pipeline for different input domains and show how one might construct the components using advanced features of Scikit-learn.  Specifically, I will introduce feature extraction methods using image and text, and show how one may use feature selection methods to reduce the input dimension space and remove the features which are not useful for classification. For optimization, I will show model selection methods using parameter search. Last in the pipeline, I will show validation methods to be able to choose best parameters. After building the pipeline, I will also show how one might deploy the model into production.\\n                ',\n",
       "  u'\\n                     IPython recently introduced a new framework for interactive javascript widgets in their online notebook.  These widgets allow one to easily create and interact with buttons, sliders, text fields, and other HTML/Javascript objects from Python code.  The framework has also been used for advanced systems of widgets, such as WebGL 3d graphics and interactive maps.  I will discuss and give examples of how to write simple and complicated widgets, demonstrate complicated widget examples such as the pythreejs widget collection for interactive 3d graphics (https://github.com/jasongrout/pythreejs), and discuss changes in the widget infrastructure in the upcoming IPython 3.0.  Since this is an advanced talk, it would be most helpful if participants worked through the widget tutorial notebooks at  http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Interactive%20Widgets/Index.ipynb before the talk.\\r\\n\\n                ',\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u\"\\n                    Python has a rich ecosystem of open source geographical information science (GIS) applications. Most of the GIS packages are Python bindings to binaries for data transformation and image manipulation. This makes it hard to study what the data processing encompasses and masks the underlying algorithms.\\r\\n\\r\\nThis talk will use Landsat 8 satellite imagery and Python scientific stack to demonstrate a typical data-centric approach for GIS analysis and at the same time explain algorithmic underpinnings. Image recognition and machine learning techniques will be applied to satellite images to expose the data's openness to exploration.\\n                \",\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u'\\n                    ------\\n                ',\n",
       "  u'\\nBlaze is a NumPy/Pandas interface to big data systems like SQL, HDFS, and Spark.  Blaze provides Python developers access to the rich analytic processing available both within the Python ecosystem and beyond.\\nInternally Blaze is a lightweight data modeling language (expressions with type information) alongside a set of interpreters (Python, SQL, Spark, MongoDB, ...).  The modeling language provides an intuitive and familiar user experience.  The interpreters connect that experience to a wide variety of data technologies.  This combination allows developers to construct connections to novel technologies.  These connections enable users to interact with their system of choice, regardless if that system is a single CSV file or a large HDFS cluster running Impala.\\nThis is followed by a second talk using Blaze in the wild.\\n',\n",
       "  u'\\n                    A round table discussion where we can talk about what types of events have worked well and share ideas. Organized by Michael Becker.\\n                ',\n",
       "  u\"\\nEver wonder how Google Chrome detects the language of every webpage you visit?\\nData science!\\nWhat is data science and how can you use Python to do it? In this talk, I'll teach you the data science process OSEMN, while creating a language prediction algorithm, utilizing nothing but Python and data from Wikipedia!\\n\\nI'll cover\\n\\nWhat is data science?\\nWriting a simple web scraping bot.\\nUsing pandas for data processing and exploration.\\nCreating beautiful data visualizations with seaborn.\\nWorking with text data in scikit-learn.\\nBuilding a predictive model.\\nEnsuring the model is accurate.\\nUtilizing the model and interpreting the results.\\n\\n\",\n",
       "  u'\\n                    Cubes is a light-weight framework for modelling of conceptual analytical data (part of OLAP) and aggregated browsing. Recent release of Cubes brings ability to build a heterogenous data warehouse  \\u2013 plug-in architecture for analytical backends (SQL, Mongo, Mixpanel, Google Analytics, custom, ...) and provide unified aggregation and conceptual browsing interface in Python or through HTTP/JSON.  This talk will go through the main features of the new Cubes, such as: analytical workspace, pluggable models (model providers) and backends, authentication/authorization (for fine-tuning reporting in a restricted business environment), new modelling concepts and advanced SQL features.\\n                ',\n",
       "  u'\\n                    The field of biology is constantly evolving new methods for quantification and must constantly adapt to different upstream workflows that quantify biological measurements such as gene expression or alternative splicing. We present flotilla (github.com/YeoLab/flotilla) as a flexible, open source, community-driven software package that enables biologists with a rudimentary knowledge of statistical methods and programming to analyze and visualize hundreds of biological datasets. Users of flotilla harness the power of IPython interactive widgets and Python\\u2019s scientific stack, and can quickly perform powerful analyses of biological datasets such as dimensionality reduction, covariance analysis, clustering, classification, regression and outlier detection. As a case study, we will present a single-cell RNA-Sequencing analysis use case of flotilla, with data from two publicly available datasets of individual mouse immune cells in response to a stimulus. With these data, we show the utility of flotilla in quantifying the variation of gene expression and mRNA splicing in single cells.\\n                ',\n",
       "  u'\\n                    Reducing uncertainty is at the heart of statistical analysis and is all the more important when modeling data with numerous predictors.  Regularization can reduce uncertainty when interpreting variables and making predictions, leading to better results.  Bayesians and Frequentists implement this in different ways; prior distributions for the former, penalized regression for the latter.  We examine how the techniques differ and how they are similar and even explore a Bayesian interpretation of the lasso (L1 Penalized Regression) that should make both sides happy.\\r\\n\\r\\n\\n                ',\n",
       "  u'\\n                    Disco is a distributed data processing platform that can be used for both batch and low-latency stream processing.  Jobs are constructed in Python using multi-stage data pipelines and run on a light weight, highly available clustered architecture.  Tim will describe building stream, batch and ad-hoc query based data infrastructure using the Disco ecosystem.\\n                ',\n",
       "  u\"\\n                    Blaze is a library for harnessing the power of big data technologies. In part deux of the blaze-a-thon, we show motivating use cases illustrating why you might want to use blaze, including a comparison of out-of-core pandas and an experimental backend using the lesser known KDB+. Time permitting, we'll show how easy it is for users of blaze to scratch their own itch by hooking an existing API into blaze via a small set of multiply dispatched functions.\\n                \",\n",
       "  u'\\n                    One of the big problems in educational assessment is *norming*, whose simplest form amounts to giving more credit for answering more difficult questions. But the ideas that come from norming have wide-ranging applications: Comparing examinees who have seen different sets of questions, effectively defining \"performance at grade level\", measuring how students learn (relative to their cohort, their school, or any other bucketing scheme). Moreover, similar techniques have been used to rank the skills of players in online video games and assessing the competency of Mechanical Turkers among other applications. In this talk, we\\'ll cover some of the common settings and objective functions whose maxima might be interpreted to solve these problems. And, like any good modeler should do, we\\'ll spend most of the time on where their assumptions break down, difficulties in computing and interpreting the solutions to these programs, and where they differ. Much Python will be discussed, and many pretty pictures will be drawn.\\n                ',\n",
       "  u'\\nAs an engineer, analyst, or scientist, sharing your work with someone outside of your immediate team can be a challenge. End-users embody many roles with a wide range of technical skill and often times no familiarity with Python or the command line. Findings, key results, and models are frequently boiled down to static graphs, tables, and figures presented in short reports or slideshow presentations. However, engaging research and data analysis is interactive, anticipating the users\\u2019 questions and giving them the tools to answer those questions with a simple and intuitive user interface.\\nBrowser based applications are an ideal vehicle for delivering these types of interactive tools, but building a web app requires setting up backend applications to serve up content and creating a UI with languages like HTML, CSS, and JavaScript.  This is a non-trivial task that can be overwhelming for anyone not familiar with the web stack.\\nSpyre is a web application framework meant to help those python developers that might have little knowledge of how web applications works, much less how to build them. Spyre takes care of setting up both the front and back-end of your web application. It uses CherryPy to handle HTTP request logic and Jinja2 to auto-generate all of the client-side nuts and bolts, allowing developers to quickly move the inputs and outputs of their python modules into a browser based application. Inputs, controls, outputs, and the relationships between all of these components are specified in a python dictionary. The developer need only define this dictionary and override the methods needed to generate content (text, tables, and plots).\\nWhile Spyre apps are launched on CherryPy\\u2019s production-ready server, Spyre\\u2019s primary goal is to provide a development path to simple light-weight apps without the need for a designer or front-end engineer. For example, Spyre can be used for\\r\\n\\nrapid prototyping and building MVPs \\ndata exploration\\ndeveloping educational resources\\nbuilding monitoring tools\\npresenting interactive scientific or analytical results to a non-technical audience\\n\\r\\n\\r\\njust to name a few.\\r\\n\\r\\nAt Next Big Sound we recently used Spyre to build an app to visualize the effects of sampling parameter values on the volume of tweets collected from one of our data providers (see screenshot below).\\r\\n\\r\\n\\nWeb applications like this can turn a highly technical process into a simple tool that can be used by anyone with any level of technical skill.\\nAfter you\\u2019ve finished the foundational parts of your project -- the data collection, data cleaning, exploration, modeling, and analysis -- Spyre provides a quick and simple way to package the results into an interactive web application that can be viewed by the rest of the world.\\n',\n",
       "  u'\\n                    A recent article in the New York Times estimates that data scientists spend somewhere between %50 and %80 of their time \"collecting and preparing unruly digital data\" before they ever get to the analysis.  Data is often badly labeled, inconsistently sampled, incorrect in strange places, missing, and otherwise contains a whole host of errors, leading to the \"garbage in, garbage out\" problem. \\r\\n\\r\\nWhile detecting the myriad ways in which the data is broken can sometimes be difficult, traditional visualization techniques, exploratory data analytics, and cluster analysis can help.  This talk will discuss some of the typical methods for sanity checking small data sets: visualization, simple statistics, and some basic combinations of the two. This talk will then veer into some machine learning techniques for exploring the underlying structure of larger data sets to verify the occurrence of known patterns and to detect outliers that could be due to errors rather than the occurance of something interesting.\\n                ',\n",
       "  u\"\\n                    This talk is aimed at scikit-learn novices who have built their first classifier with out-of-the-box features, been disappointed with the results, and wondered what to do for a next step. As a case study, I'll use a personal project to classify streetnames in Singapore according to their language of origin, which I then turned into a colour-coded streetmap. We'll build a baseline classifier using OpenStreetMap data, GeoPandas, and scikit-learn, then explore how to add your own feature Pipelines and how to tune hyperparameters using GridSearchCV, including how to pick a parameter grid. Lastly we'll plot the map and review which method of improving the baseline classifier worked best: more data, adding features, hyperparameter tuning, or swapping out classifiers?\\n                \",\n",
       "  u'\\n                    Open discussion for all  those interested in healthcare analytics.\\n                ',\n",
       "  u'\\nChoosing hardware for big data analysis is difficult because of the many options and variables involved.\\r\\nThe problem is more complicated when you need a full cluster for big data analytics.\\nThis session will cover the basic guidelines and architectural choices involved in choosing analytics hardware for Spark and Hadoop. I will cover processor core and memory ratios, disk subsystems, and network architecture. This is a practical advice oriented session, and will focus on performance and cost tradeoffs for many different options.\\n',\n",
       "  u'\\n                    It\\'s a common story. Software developers are working hard to get a project off the ground. They set up logging to catch errors, but when they go to do data science down the road, they find that their logs are missing crucial information. A few days up front doing a \"data audit\" could have saved them time, made them money, and helped them gain insight into their customers.\\r\\n\\r\\nThis talk will give you the toolkit you need to collect data properly, years before you bring on a data scientist. You will be able to do your own data audit, even if you don\\'t know anything about data science. You will learn the three major things to check\\u2014is your data complete? Is it correct? And is it connectable?\\r\\n\\r\\nYou\\'ll also get a concise list of tools\\u2014Python and the command line\\u2014to quickly look through your data to get some intuition for what\\'s hiding in those CSVs.  Be a hero to your future data team. \\r\\n\\n                ',\n",
       "  u\"\\n                    Julia is a new language for scientific computing, which combines the high-level abstractions and dynamic interactivity of languages like Python with the performance of low-level languages like C. It has been gaining traction as a faster alternative to NumPy, Matlab or R, and as a more productive alternative to C or C++. Julia is particularly relevant when both expressiveness and performance are paramount \\u2013 in areas like machine learning, \\u201cbig statistics\\u201d, data mining and linear algebra. But a major challenge for any young programming language is the lack of a large ecosystem of mature libraries and tools. To overcome this difficulty, Julia is \\u201cbootstrapping\\u201d off of the Python ecosystem, both by making it easy to call Python code and also by exploiting infrastructure such as IPython/Jupyter. Since it would be rude to take and give nothing back, we've also made it possible to call Julia libraries from Python, allowing Julia to be (yet) another alternative when CPython isn't quite fast enough. This talk will be full of live coding, with all the fun and tantalizing possibility of disaster that entails.\\n                \",\n",
       "  u'\\n                    The goal of this project is to make it easy to execute both registered and arbitrary functions on a remote server, just as easily as executing code locally. Passes back all stdout and stderr and exceptions back to the client so working remotely feels like working locally. Supports server side remote data objects, so large results can be left on the server/cluster, but interacted with easily.\\n                ',\n",
       "  u'\\nUsing machine learning to beat your friends in a NFL confidence pool.\\nBetting spreads provide a consistent and robust mechanism for encapsulating the variables and predicting outcomes of NFL games.  In a weekly confidence pool, spreads also perform very well as opposed to intuition-based guessing and supposed knowledge from years of being a fan.  We present some attempts and analysis to use machine learning in order to make improvements on the spread method of ranking winners on a weekly basis.\\n',\n",
       "  u'\\n\\nDo you fear \"Out of Memory\" errors?\\nDo you wish that there was a way to reduce the memory footprint of your data?\\nDo you cry tears of joy when you can crunch your data on an m1.small rather than an m1.xlarge?\\nIf so, this talk is for you!\\nIn this talk, I will go over conventional and unconventional techniques that I\\'ve used to reduce the size of my data. I will go over traditional dimensionality reduction techniques such as PCA and NMF. In addition, I will go over more esoteric approaches such as Random Projection.\\nBy the end of this talk, you will be able to understand when and how to appropriately apply these techniques to your own data.\\n',\n",
       "  u\"\\n                    MongoDB is a scalable, flexible way to store large data sets. Python and NumPy provide a comprehensive toolkit for analysis. But they don't work well together: the official Python driver for MongoDB is inefficient at loading MongoDB data into NumPy arrays.\\r\\n\\r\\nEnter Monary. It's a fast, specialized driver written in C, that copies data directly from MongoDB documents into NumPy arrays.\\r\\n\\r\\nThis talk will provide an introduction Monary, and practical demonstrations of Monary's speed benefits and uses. We'll use Monary to store data about millions of New York taxi rides in MongoDB, and we'll analyze it using scientific Python tools to find surprising outcomes about stingy riders and long-suffering drivers.\\r\\n\\r\\nThe combination of MongoDB, Monary, and NumPy is very powerful: it's a data analysis pipeline that is scalable, convenient, and completely free and open source.\\n                \",\n",
       "  u'\\n                    Engineering new therapeutics is hard--and getting harder.  Accurate physical modeling promises to improve the way we design drugs, but the necessary open source infrastructure is lacking.  The Omnia Consortium---a collaboration of multiple academic laboratories working on physical modeling tools for drug discovery---is producing a suite of open-source tools for understanding drugs, proteins, and the biomolecular mechanisms of disease.  Our Python-centric software stack is uses Python, Cython, C++, and CUDA/OpenCL to achieve bleeding-edge performance.  Part of our stack (OpenMM) is also implemented on the Folding@Home distributed computing project and currently runs on tens of thousands of high-end GPUs around the world, producing over 18PFLOP/s of computational power.  \\r\\n\\r\\nIn our talk, we will introduce biophysical simulation and its application to understanding mechanisms of disease and its potential for designing new therapeutics.  We will discuss the challenges in building robust tools for automating and scaling up biophysical simulations, compared with the relatively mature tools already available for modern data science.  We will describe some of the tools in our stack (OpenMM, MDTraj, Mixtape, Yank) and how we use the conda packaging environment to facilitate distribution of our domain-specific code.  Finally, we will discuss our plans to improve physical models and study drug resistance using iterative cycles of modeling and automated biophysical experiments performed at Memorial Sloan-Kettering Cancer Center.  \\n                ',\n",
       "  u\"\\nData Science is a comparatively new field and as such it is constantly changing as new techniques, tools, and problems emerge every day. Traditionally education has taken a top down approach where courses are developed on the scale of years and committees approve curricula based on what might be the most theoretically complete approach. This is at odds however with an evolving industry that needs data scientists faster than they can be (traditionally) trained.\\nIf we are to sustainably push the field of Data Science forward, we must collectively figure out how to best scale this type of education. At Zipfian I have seen (and felt) first hand what works (and what doesn't) when tools and theory are combined in a classroom environment. This talk will be a narrative about the lessons learned trying to integrate high level theory with practical application, how leveraging the Python ecosystem (numpy, scipy, pandas, scikit-learn, etc.) has made this possible, and what happens when you treat curriculum like product (and the classroom like a team).\\n\",\n",
       "  u'\\n                    Coming soon.\\n                ',\n",
       "  u'\\nThe talk illustrates how selective-search object recognition and the latest deep-learning object identification algorithm was applied to solving the problem of image cropping.\\nHow can you identify the most important part of the image that must not be cropped out when shown as a thumbnail? This is a problem faced by various media and e-commerce where space for the image can be of various sizes and the best portion of the original image must be preserved to maximize effectiveness.\\nSelective search is a new method proposed by Uijlings (U. Amsterdam) et al. which significantly improved on the accuracy of object recognition over the previous exhaustive search methods. This allows us to use advanced methods such as Convolutional Neural Network, a.k.a. deep learning object identification, to identify interesting objects contained in an image. With this, interesting parts of the image can be preserved in the process of cropping producing effective thumbnails.\\nThe code for selective search is available for matlab only while the deep learning algorithm, Caffe, is available with a Python wrapper. We will illustrate how Python is best suited to putting together the result of cutting edge research in order to solve complex data processing problems.\\n',\n",
       "  u'\\n                    PyCassa is a python wrapper for Cassandra, the open source non-relational database. Cassandra has gained a lot of momentum recently because of big data problems and its ability to scale well. Rather than adding more memory to a machine and having to change the database schema, you just add more servers which forms a cluster of \"nodes.\" In this talk I\\'ll go over setting up Cassandra in windows and connecting to it with PyCassa.\\n                ',\n",
       "  u'\\n                    Applications often have back-end data stores that developers need to query. In this situation, you may first think of SQL, whether using a lightweight tool like SQLite or a full-fledged ORM like SQLAlchemy. However, for many data storage tasks, SQL is more than we really need.\\r\\n\\r\\nThis talk presents an alternative approach that directly employs some of Python\\u2019s most powerful language features. Using a distributed key-value store, we can make our data persistent with an interface similar to a Python dictionary. Python then gives us a number of tools \"out of the box\" that we can use to form queries:\\r\\n\\r\\n* generators for memory-efficient data retrieval;\\r\\n* itertools to filter and group data;\\r\\n* comprehensions to assemble the query results.\\r\\n\\r\\nTaken together, these features give us a really powerful query capability, and most of it is straight Python. By the end of this talk, you\\'ll have an understanding of some sophisticated but accessible Python features that are immediately useful for manipulating data.\\r\\n\\n                ',\n",
       "  u'\\n                    Americans have over $10 Trillion in 401(K) and IRAs. These assets are mostly left in the default allocation of expensive funds as chosen by employers or brokerages. As a result account holders pay tens of billions of dollars in unnecessary fees for inferior returns. We will explore a simple framework for building retirement portfolios using readily available python packages and publicly available data. We will first use constraint programming to explore the universe of available portfolios. We will then build a simple portfolio rebalancer.\\r\\n\\n                ',\n",
       "  u\"\\n                    Over the past five years the drop in DNA sequencing costs has quickly transformed our ability to understand the genome. In this tutorial, we will introduce you to pyPLINK/SEQ, a toolset for working with human genetic variation data, in different ways, as: a library for accessing genomic information, connecting to reference databases, and applying statistical models; and as a library for interpreting your own genomes. We will explore the speaker's genome and give a thorough presentation of pyPLINK/SEQ functionalities. All contents of the talk will be supplemented with code available on GitHub.\\n                \",\n",
       "  u'\\n                    ---\\n                ',\n",
       "  u'\\nVisualizations are windows into datasets: they can help generate hypotheses, aid combinatory play to discover trends, and cement insight by providing structure and context.\\nThis talk will touch on the current state of the Python visualization ecosystem, offer some thoughts on iteratively building visualizations, then launch into a data-driven exploration of visualization techniques grounded in the NYC taxicab dataset.\\n',\n",
       "  u\"\\n(or: how to only forget some things about your ML models instead of literally everything)\\nYou're writing a classifier. So you trained 10 decision trees in October, with several sets of training data, different maximum depths, different scalings, and different features. Some of the experiments went better than others! Now it's November, and you want to go back to the project and start using one of these models. But which one?!\\nAt Stripe, we train models to automatically detect and block fraudulent transactions in real time. We build a lot of models, and we need a way to keep track of all kinds of information about them. I'll talk about a simple tool we built to:\\n\\n keep track of a few evaluation metrics for each model (precision vs recall, ROC curve)\\n remember which features, training data, and parameters we used\\n choose which threshold to use\\n\\nThis functions as a lightweight lab notebook for ML experiments, and been incredibly useful for us (as mere humans). Having a consistent way to look at the results of our experiments means we can compare models on equal footing. No more notes, no more forgetting, no more hand-crafted artisanal visualizations. [1]\\n[1] You're still allowed to make hand-crafted artisanal visualizations if you want.\\n\",\n",
       "  u'\\n                    As the popularity of machine learning techniques spreads to new areas of industry and science, the number of potential machine learning users is growing rapidly.  While the fantastic scikit-learn library is widely used in the Python community for tackling such tasks, there are two significant hurdles in place for people working on new machine learning problems: \\r\\n\\r\\n\\nScikit-learn requires writing a fair amount of boilerplate code to run even simple experiments.\\nObtaining good performance typically requires tuning various model parameters, which can be particularly challenging for beginners.\\n\\nSciKit-Learn Laboratory (SKLL) is an open source Python package, originally developed by the NLP & Speech group at the Educational Testing Service (ETS), that addresses these issues by providing the ability to run scikit-learn experiments with tuned models without writing any code beyond what generates the features. This talk will provide an overview of performing common machine learning tasks with SKLL, and highlight some of the new features that are present as of the 1.0 release.\\n                ',\n",
       "  u'\\n                    eXtensible Business Reporting Language (XBRL) is now required to be used by all publicly traded companies filing annual (10-K) and quarterly (10-Q) financial reports with the Securities and Exchange Commission (SEC). Popular finance websites such as Google and Yahoo Finance and academic databases, such as Compustat, aggregate financial terms to create the financial statements displayed. Obtaining XBRL tagged financial statements allows investors to get machine readable financial data directly from the company and analyze it at the granular level. In this tutorial, I show how to obtain XBRL financial statements quickly and easily from the SEC website, parse them using BeautifulSoup and the lxml engine, and analyze and visualize them using pandas and matplotlib and run regressions using the statsmodels library.  Time permitting, I will also discuss the limitations of XBRL and how the opensource community can help.\\n                ',\n",
       "  u'\\n                    Open BOF. Submit topic to admin@pydata.org.\\n                ',\n",
       "  u'\\n                    ----\\n                ',\n",
       "  u'\\n                    The Beaker Notebook is a new open source tool for collaborative data science.  Like IPython, Beaker uses a notebook-based metaphor for idea flow.  However, Beaker was designed to be polyglot from the ground up. That is, a single notebook may contain cells from multiple different languages that communicate with one another through a unique feature called autotranslation. You can set a variable in a Python cell and then read that variable in a subsequent R cell, and everything just works \\u2013 magically.  Beaker comes with built-in support for Python, R, Groovy, Julia, and Javascript.  In addition, Beaker also supports multiple kinds of cells for text, like HTML, LaTeX, Markdown, and our own visualization library that allows for the plotting of large data sets. This talk will motivate the design, review the architecture, and include a live demo of Beaker in action.\\n                ',\n",
       "  u\"\\n                    Thousands of planets outside the solar system have been\\r\\ndiscovered using time series data from NASA's Kepler mission but not a\\r\\nsingle one is a true Earth twin. I'm working to discover Earth 2.0\\r\\nusing an open dataset from NASA and custom-built, high-performance\\r\\ntools in Python. I will sketch the problem and introduce the resulting\\r\\nPython module (called George; http://dfm.io/george) for doing time\\r\\nseries analysis with Gaussian Processes (GPs). The core algorithm\\r\\n(developed in collaboration with applied mathematicians at NYU) allows\\r\\nthis code to compute GPs on general large datasets that were\\r\\npreviously intractable. This method isn't just applicable in astronomy\\r\\nso I'll demonstrate how this package can be incorporated into the\\r\\nstandard scientific Python stack and compare it to other GP\\r\\nimplementations.\\n                \",\n",
       "  u'\\n                    Coming Soon\\n                ',\n",
       "  u\"\\nSQL is still the bread-and-butter of the data world, and data analysts/scientists/engineers need to have some familiarity with it as the world runs on relational databases.\\nWhen first learning pandas (and coming from a database background), I found myself wanting to be able to compare equivalent pandas and SQL statements side-by-side, knowing that it would allow me to pick up the library quickly, but most importantly, apply it to my workflow.\\nThis tutorial will provide an introduction to both syntaxes, allowing those inexperienced with either SQL or pandas to learn a bit of both, while also bridging the gap between the two, so that practitioners of one can learn the other from their perspective. Additionally, I'll discuss the tradeoffs between each and why one might be better suited for some tasks than the other.\\n\",\n",
       "  u'\\n                    As a data scientist I frequently need to create web apps to provide interactive functionality, deliver data APIs or simply publish results. It is now easier than ever to deploy your data driven web app by using cloud based application platforms to do the heavy lifting. \\r\\n\\r\\nCloud Foundry is an open source public and private cloud platform that supports Python based applications (along with other languages) and enables simple app deployment, scaling and connectivity to data services like PostgreSQL, Redis and Cassandra. \\r\\n\\r\\nTo make use of the full PyData stack I have created a Heroku-style buildpack which uses conda for package management. This means for example that you can get a private IPython Notebook server up and running in seconds.\\r\\n\\r\\nIn this talk I want to show you how to\\r\\n\\ndeploy your first app using Cloud Foundry,\\nconnect to databases and other data services,\\nuse PyData packages with a Heroku-style buildpack,\\nfind public and private Cloud Foundry installation options.\\n\\n',\n",
       "  u'\\n                    I will talk about how we are using data science to help transform OpenTable into a local dining expert who knows you very well, and can help you and others find the best dining experience wherever we travel! This entails a whole slew of tools from natural language processing, recommendation system engineering, sentiment analysis, and predictions based on internal and external signals that have to work in synch to make that magical experience happen.  I will touch upon how the rich Python ecosystem of tools like Pandas, scikit-learn, nltk, gensim and matplotlib has helped in almost all stages of this venture. \\n                ',\n",
       "  u'\\nAttendees will learn how to use data to generate opportunities for their organizations, and rank some type of risks in scales similar to the ones used by rating agencies, opening profitable risk transfer opportunities.\\nThis presentation documents a simplified analytical approach developed by the author to illustrate key elements in the design of a parametric catastrophic bond, a type of Insurance Linked Security (ILS). The model was developed in order to 1) help potential clients of an investment bank understand advantages and disadvantages of insurance linked securities vs. traditional insurance, 2) help government decision makers draft policies to accommodate the product in their risk management efforts, 3) expand potential market of buyers of ILS by sharing analytical work to Rating Agencies and CDO managers in a reproducible way, 4) Help win structuring mandates for investment banks.\\nAlthough the original model was developed using a combination of C/C++, Visual Basic, ActiveX, and an Excel front end, this presentation will show a modern approach that will use:\\n\\nIPython Notebook to collect and explore a dataset that includes 100 years of earthquake data from the USGS, around the geographical location covered by the bond.\\nPandas for time series analysis of earthquake historical data.\\nMonte Carlo Error Propagation to generate simulated earthquakes and calculate exceedance loss probabilities on life on bond.\\nSci-kit learn, and/or other tools to select best parameters to fit target credit rating, bond parameters, and predict spread over LIBOR.\\n',\n",
       "  u'\\n                    --------\\n                '],\n",
       " 'sv2014': [u'\\n',\n",
       "  u'\\n                    The The Greater Plains Collaborative (GPC) is a new network of 10 leading medical centers in 7 states working to improve healthcare delivery and advance research by mining electronic medical records and patient registries. To do this, we must de-identify and securely migrate patient data from heterogeneous formats (e.g. Clarity, IDX, NAACCR) to our data warehouse platform (HERON) which is built on top of I2B2. Task dependencies in the complex network of python scripts that wrap our SQL code is managed via paver, permitting a robust, modular, and maintainable architecture. In the process, we developed new python tools for generating dependency graphs from SQL code and for integrating R and RedCap into our analytical pipeline. Moreover, by adapting our python code to work across multiple member institutions we have started moving toward a generic workflow for building, testing, documenting, and deploying medical informatics research data warehouses.\\n                ',\n",
       "  u\"\\n                    To a lot of people, Facebook is a website for interacting with friends and family. It's also a giant treasure trove of rich, fascinating data. Facebook has made accessing large data sets easier in part by releasing open source technologies like Presto and Tornado.\\r\\n \\r\\nIn this talk Jason will explain how he turns terabytes of data into compelling, interactive, data-driven applications whose purposes run the gamut from internal insights to debugging, to beautiful visualizations. He will show examples of each kind of visualization, and talk about the architecture behind each and how Python is integral to tying all the pieces together, quickly. Jason will cover several open source technologies like crossfilter.js, dc.js, d3.js and 0mq.\\n                \",\n",
       "  u'\\n                    The ad targeting team at Yelp is tasked with presenting the most relevant advertisements to millions of users every day. In this talk I will discuss some of the technical problems facing the ads team, the models, approaches, and workflow we have adopted for targeting, and some of the Python tools we use within that workflow.\\n                ',\n",
       "  u'\\n',\n",
       "  u'\\n                    Bokeh is a Python interactive visualization library for large datasets that natively uses the latest web technologies. Its goal is to provide elegant, concise construction of novel graphics in the style of Protovis/D3, while delivering high-performance interactivity over large data to thin clients. This tutorial will walk users through the steps to create different kinds of interactive plots using Bokeh. We will cover using Bokeh for static HTML output, the IPython notebook, and plot hosting and embedding using the Bokeh server.\\n                ',\n",
       "  u'\\n                    Bokeh is a Python interactive visualization library for large datasets that natively uses the latest web technologies. Its goal is to provide elegant, concise construction of novel graphics in the style of Protovis/D3, while delivering high-performance interactivity over large data to thin clients. This tutorial will walk users through the steps to create different kinds of interactive plots using Bokeh. We will cover using Bokeh for static HTML output, the IPython notebook, and plot hosting and embedding using the Bokeh server.\\n                ',\n",
       "  u\"\\nSometimes the greatest challenge in working with data is getting data to work with in the first place. In this talk I'll take the audience through the process of building a toolset that can be used to launch a virtual army of data collectors that can help get large volumes of useful data quickly. (No live coding or slides full of code will be presented, we're going to deal with concepts and I'll direct the audience to a github repository with examples at the end of the talk.)\\nSince it's the most widely available and a common source of valuable information, we'll focus primarily on gathering data from the web, although the principals could certainly be used to churn through other data sources as well.\\nWe'll start by examining a simple web-scraper and the limitations of a singular, linear process. We'll then progress through the concepts of threading and concurrency, and all the way through multiprocessing. (Again, very little code, mostly graphics to help improve understanding of the concepts.)\\nOnce we reach this point, we can discover together that there are limitations to this approach, even on super fast multi-core machines with tons of RAM. Network bottlenecks, ISP issues, and the possibility of creating an inadvertent Denial of Service Attack, not to mention the fact that you may not be able to use the computer in question while the data harvesting is going on.\\nFrom here we can consider the idea of using an inexpensive virtual machine running somewhere else (such as AWS) to do our bidding and harvest data while we wait. I'll show how some very simple tools like Vagrant and Fabric can be combined to make running code on a remote machine simple.\\nWe'll still have some limitations though. Moving everything to a remote machine solves some of our original problems, but in the end it's still one machine and even the most powerful machine is going to have limits.\\nI'll present ways that we can spawn a network (an Army!) of virtual machines that can all work together to complete the task at hand, and have that power available to run any python code we desire.\\n\",\n",
       "  u'\\n',\n",
       "  u'\\n                    Big Data brings with it particular challenges in any language, mostly in performance. This talk will explain how to get immediate speedups in your Python code by exploiting both timeless programming techniques and fixes specific to Python. We will cover:\\r\\n\\r\\nI. Amongst Our Weaponry\\r\\n 1. How to Time and Profile Python\\r\\n 2. Extracting Loop invariants: constants, lookup tables, even methods!\\r\\n 3. Caching: memoization and heavier things\\r\\n\\r\\nII Gunfight at the O.K. Corral in Morse Code\\r\\n 1. Python functions vs C functions\\r\\n 2. Vector operations: NumPy\\r\\n 3. Reducing calls: loops, generators, recursion\\r\\n\\r\\nIII. The Semaphore Version of Wuthering Heights\\r\\n 1. Using select instead of Queue\\r\\n 2. Serialization overhead\\r\\n 3. Parallelizing work\\r\\n\\n                ',\n",
       "  u'\\n                    Modern Data Science is enabling NASA\\'s engineers uncover actionable information from our \"dark\" data coffers.  From starting small to operating at scale, Rob will discuss applications in telemetry, workforce analytics and liberating data from the Mars Rovers.  Tools include iPython, Pandas, Boto and more.\\n                ',\n",
       "  u\"\\n                    Speakers:\\r\\nAlex Poliakov, Paradigm4 Solutions Architect\\r\\nChris Beaumont, Ph.D, Harvard Center for Astrophysics\\r\\n\\t\\r\\nWith the emergence of the Internet of Everything in the commercial and industrial worlds and with the advances in device and instrument technologies in the science world, there is an urgent need for analysts to be able to work more easily with extremely large and diverse data sets.  While the Python ecosystem thrives at in-memory data analysis, what is missing is a high performance, seamless integration with a persistent data store.  When dealing with larger datasets, data scientists are forced to think more carefully about low-level bookkeeping details -- this inhibits discovery. SciDB-Py addresses this problem by providing a high-level interface to SciDB -- a high performance database optimized for array-based computation at scale. \\r\\n\\r\\nArray-based data \\u2014 like much of the time-stamped device data, geospatial data, and genomic data central to many applications \\u2014 often do not fit neatly into classic relational or key-value paradigms of popular database architectures. SciDB is a new database platform that is built around large multidimensional arrays. It provides efficient distributed storage and fast array-oriented computations, from simple slices and joins to more involved parallel linear algebraic operations like singular value decomposition. Like traditional databases, SciDB also preserves data integrity and allows concurrent, multi-user access.\\r\\n\\r\\nSciDB-Py provides a Python interface to SciDB\\u2019s storage and computation capabilities. With SciDB-Py, data scientists can more naturally store and manipulate extremely large datasets, using a familiar NumPy-like interface and Pythonic idioms. Users can also seamlessly transfer subsets of SciDB datasets into Python to leverage analysis and visualization libraries like matplotlib, scikit-learn, and pandas.\\r\\n\\r\\nWe'll demonstrate working with time-series, geospatial, and genomics data in SciDB from Python, presenting basic examples that illustrate SciDB's native analytics and math capabilities including aggregation and data decimation, regression and generalized linear models, covariance matrices, and singular value decomposition. \\n                \",\n",
       "  u\"\\nOften times there exists a divide between data teams, engineering, and product managers in organizations, but with the dawn of data driven companies/applications, it is more prescient now than ever to be able to automate your analyses to personalize your users experiences. LinkedIn's People you May Know, Netflix and Pandora's recommenders, and Amazon's eerily custom shopping experience have all shown us why it is essential to leverage data if you want to stay relevant as a company.\\nAs data analyses turn into products, it is essential that your tech/data stack be flexible enough to run models in production, integrate with web applications, and provide users with immediate and valuable feedback. I believe Python is becoming the lingua franca of data science due to its flexibility as a general purpose performant programming language, rich scientific ecosystem (numpy, scipy, scikit-learn, pandas, etc.), web frameworks/community, and utilities/libraries for handling data at scale. In this talk I will walk through a fictional company bringing it's first data product to market. Along the way I will cover Python and data science best practices for such a pipeline, cover some of the pitfalls of what happens when you put models into production, and how to make sure your users (and engineers) are as happy as they can be.\\n\",\n",
       "  u'\\n                    I will describe data science efforts at Berkeley, with a particular focus on teaching and the new Berkeley Institute for Data Science (BIDS), funded by the Moore and Sloan Foundations. BIDS will be a space for the open and interdisciplinary work that is typical of the SciPy community. In the creation of BIDS, open source scientific tools for data science, and specifically the SciPy ecosystem, played an important role. \\n                ',\n",
       "  u\"\\n                    In this talk, I'll discuss the architecture of the DataPad system and how we\\r\\nare leveraging components in the scientific Python ecosystem (along with a wee\\r\\nbit of JavaScript) to craft a modern business intelligence and analytics\\r\\nsystem. The basic technical architecture of the Badger high performance\\r\\nanalytical query engine (inspired by experiences Wes had building pandas) will\\r\\nalso be included.\\n                \",\n",
       "  u'\\n                    At Facebook, data is used to gain insights for existing products and drive development of new products. In order to do this, engineers and analysts need to seamlessly process data across a variety of backend data stores. Dataswarm is a framework for writing data processing pipelines in Python. Using an extensible library of operations (e.g. executing queries, moving data, running scripts), developers programmatically define dependency graphs of tasks to be executed. Dataswarm takes care of the rest: distributed execution, scheduling, and dependency management. Talk will cover high level design, example pipeline code, and plans for the future.\\r\\n\\n                ',\n",
       "  u'\\n                    PlanOut is an open-source framework developed at Facebook for designing, implementing, and logging online experiments. This tutorial teaches you how to create both simple A/B tests and complex experiments in Python, as well as best practices for managing and analyzing experiments. We will also be taking a deep dive into how PlanOut is organized and can be extended to fit into production environments. For more information, see the PlanOut page on Github http://facebook.github.io/planout\\n',\n",
       "  u'\\n                    Ferry is a Python-based, open-source tool to help developers share and run big data applications. Users can provision Hadoop, Cassandra, GlusterFS, and Open MPI clusters locally on their machine using YAML and afterwards distribute their applications via Dockerfiles. These capabilities are useful for data scientists experimenting with big data technologies, developers that need an accessible big data development environment, or for developers simply interested in sharing their big data applications. \\r\\n\\r\\nIn this presentation, I\\u2019ll introduce you to Docker, show you how to create a simple big data application in Ferry, and discuss ways the Python community can contribute to the open-source project. I\\u2019ll also discuss future directions for Ferry with a focus on better application sharing and operational deployments. \\n                ',\n",
       "  u'\\n',\n",
       "  u\"\\n                    What are generators and coroutines in Python? What additional conceptualisations do they offer, and how can we use them to better model problems? This is a talk I've given at PyCon Canada, PyData Boston, and PyTexas. It's an intermediate-level talk around the core concept of generators with a lot of examples of not only neat things you can do with generators but also new ways to model and conceptualise problems.\\n                \",\n",
       "  u'\\n                    Gradient Boosted Regression Trees (GBRT) is powerful a statistical learning technique with applications in a variety of areas, ranging from web page ranking to environmental niche modeling -- it is a key ingredient of many winning solutions in data-mining competitions such as the Netflix Prize, the GE Flight Quest, or the Heritage Health Price.\\r\\n\\r\\nI will start with a brief introduction to the GBRT model -- focusing on intuition rather than mathematical formulas. The majority of the tutorial will be dedicated to an in depth discussion how to apply GBRT successfully in practice using scikit-learn. We will cover important topics such as regularization, model tuning, and model interpretation that should significantly improve your score on Kaggle.\\n                ',\n",
       "  u'\\n                    This talk will walk through what the US government has done in terms of spying on US citizens and foreigners with their PRISM program, then walk through how to do exactly that with Python.\\n                ',\n",
       "  u'\\n                    This talk will walk through what the US government has done in terms of spying on US citizens and foreigners with their PRISM program, then walk through how to do exactly that with Python.\\n                ',\n",
       "  u'\\n                    In this talk, we show how and why AdRoll built a custom,\\r\\nhigh-performance data warehouse in Python which can handle hundreds of\\r\\nbillions of data points with sub-minute latency on a small cluster of\\r\\nservers.\\r\\n\\r\\nThis feat is made possible by a non-trivial combination of compressed\\r\\ndata structures, meta-programming, and just-in-time compilation using\\r\\nNumba, a compiler for numerical Python. To enable smooth\\r\\ninteroperability with existing tools, the system provides a standard\\r\\nSQL-interface using Multicorn and Foreign Data Wrappers in PostgreSQL.\\n                ',\n",
       "  u'\\nHustle was born out of the need to efficiently process and query billions of event records per day.  It was built on the proven petabyte scale Disco MapReduce framework, and the surreally fast LMDB persistent data layer.\\r\\n\\r\\nHustle sports the following feature set:\\r\\n\\ncolumn oriented - super fast queries\\nappend-only event semantics\\ndistributed insert - massive write loads\\ncompressed - save up to 50% memory/disk space\\nPython DSL query language\\n\\n',\n",
       "  u'\\n                    IPython provides an architecture for interactive computing. The IPython Notebook is a web-based interactive computing environment for exploratory and reproducible computing. With the IPython Notebook, users create documents, called notebooks, that contain formatted text, figures, equations, programming code, and code output.\\r\\n\\r\\nAs of version 2.0, the IPython Notebook includes interactive JavaScript widgets.  These widgets provide a way for users to interact with UI controls in the browser that are tied to Python code in running in the kernel. We will begin by covering the highest-level API for these widgets, \\u201cinteract,\\u201d which automatically builds a user interface for exploring a Python function. Next we will describe the lower-level widget objects that are included with IPython: sliders, text boxes, buttons, etc. However, the full potential of the widget framework lies with its extensibility.  Users can create their own custom widgets using Python, JavaScript, HTML and CSS. We will conclude with a detailed look at custom widget creation.\\r\\n\\r\\nPython Dependencies:\\r\\nIPython 2.0 or latest stable running on Anaconda or Canopy\\r\\n\\r\\nAdditional Comments:\\r\\nAttendees should already know Python and be familiar with the IPython Notebook. Some JavaScript/HTML/CSS experience will also be helpful.\\n                ',\n",
       "  u'\\n                    IPython is an exciting, fast-moving project, which just had a major release. IPython 2.0 introduces lots of cool new features for exploratory computing, such as interactive widgets, a new, more customizable, modal user interface, notebook security, and directory navigation. With 2.0 released, we are already hard at work on our next major milestones, including updates to better integrate languages other than Python, a multi-user notebook server, static widget export, and more. I will demonstrate some of the new features,  discuss future plans, and examine some of our experiences managing a popular open source project with a small team of funded developers.\\n                ',\n",
       "  u'\\nxray is a new Python package for labeled array data. It aims to provide a data analysis toolkit as efficient and powerful as pandas but designed for homogeneous N-dimensional arrays instead of tabular data. Indeed, many of its internals are built on pandas (most notably, fast indexing), and its interface mirrors pandas for features such as label-based indexing, data alignment and group-by operations.\\r\\n\\r\\nxray implements two data-structures that are missing in pandas: the DataArray, an extended array object with labeled coordinates and dimensions, and the Dataset, a dictionary-like container for manipulating a collection of DataArrays aligned along shared dimensions. The labeled dimensions of the DataArray allow for array alignment (e.g., broadcasting) and operations (e.g., sum) based on dimension names instead of array shapes and axis numbers. The data model is based on Unidata\\u2019s Common Data Model for self-describing scientific datasets, which is widely used in the geosciences.\\n                ',\n",
       "  u\"\\n                    In machine learning, clustering is a good way to explore your data and pull out patterns and relationships. Scikit-learn has some great clustering functionality, including the k-means clustering algorithm, which is among the easiest to understand. Let's take an in-depth look at k-means clustering and how to use it. This mini-tutorial/talk will cover what sort of problems k-means clustering is good at solving, how the algorithm works, how to choose k, how to tune the algorithm's parameters, and how to implement it on a set of data.\\r\\n\\n                \",\n",
       "  u\"\\n                    This presentation will give a brief overview of machine learning, the k-nearest neighbor algorithm and scikit-learn. Sometimes developers need to make decisions, even when they don't have all of the required information. Machine learning attempts to solve this problem by using known data (a training data sample) to make predictions about the unknown. For example, usually a user doesn't tell Amazon explicitly what type of book they want to read, but based on the user's purchasing history, and the user's demographic, Amazon is able to induce what the user might like to read.\\r\\n\\r\\nScikit-learn makes use of the k-nearest neighbor algorithm and allows developers to make predictions. Using training data one could make inferences such as what type of food, tv show, or music the user prefers. In this presentation we will introduce the k-nearest neighbor algorithm, and discuss when one might use this algorithm.\\n                \",\n",
       "  u\"\\n                    This presentation will give a brief overview of machine learning, the k-nearest neighbor algorithm and scikit-learn. Sometimes developers need to make decisions, even when they don't have all of the required information. Machine learning attempts to solve this problem by using known data (a training data sample) to make predictions about the unknown. For example, usually a user doesn't tell Amazon explicitly what type of book they want to read, but based on the user's purchasing history, and the user's demographic, Amazon is able to induce what the user might like to read.\\r\\nScikit-learn makes use of the k-nearest neighbor algorithm and allows developers to make predictions. Using training data one could make inferences such as what type of food, tv show, or music the user prefers. In this presentation we will introduce the k-nearest neighbor algorithm, and discuss when one might use this algorithm.\\r\\n\\n                \",\n",
       "  u\"\\n                    In 2004, at the Sixth Symposium on Operating System Design and Implementation, Jeffrey Dean and Sanjay Ghemawat, a couple of engineers working for Google, published a paper titled \\u201cMapReduce: Simplified Data Processing on Large Clusters\\u201d that introduced the world to a simple, yet powerful heuristic for processing large amounts of data at previously unheard of scales. Though the concepts were not new---map and reduce had existed for quite some time in functional programming languages---the observation that they could be used as a general programming paradigm for solving large data processing problems changed the current state of the art.\\r\\n\\r\\nIf you find yourself working with data nowadays, you\\u2019re bound to find yourself at some point with a need to process \\u201cBig Data\\u201d. Big Data can be a troublesome phrase, arguably more hype than anything at this point, it has many different meanings, but for the purpose of this talk we\\u2019ll consider it to be any data that is too large to fit into the main memory of a single machine. With that in mind, if you\\u2019ve ever found yourself with the need to process an amount of data that stretched the boundaries of your own personal laptop, and you wanted to apply the ideas expressed in Dean's and Ghemawat's seminal paper but had no idea what to do, or even where to start, then this talk is for you.\\r\\n\\r\\nThe goal of the talk is to give attendees a basic working knowledge of what MapReduce is, and how it can be used to process massive sets of data relatively quickly. During the course of this talk we will walk through the basics of what MapReduce is and how it works. Though there are a handful of MapReduce implementations out there to choose from, Hadoop is without a doubt the most well known and, as such, we will take a look at how to use it to run our MapReduce jobs. With that in mind, we will discuss what you need to know to use Hadoop and take a look at how to write our own Hadoop jobs in Python using the Hadoop Streaming utility. Finally, we\\u2019ll look at a library created at Yelp called MRJob that can make writing Hadoop jobs in Python much easier. By the end of the talk an attendee with little to no knowledge of MapReduce, but a working knowledge of Python, should be able to write their own basic MapReduce tasks for Hadoop and run them on a cluster of machines using Amazon\\u2019s Elastic MapReduce service.\\n                \",\n",
       "  u'\\n                    Many real-world datasets have missing observations, noise and outliers; usually due to logistical problems, component failures and erroneous procedures during the data collection process. Although it is easy to avoid missing points and noise to some level, it is not easy to detect wrong measurements and outliers in the dataset. These outliers may present a larger problem in time-series signals since every data point has a temporal dependency to the data point before and after. Therefore, it is crucially important to be able to detect and possibly correct these outliers. In this talk, I will introduce three different methods to be able to detect outliers in time-series signals; Fast Fourier Transform(FFT), Median Filtering and Bayesian approach.\\r\\n\\r\\nhttp://bugra.github.io/work/notes/2014-03-31/outlier-detection-in-time-series-signals-fft-median-filtering/\\n                ',\n",
       "  u'\\nDescription:\\nPyAlgoViz is an HTML5 browser application that allows Python students and practitioners to prototype an algorithm, visualize it, replay the execution, and share the end-result with others. A great use would be as a tool in the Datastructures and Algorithm track of the Computer Science curriculum.\\nAbstract:\\nPyAlgoViz is an HTML5 browser application that allows Python students and practitioners to prototype an algorithm, visualize it, and share it with others. To visualize an algorithm, it is sent to a server that runs the code, records the execution, and sends the recording back to the client. In the browser, the recording is then replayed at the speed the user wants. Graphics primitives to draw rectangles, lines, and text, in addition to generating sounds, allow algorithm visualizations that enhance the understanding of the algorithm.\\nIntended usage for PyAlgoViz is in the Datastructures and Algorithm track of the Computer Science curriculum or for personal education in the area of program algorithms. Not only will students learn how to implement algorithms in Python, they will also will be able to better understand asymptotic or even buggy algorithms by inducing patterns from observing the visualizations they create themselves.\\nTechnologies used to develop PyAlgoViz were MacVIM, Google App Engine, Python 2.7, Python NBD DataStore, , HTML/JavaScript/CSS, CodeMirror, jQuery, and .\\n',\n",
       "  u'\\n                    Over the course of three years, we\\'ve built Stripe from scratch and scaled it to process billions of dollars of transaction volume a year by making it easy and painless for merchants to get set up and start accepting payments. While the vast majority of transactions facilitated by Stripe are honest, we do need to protect our merchants from rogue individuals and groups seeing to \"test\" or \"cash\" stolen credit cards.\\r\\n\\r\\nTo combat this sort of activity, Stripe uses Python (together with Scala and Ruby) as part of its production machine learning pipeline to detect and block fraud in real time. In this talk, I\\'ll go through the scikit-based modeling process for a sample data set that is derived from production data to illustrate how we train and validate our models. We\\'ll also walk through how we deploy the models and monitor them in our production environment and how Python has allowed us to do this at scale.\\n                ',\n",
       "  u'\\n                    (**Note: This is the same talk I gave at PyData SV 2014. I\\'m proposing it again for PyData Berlin, but I also have a different talk [at a novice+/intermediate- level] with totally new content if you\\'d rather I submit that.)\\r\\n\\r\\nOver the course of three years, we\\'ve built Stripe from scratch and scaled it to process billions of dollars of transaction volume a year by making it easy and painless for merchants to get set up and start accepting payments. While the vast majority of transactions facilitated by Stripe are honest, we do need to protect our merchants from rogue individuals and groups seeing to \"test\" or \"cash\" stolen credit cards.\\r\\n\\r\\nTo combat this sort of activity, Stripe uses Python (together with Scala and Ruby) as part of its production machine learning pipeline to detect and block fraud in real time. In this talk, I\\'ll go through the scikit-based modeling process for a sample data set that is derived from production data to illustrate how we train and validate our models. We\\'ll also walk through how we deploy the models and monitor them in our production environment and how Python has allowed us to do this at scale.\\n                ',\n",
       "  u'\\n                    Pythran is a an ahead of time compiler that turns modules written in a large subset of Python into C++ meta-programs that can be compiled into efficient native modules. It targets mainly compute intensive part of the code, hence it comes as no surprise that it focuses on scientific applications that makes extensive use of Numpy.\\r\\n\\r\\nUnder the hood, Pythran inter-procedurally analyses the program and performs high level optimizations and parallel code generation. Parallelism can be found implicitly in Python intrinsics or Numpy operations, or explicitly specified by the programmer using OpenMP directives directly in the Python source code.\\r\\n\\r\\nEither way, the input code remains fully compatible with the Python interpreter. While the idea is similar to Parakeet or Numba, the approach\\r\\ndiffers significantly: the code generation is not performed at runtime but offline. Pythran generates C++11 heavily templated code that makes use of the NT2 meta-programming library and relies on any standard-compliant compiler to generate the binary code.\\r\\n\\r\\nWe propose to walk through some examples and benchmarks, exposing the current state of what Pythran provides as well as the limit of the approach.\\r\\n\\n                ',\n",
       "  u'\\n                    Most end users can\\'t write a database query, and yet, they often have the need to access information that keyword-based searches can\\'t retrieve precisely.\\r\\n\\r\\nLately, there\\'s been an explosion of proprietary Natural Language Interfaces to knowledge databases, like Siri, Google Now and Wolfram Alpha.\\r\\n\\r\\nOn the open side, huge knowledge bases like DBpedia and Freebase exists, but access to them is typically limited to using formal database query languages.\\r\\n\\r\\nWe implemented Quepy as an approach to provide a solution for this problem. Quepy is an open source framework to transform Natural Language questions into semantic database queries that can be used with popular knowledge databases like, for example, DBPedia and Freebase.\\r\\n\\r\\nSo instead of requiring end users to learn to write some query language, a Quepy Application can fills the gap, allowing end users to make their queries in \"plain English\". In this talk we would discuss the techniques used in Quepy, what additional work can be done, and its limitations.\\r\\n\\r\\n\\n                ',\n",
       "  u\"\\nSome of the biggest issues at the center of analyzing large amounts of data are query flexibility, latency, and fault tolerance. Modern technologies that build upon the success of \\u201cbig data\\u201d platforms, such as Apache Hadoop, have made it possible to spread the load of data analysis to commodity machines, but these analyses can still take hours to run and do not respond well to rapidly-changing data sets.\\nA new generation of data processing platforms -- which we call \\u201cstream architectures\\u201d -- have converted data sources into streams of data that can be processed and analyzed in real-time. This has led to the development of various distributed real-time computation frameworks (e.g. Apache Storm) and multi-consumer data integration technologies (e.g. Apache Kafka). Together, they offer a way to do predictable computation on real-time data streams.\\nIn this talk, we will give an overview of these technologies and how they fit into the Python ecosystem. This will include a discussion of current open source interoperability options with Python, and how to combine real-time computation with batch logic written for Hadoop. We will also discuss Kafka and Storm's alternatives, current industry usage, and some real-world examples of how these technologies are being used in production by Parse.ly today.\\n\",\n",
       "  u'\\n                    Facebook users produce millions of pieces of text content every day. Text content such as status updates and comments tell us a lot about how people feel on a daily basis and how people feel about the web of things. In this talk, we discuss a system based on scikit-learn and the Python scientific computing ecosystem that describes and models positive and negative sentiment of user generated content on Facebook.  Unlike more traditional polarized word counting methodologies, our system trains machine learning classifiers using naturally labelled data to achieve high accuracy.\\n                ',\n",
       "  u'\\n                    SociaLite is a Python-integrated query language for big data analysis.  It makes big data analysis simple, yet achieves fast performance with its compiler optimizations, often more than three orders of magnitude faster than Hadoop MapReduce programs.  For example, PageRank algorithm can be implemented in just 2 lines of SociaLite query, which runs nearly as fast as an optimal C implementation. \\r\\n\\r\\nHigh-level abstractions in SociaLite help implement distributed data analysis algorithms.  For example, its distributed in-memory tables allow large data to be stored across multiple machines, and with minimal user annotations, fast distributed join operations can be performed. \\r\\n\\r\\nMoreover, its Python integration makes SociaLite very powerful. We support embedding and extending, where embedding supports using SociaLite queries directly in Python code, and extending supports using Python functions in SociaLite queries. To support embedding, we apply source code rewriting that transforms SociaLite queries to invoke SociaLite runtime interfaces. For extending, we keep track of functions defined in Python interpreter and make them accesible from SociaLite.\\r\\n\\r\\nThe integration makes it easy to implement various data mining algorithms in SociaLite and Python. I will demonstrate in the talk a few well-known algorithms implemented in SociaLite, including PageRank, k-means, and logistic regression.\\r\\n\\r\\nThe high-level queries can achieve fast performance with various optimizations.  The queries are compiled into Java bytecode with compiler optimizations applied, such as prioritizations or pipelined evaluation. Also, the runtime system gives its best effort to achieve full utilizations of multi-core processors as well as network bandwidths. With the compiler optimizations and the runtime system we achieve very fast performance that is often close to optimal C implementations.\\n                ',\n",
       "  u\"\\n                    Speed without drag: making code faster when there's no time to waste\\r\\n\\r\\nA practical walkthrough over the state-of-the-art of low-friction numerical Python enhancing solutions, covering: exhausting CPython, NumPy, Numba, Parakeet, Cython, Theano, Pyston, PyPy/NumPyPy and Blaze.\\n                \",\n",
       "  u\"\\n                    A key idea behind the IPython Notebook is decoupling code execution from user interfaces. IPython relies on a documented JSON protocol, which can be implemented by different frontends and different kernels. By implementing the messaging protocol, new frontends gain the ability to communicate with a kernel regardless of the kernel implementation language. Conversely, new kernels automatically gain access to the existing client ecosystem. The IPython project maintains three different frontends, and there are multiple third party frontends and kernels already in use.\\r\\n\\r\\nWe'll describe some important features of the messaging protocol, before demonstrating some of our alternative frontends, including vim-ipython and bipython. We'll show kernels that people have written in other languages, such as Julia and R, and preview the upcoming features that will expose these alternative kernels in the Notebook user interface.\\r\\n\\r\\nThis talk is proposed jointly by Paul Ivanov and myself, both core IPython developers.\\n                \",\n",
       "  u'\\n                    The Reference Model for Disease progression [1,2] is a league of disease models that compete amongst themselves to fit existing clinical trial results. Clinical Trial results are widely available publicly at the summary level. Disease models are typically extracted from a single trial and therefore may not generalize well to all populations. The Reference Model determines the fitness of multiple disease models for multiple populations and helps deduce better fitting scenarios. This is done using High Performance Computing (HPC) techniques that support Monte Carlo simulation at the Micro individual level. The MIcro Simulation Tool (MIST) [3,4] facilitates running those simulations in HPC environment using Sun Grid Engine (SGE) [5]. MIST can even run over the cloud using StarCluster [6] and an anaconda AMI [7]. Note, however, the public published data is summary data while simulations are conducted at the individual level. The individual population is reconstructed from summary data using the MIST Domain Specific Language (DSL) and is optimized using evolutionary computation using Inspyred [8]. This allows creating populations that conform to the clinical trial summary statistics and allow incorporating trial inclusion and exclusion criteria as well as cope with skewed population distributions. \\r\\nThe Reference Model allows exploring new assumptions and hypothesis about disease progression and determines their fitness to existing population/model data. These virtual trials consider much more information than a single trial, using already available and public data. \\r\\n\\r\\nLinks to relevant own publications:\\r\\n[1] The Reference Model  video: http://youtu.be/7qxPSgINaD8 \\r\\n[2] The Reference Model short description: http://healtheconblog.com/2014/03/04/the-reference-model-for-disease-progression/ \\r\\n[3] MIST video presentation: http://www.youtube.com/watch?v=AD896WakR94 \\r\\n[4] MIST github repository: https://github.com/Jacob-Barhak/MIST\\r\\n\\r\\nLinks to external free software tools relevant to this work:\\r\\n[5] SGE: http://gridengine.org/blog/ \\r\\n[6] INSPYRED github repository: https://github.com/inspyred/inspyred \\r\\n[7] StarCluster home page: http://star.mit.edu/cluster/ \\r\\n[8] B. Zaitlen, StarCluster Anaconda. Online: http://continuum.io/blog/starcluster-anaconda \\r\\n\\n                ',\n",
       "  u'\\n                    In the past two years, there has been incredible progress in Python data visualization libraries, particularly those built on client-side JavaScript tools such as D3 and Leaflet. This talk will give a brief demonstration of many of the newest charting libs: mpld3 (using Seaborn/ggplot), nvd3-python,  ggplot, Vincent, Bearcart, Folium,and Kartograph will be used to visualize a newly-released USGS/FAA wind energy dataset (with an assist from Pandas and the IPython Notebook). After a demo of the current state of Python and web viz, it will discuss the future of how the Python data stack can have seamless interoperability and interactivity with JavaScript visualization libraries. \\n                ',\n",
       "  u\"\\n                    Today's world is full of data that is easily accessible for anyone. The problem now is how to make sense of this data and extract some useful insights from the terabytes of raw material. Typically, this involves using machine learning tools - allowing you to build classifiers, cluster data, etc. Many of these approaches give you models that describe the data accurately, but may be difficult to interpret. If you want to be able to understand the result more intuitively it is worth looking at Bayesian Networks - a graphical representation that simplifies complex mathematical model into a most likely graph of dependencies between your variables. \\r\\n\\r\\nI will talk about BNFinder - a python library allowing you to take any tabular data and convert it to a much simplified representation of conditional dependencies between variables. It can be the used for classification of unseen objects while the connection structure can be interpreted even by a non specialist. BNfinder is publicly available under GNU GPL and it can be used by anyone on their data.\\r\\n\\n                \",\n",
       "  u'\\n                    Intro to Burc Arpat\\n                ',\n",
       "  u\"\\n                    Day 42 of your brand new startup.  You are the CTO; your friend from b-school is the CEO.  She is busy meeting with potential clients to understand their needs.  You are busy with coding and you are about to start implementing a machine learning algorithm.  Which language do you choose?  So many possibilities: R, Julia, Lua, Java, C++, etc. etc. etc.  I'd pick Python.  After all, Python is a highly productive, highly customizable, batteries included language that integrates nicely with low-level code and is supported by a great community.  But mainly because\\u2026  Well\\u2026  Python will stay with you and continue to make your life easier even when you get to Facebook scale.  Don't believe me?  I'll show you concrete proof in the form of examples from a day in the life at Facebook.\\n                \",\n",
       "  u'\\n                    Making basic, good-looking plots in Python is tough. Matplotlib gives you great\\r\\ncontrol, but at the expense of being very detailed. The rise of pandas has made\\r\\nPython the go-to language for data wrangling and munging but many people are\\r\\nstill reluctant to leave R because of its outstanding data viz packages. ggplot\\r\\nis a port of the popular R package ggplot2. It provides a high level grammar\\r\\nthat allow users to quickly and easily make good looking plots. So say good-bye\\r\\nto matplotlib, and hello to ggplot as your everyday Python plotting library!\\r\\n\\r\\nhttps://github.com/yhat/ggplot\\n                ']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "text = {}\n",
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = \" \".join(abstracts)\n",
    "tokens = nltk.WordPunctTokenizer().tokenize(raw)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning; open source; data science; Cloud Foundry; command\n",
      "line; Coming soon; time series; financial statements; New York; Time\n",
      "permitting; confidence pool; crafted artisanal; dimensionality\n",
      "reduction; gene expression; keep track; rapid prototyping; big data;\n",
      "http ://; style buildpack; web application\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [w.lower() for w in text]\n",
    "words = [w for w in words if w not in stop]\n",
    "words = filter(lambda word: word not in u'%,-:()$\\/;?.!\\'[', words)\n",
    "words = [w for w in words if w not in [\"ll\", \"II\", \"ll\", \"http\", \"://\", \"e\", \"g\", \"2\", \"0\", \"1\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'talk',\n",
       " u'covers',\n",
       " u'rapid',\n",
       " u'prototyping',\n",
       " u'high',\n",
       " u'performance',\n",
       " u'scalable',\n",
       " u'text',\n",
       " u'processing',\n",
       " u'pipeline',\n",
       " u'development',\n",
       " u'python',\n",
       " u'demonstrate',\n",
       " u'python',\n",
       " u'modules',\n",
       " u'particular',\n",
       " u'rosetta',\n",
       " u'library',\n",
       " u'used',\n",
       " u'analyze',\n",
       " u'clean',\n",
       " u'extract',\n",
       " u'features',\n",
       " u'finally',\n",
       " u'perform',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'tasks',\n",
       " u'classification',\n",
       " u'topic',\n",
       " u'modeling',\n",
       " u'millions',\n",
       " u'documents',\n",
       " u'style',\n",
       " u'build',\n",
       " u'small',\n",
       " u'simple',\n",
       " u'modules',\n",
       " u'command',\n",
       " u'line',\n",
       " u'interfaces',\n",
       " u'use',\n",
       " u'little',\n",
       " u'memory',\n",
       " u'parallelized',\n",
       " u'multiprocessing',\n",
       " u'library',\n",
       " u'scikit',\n",
       " u'learn',\n",
       " u'one',\n",
       " u'popular',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'library',\n",
       " u'written',\n",
       " u'python',\n",
       " u'quite',\n",
       " u'active',\n",
       " u'community',\n",
       " u'extensive',\n",
       " u'coverage',\n",
       " u'number',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'algorithms',\n",
       " u'feature',\n",
       " u'extraction',\n",
       " u'feature',\n",
       " u'model',\n",
       " u'selection',\n",
       " u'algorithms',\n",
       " u'validation',\n",
       " u'methods',\n",
       " u'well',\n",
       " u'build',\n",
       " u'modern',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'pipeline',\n",
       " u'tutorial',\n",
       " u'introduces',\n",
       " u'common',\n",
       " u'recipes',\n",
       " u'build',\n",
       " u'modern',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'pipeline',\n",
       " u'different',\n",
       " u'input',\n",
       " u'domains',\n",
       " u'show',\n",
       " u'one',\n",
       " u'might',\n",
       " u'construct',\n",
       " u'components',\n",
       " u'using',\n",
       " u'advanced',\n",
       " u'features',\n",
       " u'scikit',\n",
       " u'learn',\n",
       " u'specifically',\n",
       " u'introduce',\n",
       " u'feature',\n",
       " u'extraction',\n",
       " u'methods',\n",
       " u'using',\n",
       " u'image',\n",
       " u'text',\n",
       " u'show',\n",
       " u'one',\n",
       " u'may',\n",
       " u'use',\n",
       " u'feature',\n",
       " u'selection',\n",
       " u'methods',\n",
       " u'reduce',\n",
       " u'input',\n",
       " u'dimension',\n",
       " u'space',\n",
       " u'remove',\n",
       " u'features',\n",
       " u'useful',\n",
       " u'classification',\n",
       " u'optimization',\n",
       " u'show',\n",
       " u'model',\n",
       " u'selection',\n",
       " u'methods',\n",
       " u'using',\n",
       " u'parameter',\n",
       " u'search',\n",
       " u'last',\n",
       " u'pipeline',\n",
       " u'show',\n",
       " u'validation',\n",
       " u'methods',\n",
       " u'able',\n",
       " u'choose',\n",
       " u'best',\n",
       " u'parameters',\n",
       " u'building',\n",
       " u'pipeline',\n",
       " u'also',\n",
       " u'show',\n",
       " u'one',\n",
       " u'might',\n",
       " u'deploy',\n",
       " u'model',\n",
       " u'production',\n",
       " u'ipython',\n",
       " u'recently',\n",
       " u'introduced',\n",
       " u'new',\n",
       " u'framework',\n",
       " u'interactive',\n",
       " u'javascript',\n",
       " u'widgets',\n",
       " u'online',\n",
       " u'notebook',\n",
       " u'widgets',\n",
       " u'allow',\n",
       " u'one',\n",
       " u'easily',\n",
       " u'create',\n",
       " u'interact',\n",
       " u'buttons',\n",
       " u'sliders',\n",
       " u'text',\n",
       " u'fields',\n",
       " u'html',\n",
       " u'javascript',\n",
       " u'objects',\n",
       " u'python',\n",
       " u'code',\n",
       " u'framework',\n",
       " u'also',\n",
       " u'used',\n",
       " u'advanced',\n",
       " u'systems',\n",
       " u'widgets',\n",
       " u'webgl',\n",
       " u'3d',\n",
       " u'graphics',\n",
       " u'interactive',\n",
       " u'maps',\n",
       " u'discuss',\n",
       " u'give',\n",
       " u'examples',\n",
       " u'write',\n",
       " u'simple',\n",
       " u'complicated',\n",
       " u'widgets',\n",
       " u'demonstrate',\n",
       " u'complicated',\n",
       " u'widget',\n",
       " u'examples',\n",
       " u'pythreejs',\n",
       " u'widget',\n",
       " u'collection',\n",
       " u'interactive',\n",
       " u'3d',\n",
       " u'graphics',\n",
       " u'https',\n",
       " u'github',\n",
       " u'com',\n",
       " u'jasongrout',\n",
       " u'pythreejs',\n",
       " u'),',\n",
       " u'discuss',\n",
       " u'changes',\n",
       " u'widget',\n",
       " u'infrastructure',\n",
       " u'upcoming',\n",
       " u'ipython',\n",
       " u'3',\n",
       " u'since',\n",
       " u'advanced',\n",
       " u'talk',\n",
       " u'would',\n",
       " u'helpful',\n",
       " u'participants',\n",
       " u'worked',\n",
       " u'widget',\n",
       " u'tutorial',\n",
       " u'notebooks',\n",
       " u'nbviewer',\n",
       " u'ipython',\n",
       " u'org',\n",
       " u'github',\n",
       " u'ipython',\n",
       " u'ipython',\n",
       " u'blob',\n",
       " u'master',\n",
       " u'examples',\n",
       " u'interactive',\n",
       " u'20widgets',\n",
       " u'index',\n",
       " u'ipynb',\n",
       " u'talk',\n",
       " u'coming',\n",
       " u'soon',\n",
       " u'python',\n",
       " u'rich',\n",
       " u'ecosystem',\n",
       " u'open',\n",
       " u'source',\n",
       " u'geographical',\n",
       " u'information',\n",
       " u'science',\n",
       " u'gis',\n",
       " u'applications',\n",
       " u'gis',\n",
       " u'packages',\n",
       " u'python',\n",
       " u'bindings',\n",
       " u'binaries',\n",
       " u'data',\n",
       " u'transformation',\n",
       " u'image',\n",
       " u'manipulation',\n",
       " u'makes',\n",
       " u'hard',\n",
       " u'study',\n",
       " u'data',\n",
       " u'processing',\n",
       " u'encompasses',\n",
       " u'masks',\n",
       " u'underlying',\n",
       " u'algorithms',\n",
       " u'talk',\n",
       " u'use',\n",
       " u'landsat',\n",
       " u'8',\n",
       " u'satellite',\n",
       " u'imagery',\n",
       " u'python',\n",
       " u'scientific',\n",
       " u'stack',\n",
       " u'demonstrate',\n",
       " u'typical',\n",
       " u'data',\n",
       " u'centric',\n",
       " u'approach',\n",
       " u'gis',\n",
       " u'analysis',\n",
       " u'time',\n",
       " u'explain',\n",
       " u'algorithmic',\n",
       " u'underpinnings',\n",
       " u'image',\n",
       " u'recognition',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'techniques',\n",
       " u'applied',\n",
       " u'satellite',\n",
       " u'images',\n",
       " u'expose',\n",
       " u'data',\n",
       " u'openness',\n",
       " u'exploration',\n",
       " u'coming',\n",
       " u'soon',\n",
       " u'------',\n",
       " u'blaze',\n",
       " u'numpy',\n",
       " u'pandas',\n",
       " u'interface',\n",
       " u'big',\n",
       " u'data',\n",
       " u'systems',\n",
       " u'like',\n",
       " u'sql',\n",
       " u'hdfs',\n",
       " u'spark',\n",
       " u'blaze',\n",
       " u'provides',\n",
       " u'python',\n",
       " u'developers',\n",
       " u'access',\n",
       " u'rich',\n",
       " u'analytic',\n",
       " u'processing',\n",
       " u'available',\n",
       " u'within',\n",
       " u'python',\n",
       " u'ecosystem',\n",
       " u'beyond',\n",
       " u'internally',\n",
       " u'blaze',\n",
       " u'lightweight',\n",
       " u'data',\n",
       " u'modeling',\n",
       " u'language',\n",
       " u'expressions',\n",
       " u'type',\n",
       " u'information',\n",
       " u'alongside',\n",
       " u'set',\n",
       " u'interpreters',\n",
       " u'python',\n",
       " u'sql',\n",
       " u'spark',\n",
       " u'mongodb',\n",
       " u'...).',\n",
       " u'modeling',\n",
       " u'language',\n",
       " u'provides',\n",
       " u'intuitive',\n",
       " u'familiar',\n",
       " u'user',\n",
       " u'experience',\n",
       " u'interpreters',\n",
       " u'connect',\n",
       " u'experience',\n",
       " u'wide',\n",
       " u'variety',\n",
       " u'data',\n",
       " u'technologies',\n",
       " u'combination',\n",
       " u'allows',\n",
       " u'developers',\n",
       " u'construct',\n",
       " u'connections',\n",
       " u'novel',\n",
       " u'technologies',\n",
       " u'connections',\n",
       " u'enable',\n",
       " u'users',\n",
       " u'interact',\n",
       " u'system',\n",
       " u'choice',\n",
       " u'regardless',\n",
       " u'system',\n",
       " u'single',\n",
       " u'csv',\n",
       " u'file',\n",
       " u'large',\n",
       " u'hdfs',\n",
       " u'cluster',\n",
       " u'running',\n",
       " u'impala',\n",
       " u'followed',\n",
       " u'second',\n",
       " u'talk',\n",
       " u'using',\n",
       " u'blaze',\n",
       " u'wild',\n",
       " u'round',\n",
       " u'table',\n",
       " u'discussion',\n",
       " u'talk',\n",
       " u'types',\n",
       " u'events',\n",
       " u'worked',\n",
       " u'well',\n",
       " u'share',\n",
       " u'ideas',\n",
       " u'organized',\n",
       " u'michael',\n",
       " u'becker',\n",
       " u'ever',\n",
       " u'wonder',\n",
       " u'google',\n",
       " u'chrome',\n",
       " u'detects',\n",
       " u'language',\n",
       " u'every',\n",
       " u'webpage',\n",
       " u'visit',\n",
       " u'data',\n",
       " u'science',\n",
       " u'data',\n",
       " u'science',\n",
       " u'use',\n",
       " u'python',\n",
       " u'talk',\n",
       " u'teach',\n",
       " u'data',\n",
       " u'science',\n",
       " u'process',\n",
       " u'osemn',\n",
       " u'creating',\n",
       " u'language',\n",
       " u'prediction',\n",
       " u'algorithm',\n",
       " u'utilizing',\n",
       " u'nothing',\n",
       " u'python',\n",
       " u'data',\n",
       " u'wikipedia',\n",
       " u'cover',\n",
       " u'data',\n",
       " u'science',\n",
       " u'writing',\n",
       " u'simple',\n",
       " u'web',\n",
       " u'scraping',\n",
       " u'bot',\n",
       " u'using',\n",
       " u'pandas',\n",
       " u'data',\n",
       " u'processing',\n",
       " u'exploration',\n",
       " u'creating',\n",
       " u'beautiful',\n",
       " u'data',\n",
       " u'visualizations',\n",
       " u'seaborn',\n",
       " u'working',\n",
       " u'text',\n",
       " u'data',\n",
       " u'scikit',\n",
       " u'learn',\n",
       " u'building',\n",
       " u'predictive',\n",
       " u'model',\n",
       " u'ensuring',\n",
       " u'model',\n",
       " u'accurate',\n",
       " u'utilizing',\n",
       " u'model',\n",
       " u'interpreting',\n",
       " u'results',\n",
       " u'cubes',\n",
       " u'light',\n",
       " u'weight',\n",
       " u'framework',\n",
       " u'modelling',\n",
       " u'conceptual',\n",
       " u'analytical',\n",
       " u'data',\n",
       " u'part',\n",
       " u'olap',\n",
       " u'aggregated',\n",
       " u'browsing',\n",
       " u'recent',\n",
       " u'release',\n",
       " u'cubes',\n",
       " u'brings',\n",
       " u'ability',\n",
       " u'build',\n",
       " u'heterogenous',\n",
       " u'data',\n",
       " u'warehouse',\n",
       " u'plug',\n",
       " u'architecture',\n",
       " u'analytical',\n",
       " u'backends',\n",
       " u'sql',\n",
       " u'mongo',\n",
       " u'mixpanel',\n",
       " u'google',\n",
       " u'analytics',\n",
       " u'custom',\n",
       " u'...)',\n",
       " u'provide',\n",
       " u'unified',\n",
       " u'aggregation',\n",
       " u'conceptual',\n",
       " u'browsing',\n",
       " u'interface',\n",
       " u'python',\n",
       " u'json',\n",
       " u'talk',\n",
       " u'go',\n",
       " u'main',\n",
       " u'features',\n",
       " u'new',\n",
       " u'cubes',\n",
       " u'analytical',\n",
       " u'workspace',\n",
       " u'pluggable',\n",
       " u'models',\n",
       " u'model',\n",
       " u'providers',\n",
       " u'backends',\n",
       " u'authentication',\n",
       " u'authorization',\n",
       " u'fine',\n",
       " u'tuning',\n",
       " u'reporting',\n",
       " u'restricted',\n",
       " u'business',\n",
       " u'environment',\n",
       " u'),',\n",
       " u'new',\n",
       " u'modelling',\n",
       " u'concepts',\n",
       " u'advanced',\n",
       " u'sql',\n",
       " u'features',\n",
       " u'field',\n",
       " u'biology',\n",
       " u'constantly',\n",
       " u'evolving',\n",
       " u'new',\n",
       " u'methods',\n",
       " u'quantification',\n",
       " u'must',\n",
       " u'constantly',\n",
       " u'adapt',\n",
       " u'different',\n",
       " u'upstream',\n",
       " u'workflows',\n",
       " u'quantify',\n",
       " u'biological',\n",
       " u'measurements',\n",
       " u'gene',\n",
       " u'expression',\n",
       " u'alternative',\n",
       " u'splicing',\n",
       " u'present',\n",
       " u'flotilla',\n",
       " u'github',\n",
       " u'com',\n",
       " u'yeolab',\n",
       " u'flotilla',\n",
       " u'flexible',\n",
       " u'open',\n",
       " u'source',\n",
       " u'community',\n",
       " u'driven',\n",
       " u'software',\n",
       " u'package',\n",
       " u'enables',\n",
       " u'biologists',\n",
       " u'rudimentary',\n",
       " u'knowledge',\n",
       " u'statistical',\n",
       " u'methods',\n",
       " u'programming',\n",
       " u'analyze',\n",
       " u'visualize',\n",
       " u'hundreds',\n",
       " u'biological',\n",
       " u'datasets',\n",
       " u'users',\n",
       " u'flotilla',\n",
       " u'harness',\n",
       " u'power',\n",
       " u'ipython',\n",
       " u'interactive',\n",
       " u'widgets',\n",
       " u'python',\n",
       " u'scientific',\n",
       " u'stack',\n",
       " u'quickly',\n",
       " u'perform',\n",
       " u'powerful',\n",
       " u'analyses',\n",
       " u'biological',\n",
       " u'datasets',\n",
       " u'dimensionality',\n",
       " u'reduction',\n",
       " u'covariance',\n",
       " u'analysis',\n",
       " u'clustering',\n",
       " u'classification',\n",
       " u'regression',\n",
       " u'outlier',\n",
       " u'detection',\n",
       " u'case',\n",
       " u'study',\n",
       " u'present',\n",
       " u'single',\n",
       " u'cell',\n",
       " u'rna',\n",
       " u'sequencing',\n",
       " u'analysis',\n",
       " u'use',\n",
       " u'case',\n",
       " u'flotilla',\n",
       " u'data',\n",
       " u'two',\n",
       " u'publicly',\n",
       " u'available',\n",
       " u'datasets',\n",
       " u'individual',\n",
       " u'mouse',\n",
       " u'immune',\n",
       " u'cells',\n",
       " u'response',\n",
       " u'stimulus',\n",
       " u'data',\n",
       " u'show',\n",
       " u'utility',\n",
       " u'flotilla',\n",
       " u'quantifying',\n",
       " u'variation',\n",
       " u'gene',\n",
       " u'expression',\n",
       " u'mrna',\n",
       " u'splicing',\n",
       " u'single',\n",
       " u'cells',\n",
       " u'reducing',\n",
       " u'uncertainty',\n",
       " u'heart',\n",
       " u'statistical',\n",
       " u'analysis',\n",
       " u'important',\n",
       " u'modeling',\n",
       " u'data',\n",
       " u'numerous',\n",
       " u'predictors',\n",
       " u'regularization',\n",
       " u'reduce',\n",
       " u'uncertainty',\n",
       " u'interpreting',\n",
       " u'variables',\n",
       " u'making',\n",
       " u'predictions',\n",
       " u'leading',\n",
       " u'better',\n",
       " u'results',\n",
       " u'bayesians',\n",
       " u'frequentists',\n",
       " u'implement',\n",
       " u'different',\n",
       " u'ways',\n",
       " u'prior',\n",
       " u'distributions',\n",
       " u'former',\n",
       " u'penalized',\n",
       " u'regression',\n",
       " u'latter',\n",
       " u'examine',\n",
       " u'techniques',\n",
       " u'differ',\n",
       " u'similar',\n",
       " u'even',\n",
       " u'explore',\n",
       " u'bayesian',\n",
       " u'interpretation',\n",
       " u'lasso',\n",
       " u'l1',\n",
       " u'penalized',\n",
       " u'regression',\n",
       " u'make',\n",
       " u'sides',\n",
       " u'happy',\n",
       " u'disco',\n",
       " u'distributed',\n",
       " u'data',\n",
       " u'processing',\n",
       " u'platform',\n",
       " u'used',\n",
       " u'batch',\n",
       " u'low',\n",
       " u'latency',\n",
       " u'stream',\n",
       " u'processing',\n",
       " u'jobs',\n",
       " u'constructed',\n",
       " u'python',\n",
       " u'using',\n",
       " u'multi',\n",
       " u'stage',\n",
       " u'data',\n",
       " u'pipelines',\n",
       " u'run',\n",
       " u'light',\n",
       " u'weight',\n",
       " u'highly',\n",
       " u'available',\n",
       " u'clustered',\n",
       " u'architecture',\n",
       " u'tim',\n",
       " u'describe',\n",
       " u'building',\n",
       " u'stream',\n",
       " u'batch',\n",
       " u'ad',\n",
       " u'hoc',\n",
       " u'query',\n",
       " u'based',\n",
       " u'data',\n",
       " u'infrastructure',\n",
       " u'using',\n",
       " u'disco',\n",
       " u'ecosystem',\n",
       " u'blaze',\n",
       " u'library',\n",
       " u'harnessing',\n",
       " u'power',\n",
       " u'big',\n",
       " u'data',\n",
       " u'technologies',\n",
       " u'part',\n",
       " u'deux',\n",
       " u'blaze',\n",
       " u'thon',\n",
       " u'show',\n",
       " u'motivating',\n",
       " u'use',\n",
       " u'cases',\n",
       " u'illustrating',\n",
       " u'might',\n",
       " u'want',\n",
       " u'use',\n",
       " u'blaze',\n",
       " u'including',\n",
       " u'comparison',\n",
       " u'core',\n",
       " u'pandas',\n",
       " u'experimental',\n",
       " u'backend',\n",
       " u'using',\n",
       " u'lesser',\n",
       " u'known',\n",
       " u'kdb',\n",
       " u'+.',\n",
       " u'time',\n",
       " u'permitting',\n",
       " u'show',\n",
       " u'easy',\n",
       " u'users',\n",
       " u'blaze',\n",
       " u'scratch',\n",
       " u'itch',\n",
       " u'hooking',\n",
       " u'existing',\n",
       " u'api',\n",
       " u'blaze',\n",
       " u'via',\n",
       " u'small',\n",
       " u'set',\n",
       " u'multiply',\n",
       " u'dispatched',\n",
       " u'functions',\n",
       " u'one',\n",
       " u'big',\n",
       " u'problems',\n",
       " u'educational',\n",
       " u'assessment',\n",
       " u'*',\n",
       " u'norming',\n",
       " u'*,',\n",
       " u'whose',\n",
       " u'simplest',\n",
       " u'form',\n",
       " u'amounts',\n",
       " u'giving',\n",
       " u'credit',\n",
       " u'answering',\n",
       " u'difficult',\n",
       " u'questions',\n",
       " u'ideas',\n",
       " u'come',\n",
       " u'norming',\n",
       " u'wide',\n",
       " u'ranging',\n",
       " u'applications',\n",
       " u'comparing',\n",
       " u'examinees',\n",
       " u'seen',\n",
       " u'different',\n",
       " u'sets',\n",
       " u'questions',\n",
       " u'effectively',\n",
       " u'defining',\n",
       " u'\"',\n",
       " u'performance',\n",
       " u'grade',\n",
       " u'level',\n",
       " u'\",',\n",
       " u'measuring',\n",
       " u'students',\n",
       " u'learn',\n",
       " u'relative',\n",
       " u'cohort',\n",
       " u'school',\n",
       " u'bucketing',\n",
       " u'scheme',\n",
       " u').',\n",
       " u'moreover',\n",
       " u'similar',\n",
       " u'techniques',\n",
       " u'used',\n",
       " u'rank',\n",
       " u'skills',\n",
       " u'players',\n",
       " u'online',\n",
       " u'video',\n",
       " u'games',\n",
       " u'assessing',\n",
       " u'competency',\n",
       " u'mechanical',\n",
       " u'turkers',\n",
       " u'among',\n",
       " u'applications',\n",
       " u'talk',\n",
       " u'cover',\n",
       " u'common',\n",
       " u'settings',\n",
       " u'objective',\n",
       " u'functions',\n",
       " u'whose',\n",
       " u'maxima',\n",
       " u'might',\n",
       " u'interpreted',\n",
       " u'solve',\n",
       " u'problems',\n",
       " u'like',\n",
       " u'good',\n",
       " u'modeler',\n",
       " u'spend',\n",
       " u'time',\n",
       " u'assumptions',\n",
       " u'break',\n",
       " u'difficulties',\n",
       " u'computing',\n",
       " u'interpreting',\n",
       " u'solutions',\n",
       " u'programs',\n",
       " u'differ',\n",
       " u'much',\n",
       " u'python',\n",
       " u'discussed',\n",
       " u'many',\n",
       " u'pretty',\n",
       " u'pictures',\n",
       " u'drawn',\n",
       " u'engineer',\n",
       " u'analyst',\n",
       " u'scientist',\n",
       " u'sharing',\n",
       " u'work',\n",
       " u'someone',\n",
       " u'outside',\n",
       " u'immediate',\n",
       " u'team',\n",
       " u'challenge',\n",
       " u'end',\n",
       " u'users',\n",
       " u'embody',\n",
       " u'many',\n",
       " u'roles',\n",
       " u'wide',\n",
       " u'range',\n",
       " u'technical',\n",
       " u'skill',\n",
       " u'often',\n",
       " u'times',\n",
       " u'familiarity',\n",
       " u'python',\n",
       " u'command',\n",
       " u'line',\n",
       " u'findings',\n",
       " u'key',\n",
       " u'results',\n",
       " u'models',\n",
       " u'frequently',\n",
       " u'boiled',\n",
       " u'static',\n",
       " u'graphs',\n",
       " u'tables',\n",
       " u'figures',\n",
       " u'presented',\n",
       " u'short',\n",
       " u'reports',\n",
       " u'slideshow',\n",
       " u'presentations',\n",
       " u'however',\n",
       " u'engaging',\n",
       " u'research',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'interactive',\n",
       " u'anticipating',\n",
       " u'users',\n",
       " u'questions',\n",
       " u'giving',\n",
       " u'tools',\n",
       " u'answer',\n",
       " u'questions',\n",
       " u'simple',\n",
       " u'intuitive',\n",
       " u'user',\n",
       " u'interface',\n",
       " u'browser',\n",
       " u'based',\n",
       " u'applications',\n",
       " u'ideal',\n",
       " u'vehicle',\n",
       " u'delivering',\n",
       " u'types',\n",
       " u'interactive',\n",
       " u'tools',\n",
       " u'building',\n",
       " u'web',\n",
       " u'app',\n",
       " u'requires',\n",
       " u'setting',\n",
       " u'backend',\n",
       " u'applications',\n",
       " u'serve',\n",
       " u'content',\n",
       " u'creating',\n",
       " u'ui',\n",
       " u'languages',\n",
       " u'like',\n",
       " u'html',\n",
       " u'css',\n",
       " u'javascript',\n",
       " u'non',\n",
       " u'trivial',\n",
       " u'task',\n",
       " u'overwhelming',\n",
       " u'anyone',\n",
       " u'familiar',\n",
       " u'web',\n",
       " u'stack',\n",
       " u'spyre',\n",
       " u'web',\n",
       " u'application',\n",
       " u'framework',\n",
       " u'meant',\n",
       " u'help',\n",
       " u'python',\n",
       " u'developers',\n",
       " u'might',\n",
       " u'little',\n",
       " u'knowledge',\n",
       " u'web',\n",
       " u'applications',\n",
       " u'works',\n",
       " u'much',\n",
       " u'less',\n",
       " u'build',\n",
       " u'spyre',\n",
       " u'takes',\n",
       " u'care',\n",
       " u'setting',\n",
       " u'front',\n",
       " u'back',\n",
       " u'end',\n",
       " u'web',\n",
       " u'application',\n",
       " u'uses',\n",
       " u'cherrypy',\n",
       " u'handle',\n",
       " u'request',\n",
       " u'logic',\n",
       " u'jinja2',\n",
       " u'auto',\n",
       " u'generate',\n",
       " u'client',\n",
       " u'side',\n",
       " u'nuts',\n",
       " u'bolts',\n",
       " u'allowing',\n",
       " u'developers',\n",
       " u'quickly',\n",
       " u'move',\n",
       " u'inputs',\n",
       " u'outputs',\n",
       " u'python',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(words)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'machine', u'learning'), 0.004191616766467066),\n",
       " ((u'data', u'science'), 0.0032934131736526945),\n",
       " ((u'scikit', u'learn'), 0.0032934131736526945),\n",
       " ((u'open', u'source'), 0.002694610778443114),\n",
       " ((u'big', u'data'), 0.0014970059880239522),\n",
       " ((u'coming', u'soon'), 0.0011976047904191617),\n",
       " ((u'data', u'processing'), 0.0011976047904191617),\n",
       " ((u'data', u'sets'), 0.0011976047904191617),\n",
       " ((u'python', u'ecosystem'), 0.0011976047904191617),\n",
       " ((u'cloud', u'foundry'), 0.0008982035928143712),\n",
       " ((u'command', u'line'), 0.0008982035928143712),\n",
       " ((u'data', u'analysis'), 0.0008982035928143712),\n",
       " ((u'deep', u'learning'), 0.0008982035928143712),\n",
       " ((u'financial', u'statements'), 0.0008982035928143712),\n",
       " ((u'languages', u'like'), 0.0008982035928143712),\n",
       " ((u'learning', u'techniques'), 0.0008982035928143712),\n",
       " ((u'light', u'weight'), 0.0008982035928143712),\n",
       " ((u'one', u'might'), 0.0008982035928143712),\n",
       " ((u'selective', u'search'), 0.0008982035928143712),\n",
       " ((u'show', u'one'), 0.0008982035928143712),\n",
       " ((u'talk', u'go'), 0.0008982035928143712),\n",
       " ((u'time', u'series'), 0.0008982035928143712),\n",
       " ((u'web', u'application'), 0.0008982035928143712),\n",
       " ((u'3d', u'graphics'), 0.0005988023952095808),\n",
       " ((u'analysis', u'use'), 0.0005988023952095808),\n",
       " ((u'analyze', u'visualize'), 0.0005988023952095808),\n",
       " ((u'app', u'using'), 0.0005988023952095808),\n",
       " ((u'artisanal', u'visualizations'), 0.0005988023952095808),\n",
       " ((u'available', u'python'), 0.0005988023952095808),\n",
       " ((u'back', u'end'), 0.0005988023952095808),\n",
       " ((u'based', u'application'), 0.0005988023952095808),\n",
       " ((u'based', u'applications'), 0.0005988023952095808),\n",
       " ((u'baseline', u'classifier'), 0.0005988023952095808),\n",
       " ((u'best', u'parameters'), 0.0005988023952095808),\n",
       " ((u'biological', u'datasets'), 0.0005988023952095808),\n",
       " ((u'browser', u'based'), 0.0005988023952095808),\n",
       " ((u'build', u'modern'), 0.0005988023952095808),\n",
       " ((u'c', u'++,'), 0.0005988023952095808),\n",
       " ((u'c', u'c'), 0.0005988023952095808),\n",
       " ((u'case', u'study'), 0.0005988023952095808),\n",
       " ((u'confidence', u'pool'), 0.0005988023952095808),\n",
       " ((u'crafted', u'artisanal'), 0.0005988023952095808),\n",
       " ((u'data', u'analytics'), 0.0005988023952095808),\n",
       " ((u'data', u'audit'), 0.0005988023952095808),\n",
       " ((u'data', u'different'), 0.0005988023952095808),\n",
       " ((u'data', u'directly'), 0.0005988023952095808),\n",
       " ((u'data', u'driven'), 0.0005988023952095808),\n",
       " ((u'data', u'scientist'), 0.0005988023952095808),\n",
       " ((u'data', u'scientists'), 0.0005988023952095808),\n",
       " ((u'data', u'services'), 0.0005988023952095808),\n",
       " ((u'data', u'technologies'), 0.0005988023952095808),\n",
       " ((u'different', u'ways'), 0.0005988023952095808),\n",
       " ((u'dimensionality', u'reduction'), 0.0005988023952095808),\n",
       " ((u'end', u'talk'), 0.0005988023952095808),\n",
       " ((u'feature', u'extraction'), 0.0005988023952095808),\n",
       " ((u'front', u'end'), 0.0005988023952095808),\n",
       " ((u'gene', u'expression'), 0.0005988023952095808),\n",
       " ((u'github', u'com'), 0.0005988023952095808),\n",
       " ((u'hand', u'crafted'), 0.0005988023952095808),\n",
       " ((u'heroku', u'style'), 0.0005988023952095808),\n",
       " ((u'high', u'level'), 0.0005988023952095808),\n",
       " ((u'high', u'performance'), 0.0005988023952095808),\n",
       " ((u'image', u'must'), 0.0005988023952095808),\n",
       " ((u'insurance', u'linked'), 0.0005988023952095808),\n",
       " ((u'ipython', u'notebook'), 0.0005988023952095808),\n",
       " ((u'keep', u'track'), 0.0005988023952095808),\n",
       " ((u'large', u'data'), 0.0005988023952095808),\n",
       " ((u'learning', u'object'), 0.0005988023952095808),\n",
       " ((u'learning', u'pipeline'), 0.0005988023952095808),\n",
       " ((u'learning', u'tasks'), 0.0005988023952095808),\n",
       " ((u'like', u'html'), 0.0005988023952095808),\n",
       " ((u'made', u'possible'), 0.0005988023952095808),\n",
       " ((u'mechanisms', u'disease'), 0.0005988023952095808),\n",
       " ((u'methods', u'using'), 0.0005988023952095808),\n",
       " ((u'model', u'developed'), 0.0005988023952095808),\n",
       " ((u'model', u'selection'), 0.0005988023952095808),\n",
       " ((u'modeling', u'language'), 0.0005988023952095808),\n",
       " ((u'modern', u'machine'), 0.0005988023952095808),\n",
       " ((u'new', u'therapeutics'), 0.0005988023952095808),\n",
       " ((u'new', u'york'), 0.0005988023952095808),\n",
       " ((u'numpy', u'arrays'), 0.0005988023952095808),\n",
       " ((u'object', u'identification'), 0.0005988023952095808),\n",
       " ((u'object', u'recognition'), 0.0005988023952095808),\n",
       " ((u'pandas', u'scikit'), 0.0005988023952095808),\n",
       " ((u'penalized', u'regression'), 0.0005988023952095808),\n",
       " ((u'physical', u'modeling'), 0.0005988023952095808),\n",
       " ((u'private', u'cloud'), 0.0005988023952095808),\n",
       " ((u'provide', u'introduction'), 0.0005988023952095808),\n",
       " ((u'public', u'private'), 0.0005988023952095808),\n",
       " ((u'publicly', u'available'), 0.0005988023952095808),\n",
       " ((u'pyplink', u'seq'), 0.0005988023952095808),\n",
       " ((u'python', u'code'), 0.0005988023952095808),\n",
       " ((u'python', u'command'), 0.0005988023952095808),\n",
       " ((u'python', u'developers'), 0.0005988023952095808),\n",
       " ((u'python', u'dictionary'), 0.0005988023952095808),\n",
       " ((u'python', u'modules'), 0.0005988023952095808),\n",
       " ((u'python', u'scientific'), 0.0005988023952095808),\n",
       " ((u'python', u'wrapper'), 0.0005988023952095808),\n",
       " ((u'rapid', u'prototyping'), 0.0005988023952095808),\n",
       " ((u'rating', u'agencies'), 0.0005988023952095808),\n",
       " ((u'scientific', u'python'), 0.0005988023952095808),\n",
       " ((u'scientific', u'stack'), 0.0005988023952095808),\n",
       " ((u'selection', u'methods'), 0.0005988023952095808),\n",
       " ((u'series', u'analysis'), 0.0005988023952095808),\n",
       " ((u'simple', u'tool'), 0.0005988023952095808),\n",
       " ((u'stack', u'openmm'), 0.0005988023952095808),\n",
       " ((u'style', u'buildpack'), 0.0005988023952095808),\n",
       " ((u'talk', u'provide'), 0.0005988023952095808),\n",
       " ((u'talk', u'using'), 0.0005988023952095808),\n",
       " ((u'technical', u'skill'), 0.0005988023952095808),\n",
       " ((u'time', u'permitting'), 0.0005988023952095808),\n",
       " ((u'training', u'data'), 0.0005988023952095808),\n",
       " ((u'using', u'cloud'), 0.0005988023952095808),\n",
       " ((u'using', u'pandas'), 0.0005988023952095808),\n",
       " ((u'validation', u'methods'), 0.0005988023952095808),\n",
       " ((u'visualization', u'techniques'), 0.0005988023952095808),\n",
       " ((u'web', u'app'), 0.0005988023952095808),\n",
       " ((u'web', u'applications'), 0.0005988023952095808),\n",
       " ((u'\"', u'box'), 0.0002994011976047904),\n",
       " ((u'\"', u'collecting'), 0.0002994011976047904),\n",
       " ((u'\"', u'could'), 0.0002994011976047904),\n",
       " ((u'\"', u'data'), 0.0002994011976047904),\n",
       " ((u'\"', u'errors'), 0.0002994011976047904),\n",
       " ((u'\"', u'ever'), 0.0002994011976047904),\n",
       " ((u'\"', u'garbage'), 0.0002994011976047904),\n",
       " ((u'\"', u'memory'), 0.0002994011976047904),\n",
       " ((u'\"', u'nodes'), 0.0002994011976047904),\n",
       " ((u'\"', u'performance'), 0.0002994011976047904),\n",
       " ((u'\"', u'problem'), 0.0002994011976047904),\n",
       " ((u'\"', u'use'), 0.0002994011976047904),\n",
       " ((u'\",', u'measuring'), 0.0002994011976047904),\n",
       " ((u'&', u'speech'), 0.0002994011976047904),\n",
       " ((u'),', u'addresses'), 0.0002994011976047904),\n",
       " ((u'),', u'discuss'), 0.0002994011976047904),\n",
       " ((u'),', u'found'), 0.0002994011976047904),\n",
       " ((u'),', u'new'), 0.0002994011976047904),\n",
       " ((u').', u'coming'), 0.0002994011976047904),\n",
       " ((u').', u'consistent'), 0.0002994011976047904),\n",
       " ((u').', u'core'), 0.0002994011976047904),\n",
       " ((u').', u'model'), 0.0002994011976047904),\n",
       " ((u').', u'moreover'), 0.0002994011976047904),\n",
       " ((u').', u'popular'), 0.0002994011976047904),\n",
       " ((u').', u'spyre'), 0.0002994011976047904),\n",
       " ((u').', u'web'), 0.0002994011976047904),\n",
       " ((u'*', u'comprehensions'), 0.0002994011976047904),\n",
       " ((u'*', u'generators'), 0.0002994011976047904),\n",
       " ((u'*', u'itertools'), 0.0002994011976047904),\n",
       " ((u'*', u'norming'), 0.0002994011976047904),\n",
       " ((u'*,', u'whose'), 0.0002994011976047904),\n",
       " ((u'++,', u'cuda'), 0.0002994011976047904),\n",
       " ((u'++,', u'visual'), 0.0002994011976047904),\n",
       " ((u'++.', u'julia'), 0.0002994011976047904),\n",
       " ((u'+.', u'time'), 0.0002994011976047904),\n",
       " ((u'--', u'data'), 0.0002994011976047904),\n",
       " ((u'--', u'getting'), 0.0002994011976047904),\n",
       " ((u'--', u'spyre'), 0.0002994011976047904),\n",
       " ((u'---', u'collaboration'), 0.0002994011976047904),\n",
       " ((u'---', u'producing'), 0.0002994011976047904),\n",
       " ((u'---', u'visualizations'), 0.0002994011976047904),\n",
       " ((u'----', u'beaker'), 0.0002994011976047904),\n",
       " ((u'------', u'blaze'), 0.0002994011976047904),\n",
       " ((u'.\"', u'talk'), 0.0002994011976047904),\n",
       " ((u'.)', u'made'), 0.0002994011976047904),\n",
       " ((u'...)', u'provide'), 0.0002994011976047904),\n",
       " ((u'...).', u'modeling'), 0.0002994011976047904),\n",
       " ((u'10', u'decision'), 0.0002994011976047904),\n",
       " ((u'10', u'k'), 0.0002994011976047904),\n",
       " ((u'10', u'q'), 0.0002994011976047904),\n",
       " ((u'10', u'trillion'), 0.0002994011976047904),\n",
       " ((u'100', u'years'), 0.0002994011976047904),\n",
       " ((u'18pflop', u'computational'), 0.0002994011976047904),\n",
       " ((u'20widgets', u'index'), 0.0002994011976047904),\n",
       " ((u'3', u'expand'), 0.0002994011976047904),\n",
       " ((u'3', u'since'), 0.0002994011976047904),\n",
       " ((u'4', u'help'), 0.0002994011976047904),\n",
       " ((u'401', u'k'), 0.0002994011976047904),\n",
       " ((u'50', u'80'), 0.0002994011976047904),\n",
       " ((u'8', u'satellite'), 0.0002994011976047904),\n",
       " ((u'80', u'time'), 0.0002994011976047904),\n",
       " ((u'?!', u'stripe'), 0.0002994011976047904),\n",
       " ((u'@', u'home'), 0.0002994011976047904),\n",
       " ((u'@', u'pydata'), 0.0002994011976047904),\n",
       " ((u']', u']'), 0.0002994011976047904),\n",
       " ((u']', u're'), 0.0002994011976047904),\n",
       " ((u'ability', u'build'), 0.0002994011976047904),\n",
       " ((u'ability', u'run'), 0.0002994011976047904),\n",
       " ((u'ability', u'scale'), 0.0002994011976047904),\n",
       " ((u'ability', u'understand'), 0.0002994011976047904),\n",
       " ((u'able', u'choose'), 0.0002994011976047904),\n",
       " ((u'able', u'compare'), 0.0002994011976047904),\n",
       " ((u'able', u'data'), 0.0002994011976047904),\n",
       " ((u'able', u'understand'), 0.0002994011976047904),\n",
       " ((u'abstractions', u'dynamic'), 0.0002994011976047904),\n",
       " ((u'academic', u'databases'), 0.0002994011976047904),\n",
       " ((u'academic', u'laboratories'), 0.0002994011976047904),\n",
       " ((u'access', u'rich'), 0.0002994011976047904),\n",
       " ((u'accessible', u'python'), 0.0002994011976047904),\n",
       " ((u'accessing', u'genomic'), 0.0002994011976047904),\n",
       " ((u'accommodate', u'product'), 0.0002994011976047904),\n",
       " ((u'according', u'language'), 0.0002994011976047904),\n",
       " ((u'account', u'holders'), 0.0002994011976047904),\n",
       " ((u'accuracy', u'object'), 0.0002994011976047904),\n",
       " ((u'accurate', u'physical'), 0.0002994011976047904),\n",
       " ((u'accurate', u'utilizing'), 0.0002994011976047904),\n",
       " ((u'achieve', u'bleeding'), 0.0002994011976047904),\n",
       " ((u'action', u'thousands'), 0.0002994011976047904),\n",
       " ((u'active', u'community'), 0.0002994011976047904),\n",
       " ((u'activex', u'excel'), 0.0002994011976047904),\n",
       " ((u'ad', u'hoc'), 0.0002994011976047904),\n",
       " ((u'adapt', u'different'), 0.0002994011976047904),\n",
       " ((u'add', u'feature'), 0.0002994011976047904),\n",
       " ((u'add', u'servers'), 0.0002994011976047904),\n",
       " ((u'adding', u'features'), 0.0002994011976047904),\n",
       " ((u'adding', u'memory'), 0.0002994011976047904),\n",
       " ((u'addition', u'beaker'), 0.0002994011976047904),\n",
       " ((u'addition', u'go'), 0.0002994011976047904),\n",
       " ((u'additionally', u'discuss'), 0.0002994011976047904),\n",
       " ((u'addresses', u'issues'), 0.0002994011976047904),\n",
       " ((u'admin', u'@'), 0.0002994011976047904),\n",
       " ((u'advanced', u'features'), 0.0002994011976047904),\n",
       " ((u'advanced', u'methods'), 0.0002994011976047904),\n",
       " ((u'advanced', u'sql'), 0.0002994011976047904),\n",
       " ((u'advanced', u'systems'), 0.0002994011976047904),\n",
       " ((u'advanced', u'talk'), 0.0002994011976047904),\n",
       " ((u'advantages', u'disadvantages'), 0.0002994011976047904),\n",
       " ((u'advice', u'oriented'), 0.0002994011976047904),\n",
       " ((u'agencies', u'cdo'), 0.0002994011976047904),\n",
       " ((u'agencies', u'opening'), 0.0002994011976047904),\n",
       " ((u'aggregate', u'financial'), 0.0002994011976047904),\n",
       " ((u'aggregated', u'browsing'), 0.0002994011976047904),\n",
       " ((u'aggregation', u'conceptual'), 0.0002994011976047904),\n",
       " ((u'aid', u'combinatory'), 0.0002994011976047904),\n",
       " ((u'aimed', u'scikit'), 0.0002994011976047904),\n",
       " ((u'al', u'significantly'), 0.0002994011976047904),\n",
       " ((u'algebra', u'major'), 0.0002994011976047904),\n",
       " ((u'algorithm', u'applied'), 0.0002994011976047904),\n",
       " ((u'algorithm', u'caffe'), 0.0002994011976047904),\n",
       " ((u'algorithm', u'developed'), 0.0002994011976047904),\n",
       " ((u'algorithm', u'utilizing'), 0.0002994011976047904),\n",
       " ((u'algorithmic', u'underpinnings'), 0.0002994011976047904),\n",
       " ((u'algorithms', u'feature'), 0.0002994011976047904),\n",
       " ((u'algorithms', u'talk'), 0.0002994011976047904),\n",
       " ((u'algorithms', u'validation'), 0.0002994011976047904),\n",
       " ((u'allocation', u'expensive'), 0.0002994011976047904),\n",
       " ((u'allow', u'one'), 0.0002994011976047904),\n",
       " ((u'allow', u'pick'), 0.0002994011976047904),\n",
       " ((u'allowed', u'make'), 0.0002994011976047904),\n",
       " ((u'allowing', u'developers'), 0.0002994011976047904),\n",
       " ((u'allowing', u'inexperienced'), 0.0002994011976047904),\n",
       " ((u'allowing', u'julia'), 0.0002994011976047904),\n",
       " ((u'allows', u'code'), 0.0002994011976047904),\n",
       " ((u'allows', u'developers'), 0.0002994011976047904),\n",
       " ((u'allows', u'investors'), 0.0002994011976047904),\n",
       " ((u'allows', u'plotting'), 0.0002994011976047904),\n",
       " ((u'allows', u'us'), 0.0002994011976047904),\n",
       " ((u'almost', u'stages'), 0.0002994011976047904),\n",
       " ((u'along', u'languages'), 0.0002994011976047904),\n",
       " ((u'alongside', u'set'), 0.0002994011976047904),\n",
       " ((u'already', u'available'), 0.0002994011976047904),\n",
       " ((u'also', u'bridging'), 0.0002994011976047904),\n",
       " ((u'also', u'discuss'), 0.0002994011976047904),\n",
       " ((u'also', u'exploiting'), 0.0002994011976047904),\n",
       " ((u'also', u'get'), 0.0002994011976047904),\n",
       " ((u'also', u'implemented'), 0.0002994011976047904),\n",
       " ((u'also', u'made'), 0.0002994011976047904),\n",
       " ((u'also', u'perform'), 0.0002994011976047904),\n",
       " ((u'also', u'show'), 0.0002994011976047904),\n",
       " ((u'also', u'supports'), 0.0002994011976047904),\n",
       " ((u'also', u'used'), 0.0002994011976047904),\n",
       " ((u'alternative', u'approach'), 0.0002994011976047904),\n",
       " ((u'alternative', u'c'), 0.0002994011976047904),\n",
       " ((u'alternative', u'cpython'), 0.0002994011976047904),\n",
       " ((u'alternative', u'numpy'), 0.0002994011976047904),\n",
       " ((u'alternative', u'splicing'), 0.0002994011976047904),\n",
       " ((u'although', u'original'), 0.0002994011976047904),\n",
       " ((u'americans', u'10'), 0.0002994011976047904),\n",
       " ((u'among', u'applications'), 0.0002994011976047904),\n",
       " ((u'amount', u'boilerplate'), 0.0002994011976047904),\n",
       " ((u'amounts', u'giving'), 0.0002994011976047904),\n",
       " ((u'amsterdam', u'et'), 0.0002994011976047904),\n",
       " ((u'analyses', u'biological'), 0.0002994011976047904),\n",
       " ((u'analysis', u'--'), 0.0002994011976047904),\n",
       " ((u'analysis', u'clustering'), 0.0002994011976047904),\n",
       " ((u'analysis', u'data'), 0.0002994011976047904),\n",
       " ((u'analysis', u'difficult'), 0.0002994011976047904),\n",
       " ((u'analysis', u'earthquake'), 0.0002994011976047904),\n",
       " ((u'analysis', u'gaussian'), 0.0002994011976047904),\n",
       " ((u'analysis', u'help'), 0.0002994011976047904),\n",
       " ((u'analysis', u'important'), 0.0002994011976047904),\n",
       " ((u'analysis', u'interactive'), 0.0002994011976047904),\n",
       " ((u'analysis', u'pipeline'), 0.0002994011976047904),\n",
       " ((u'analysis', u'predictions'), 0.0002994011976047904),\n",
       " ((u'analysis', u'time'), 0.0002994011976047904),\n",
       " ((u'analysis', u'work'), 0.0002994011976047904),\n",
       " ((u'analyst', u'scientist'), 0.0002994011976047904),\n",
       " ((u'analysts', u'scientists'), 0.0002994011976047904),\n",
       " ((u'analytic', u'processing'), 0.0002994011976047904),\n",
       " ((u'analytical', u'approach'), 0.0002994011976047904),\n",
       " ((u'analytical', u'backends'), 0.0002994011976047904),\n",
       " ((u'analytical', u'data'), 0.0002994011976047904),\n",
       " ((u'analytical', u'results'), 0.0002994011976047904),\n",
       " ((u'analytical', u'work'), 0.0002994011976047904),\n",
       " ((u'analytical', u'workspace'), 0.0002994011976047904),\n",
       " ((u'analytics', u'choosing'), 0.0002994011976047904),\n",
       " ((u'analytics', u'cluster'), 0.0002994011976047904),\n",
       " ((u'analytics', u'custom'), 0.0002994011976047904),\n",
       " ((u'analytics', u'hardware'), 0.0002994011976047904),\n",
       " ((u'analytics', u'session'), 0.0002994011976047904),\n",
       " ((u'analyze', u'clean'), 0.0002994011976047904),\n",
       " ((u'analyze', u'granular'), 0.0002994011976047904),\n",
       " ((u'analyze', u'using'), 0.0002994011976047904),\n",
       " ((u'annual', u'10'), 0.0002994011976047904),\n",
       " ((u'another', u'alternative'), 0.0002994011976047904),\n",
       " ((u'another', u'unique'), 0.0002994011976047904),\n",
       " ((u'answer', u'questions'), 0.0002994011976047904),\n",
       " ((u'answering', u'difficult'), 0.0002994011976047904),\n",
       " ((u'anticipating', u'users'), 0.0002994011976047904),\n",
       " ((u'anyone', u'familiar'), 0.0002994011976047904),\n",
       " ((u'anyone', u'level'), 0.0002994011976047904),\n",
       " ((u'anything', u'data'), 0.0002994011976047904),\n",
       " ((u'api', u'blaze'), 0.0002994011976047904),\n",
       " ((u'apis', u'simply'), 0.0002994011976047904),\n",
       " ((u'app', u'deployment'), 0.0002994011976047904),\n",
       " ((u'app', u'requires'), 0.0002994011976047904),\n",
       " ((u'app', u'visualize'), 0.0002994011976047904),\n",
       " ((u'applicable', u'astronomy'), 0.0002994011976047904),\n",
       " ((u'application', u'framework'), 0.0002994011976047904),\n",
       " ((u'application', u'inputs'), 0.0002994011976047904),\n",
       " ((u'application', u'leveraging'), 0.0002994011976047904),\n",
       " ((u'application', u'platforms'), 0.0002994011976047904),\n",
       " ((u'application', u'understanding'), 0.0002994011976047904),\n",
       " ((u'application', u'uses'), 0.0002994011976047904),\n",
       " ((u'application', u'viewed'), 0.0002994011976047904),\n",
       " ((u'applications', u'along'), 0.0002994011976047904),\n",
       " ((u'applications', u'comparing'), 0.0002994011976047904),\n",
       " ((u'applications', u'gis'), 0.0002994011976047904),\n",
       " ((u'applications', u'ideal'), 0.0002994011976047904),\n",
       " ((u'applications', u'like'), 0.0002994011976047904),\n",
       " ((u'applications', u'often'), 0.0002994011976047904),\n",
       " ((u'applications', u'serve'), 0.0002994011976047904),\n",
       " ((u'applications', u'talk'), 0.0002994011976047904),\n",
       " ((u'applications', u'works'), 0.0002994011976047904),\n",
       " ((u'applied', u'mathematicians'), 0.0002994011976047904),\n",
       " ((u'applied', u'satellite'), 0.0002994011976047904),\n",
       " ((u'applied', u'solving'), 0.0002994011976047904),\n",
       " ((u'apply', u'techniques'), 0.0002994011976047904),\n",
       " ((u'apply', u'workflow'), 0.0002994011976047904),\n",
       " ((u'applying', u'statistical'), 0.0002994011976047904),\n",
       " ((u'approach', u'courses'), 0.0002994011976047904),\n",
       " ((u'approach', u'developed'), 0.0002994011976047904),\n",
       " ((u'approach', u'directly'), 0.0002994011976047904),\n",
       " ((u'approach', u'gis'), 0.0002994011976047904),\n",
       " ((u'approach', u'odds'), 0.0002994011976047904),\n",
       " ((u'approach', u'use'), 0.0002994011976047904),\n",
       " ((u'approaches', u'random'), 0.0002994011976047904),\n",
       " ((u'appropriately', u'apply'), 0.0002994011976047904),\n",
       " ((u'approve', u'curricula'), 0.0002994011976047904),\n",
       " ((u'apps', u'launched'), 0.0002994011976047904),\n",
       " ((u'apps', u'provide'), 0.0002994011976047904),\n",
       " ((u'apps', u'without'), 0.0002994011976047904),\n",
       " ((u'arbitrary', u'functions'), 0.0002994011976047904),\n",
       " ((u'architectural', u'choices'), 0.0002994011976047904),\n",
       " ((u'architecture', u'analytical'), 0.0002994011976047904),\n",
       " ((u'architecture', u'include'), 0.0002994011976047904),\n",
       " ((u'architecture', u'practical'), 0.0002994011976047904),\n",
       " ((u'architecture', u'tim'), 0.0002994011976047904),\n",
       " ((u'areas', u'industry'), 0.0002994011976047904),\n",
       " ((u'areas', u'like'), 0.0002994011976047904),\n",
       " ((u'around', u'geographical'), 0.0002994011976047904),\n",
       " ((u'around', u'world'), 0.0002994011976047904),\n",
       " ((u'arrays', u'enter'), 0.0002994011976047904),\n",
       " ((u'arrays', u'talk'), 0.0002994011976047904),\n",
       " ((u'article', u'new'), 0.0002994011976047904),\n",
       " ((u'assemble', u'query'), 0.0002994011976047904),\n",
       " ((u'assessing', u'competency'), 0.0002994011976047904),\n",
       " ((u'assessment', u'*'), 0.0002994011976047904),\n",
       " ((u'assets', u'mostly'), 0.0002994011976047904),\n",
       " ((u'assumptions', u'break'), 0.0002994011976047904),\n",
       " ((u'astronomy', u'demonstrate'), 0.0002994011976047904),\n",
       " ((u'attempts', u'analysis'), 0.0002994011976047904),\n",
       " ((u'attendees', u'learn'), 0.0002994011976047904),\n",
       " ((u'audience', u'name'), 0.0002994011976047904),\n",
       " ((u'audit', u'\"'), 0.0002994011976047904),\n",
       " ((u'audit', u'even'), 0.0002994011976047904),\n",
       " ((u'authentication', u'authorization'), 0.0002994011976047904),\n",
       " ((u'author', u'illustrate'), 0.0002994011976047904),\n",
       " ((u'authorization', u'fine'), 0.0002994011976047904),\n",
       " ((u'auto', u'generate'), 0.0002994011976047904),\n",
       " ((u'automated', u'biophysical'), 0.0002994011976047904),\n",
       " ((u'automatically', u'detect'), 0.0002994011976047904),\n",
       " ((u'automating', u'scaling'), 0.0002994011976047904),\n",
       " ((u'autotranslation', u'set'), 0.0002994011976047904),\n",
       " ((u'available', u'clustered'), 0.0002994011976047904),\n",
       " ((u'available', u'data'), 0.0002994011976047904),\n",
       " ((u'available', u'datasets'), 0.0002994011976047904),\n",
       " ((u'available', u'github'), 0.0002994011976047904),\n",
       " ((u'available', u'matlab'), 0.0002994011976047904),\n",
       " ((u'available', u'modern'), 0.0002994011976047904),\n",
       " ((u'available', u'portfolios'), 0.0002994011976047904),\n",
       " ((u'available', u'within'), 0.0002994011976047904),\n",
       " ((u'back', u'client'), 0.0002994011976047904),\n",
       " ((u'back', u'project'), 0.0002994011976047904),\n",
       " ((u'back', u'stdout'), 0.0002994011976047904),\n",
       " ((u'back', u've'), 0.0002994011976047904),\n",
       " ((u'backend', u'applications'), 0.0002994011976047904),\n",
       " ((u'backend', u'using'), 0.0002994011976047904),\n",
       " ((u'backends', u'authentication'), 0.0002994011976047904),\n",
       " ((u'backends', u'sql'), 0.0002994011976047904),\n",
       " ((u'background', u'),'), 0.0002994011976047904),\n",
       " ((u'badly', u'labeled'), 0.0002994011976047904),\n",
       " ((u'bank', u'understand'), 0.0002994011976047904),\n",
       " ((u'banks', u'although'), 0.0002994011976047904),\n",
       " ((u'based', u'data'), 0.0002994011976047904),\n",
       " ((u'based', u'guessing'), 0.0002994011976047904),\n",
       " ((u'based', u'internal'), 0.0002994011976047904),\n",
       " ((u'based', u'metaphor'), 0.0002994011976047904),\n",
       " ((u'based', u'might'), 0.0002994011976047904),\n",
       " ((u'basic', u'activex'), 0.0002994011976047904),\n",
       " ((u'basic', u'combinations'), 0.0002994011976047904),\n",
       " ((u'basic', u'guidelines'), 0.0002994011976047904),\n",
       " ((u'basis', u'fear'), 0.0002994011976047904),\n",
       " ((u'batch', u'ad'), 0.0002994011976047904),\n",
       " ((u'batch', u'low'), 0.0002994011976047904),\n",
       " ((u'bayesian', u'interpretation'), 0.0002994011976047904),\n",
       " ((u'bayesians', u'frequentists'), 0.0002994011976047904),\n",
       " ((u'beaker', u'action'), 0.0002994011976047904),\n",
       " ((u'beaker', u'also'), 0.0002994011976047904),\n",
       " ((u'beaker', u'comes'), 0.0002994011976047904),\n",
       " ((u'beaker', u'designed'), 0.0002994011976047904),\n",
       " ((u'beaker', u'notebook'), 0.0002994011976047904),\n",
       " ((u'beaker', u'uses'), 0.0002994011976047904),\n",
       " ((u'beat', u'friends'), 0.0002994011976047904),\n",
       " ((u'beautiful', u'data'), 0.0002994011976047904),\n",
       " ((u'beautifulsoup', u'lxml'), 0.0002994011976047904),\n",
       " ((u'becker', u'ever'), 0.0002994011976047904),\n",
       " ((u'beginners', u'scikit'), 0.0002994011976047904),\n",
       " ((u'benefits', u'uses'), 0.0002994011976047904),\n",
       " ((u'best', u'data'), 0.0002994011976047904),\n",
       " ((u'best', u'dining'), 0.0002994011976047904),\n",
       " ((u'best', u'portion'), 0.0002994011976047904),\n",
       " ((u'best', u'scale'), 0.0002994011976047904),\n",
       " ((u'best', u'suited'), 0.0002994011976047904),\n",
       " ((u'better', u'others'), 0.0002994011976047904),\n",
       " ((u'better', u'results'), 0.0002994011976047904),\n",
       " ((u'better', u'suited'), 0.0002994011976047904),\n",
       " ((u'betting', u'spreads'), 0.0002994011976047904),\n",
       " ((u'beyond', u'generates'), 0.0002994011976047904),\n",
       " ((u'beyond', u'internally'), 0.0002994011976047904),\n",
       " ((u'big', u'problems'), 0.0002994011976047904),\n",
       " ((u'big', u'sound'), 0.0002994011976047904),\n",
       " ((u'big', u'statistics'), 0.0002994011976047904),\n",
       " ((u'billions', u'dollars'), 0.0002994011976047904),\n",
       " ((u'binaries', u'data'), 0.0002994011976047904),\n",
       " ((u'bindings', u'binaries'), 0.0002994011976047904),\n",
       " ((u'biological', u'measurements'), 0.0002994011976047904),\n",
       " ((u'biologists', u'rudimentary'), 0.0002994011976047904),\n",
       " ((u'biology', u'constantly'), 0.0002994011976047904),\n",
       " ((u'biomolecular', u'mechanisms'), 0.0002994011976047904),\n",
       " ((u'biophysical', u'experiments'), 0.0002994011976047904),\n",
       " ((u'biophysical', u'simulation'), 0.0002994011976047904),\n",
       " ((u'biophysical', u'simulations'), 0.0002994011976047904),\n",
       " ((u'bit', u'also'), 0.0002994011976047904),\n",
       " ((u'blaze', u'including'), 0.0002994011976047904),\n",
       " ((u'blaze', u'library'), 0.0002994011976047904),\n",
       " ((u'blaze', u'lightweight'), 0.0002994011976047904),\n",
       " ((u'blaze', u'numpy'), 0.0002994011976047904),\n",
       " ((u'blaze', u'provides'), 0.0002994011976047904),\n",
       " ((u'blaze', u'scratch'), 0.0002994011976047904),\n",
       " ((u'blaze', u'thon'), 0.0002994011976047904),\n",
       " ((u'blaze', u'via'), 0.0002994011976047904),\n",
       " ((u'blaze', u'wild'), 0.0002994011976047904),\n",
       " ((u'bleeding', u'edge'), 0.0002994011976047904),\n",
       " ((u'blob', u'master'), 0.0002994011976047904),\n",
       " ((u'block', u'fraudulent'), 0.0002994011976047904),\n",
       " ((u'bof', u'submit'), 0.0002994011976047904),\n",
       " ((u'boiled', u'static'), 0.0002994011976047904),\n",
       " ((u'boilerplate', u'code'), 0.0002994011976047904),\n",
       " ((u'bolts', u'allowing'), 0.0002994011976047904),\n",
       " ((u'bond', u'pandas'), 0.0002994011976047904),\n",
       " ((u'bond', u'parameters'), 0.0002994011976047904),\n",
       " ((u'bond', u'sci'), 0.0002994011976047904),\n",
       " ((u'bond', u'type'), 0.0002994011976047904),\n",
       " ((u'bootstrapping', u'python'), 0.0002994011976047904),\n",
       " ((u'bot', u'using'), 0.0002994011976047904),\n",
       " ((u'box', u'\"'), 0.0002994011976047904),\n",
       " ((u'box', u'features'), 0.0002994011976047904),\n",
       " ((u'bread', u'butter'), 0.0002994011976047904),\n",
       " ((u'break', u'difficulties'), 0.0002994011976047904),\n",
       " ((u'bridging', u'gap'), 0.0002994011976047904),\n",
       " ((u'bring', u'data'), 0.0002994011976047904),\n",
       " ((u'brings', u'ability'), 0.0002994011976047904),\n",
       " ((u'broken', u'sometimes'), 0.0002994011976047904),\n",
       " ((u'brokerages', u'result'), 0.0002994011976047904),\n",
       " ((u'browsing', u'interface'), 0.0002994011976047904),\n",
       " ((u'browsing', u'recent'), 0.0002994011976047904),\n",
       " ((u'bucketing', u'scheme'), 0.0002994011976047904),\n",
       " ((u'build', u'app'), 0.0002994011976047904),\n",
       " ((u'build', u'baseline'), 0.0002994011976047904),\n",
       " ((u'build', u'heterogenous'), 0.0002994011976047904),\n",
       " ((u'build', u'lot'), 0.0002994011976047904),\n",
       " ((u'build', u'simple'), 0.0002994011976047904),\n",
       " ((u'build', u'small'), 0.0002994011976047904),\n",
       " ((u'build', u'spyre'), 0.0002994011976047904),\n",
       " ((u'building', u'monitoring'), 0.0002994011976047904),\n",
       " ((u'building', u'mvps'), 0.0002994011976047904),\n",
       " ((u'building', u'pipeline'), 0.0002994011976047904),\n",
       " ((u'building', u'predictive'), 0.0002994011976047904),\n",
       " ((u'building', u'retirement'), 0.0002994011976047904),\n",
       " ((u'building', u'robust'), 0.0002994011976047904),\n",
       " ((u'building', u'stream'), 0.0002994011976047904),\n",
       " ((u'building', u'visualizations'), 0.0002994011976047904),\n",
       " ((u'building', u'web'), 0.0002994011976047904),\n",
       " ((u'buildpack', u'find'), 0.0002994011976047904),\n",
       " ((u'buildpack', u'uses'), 0.0002994011976047904),\n",
       " ((u'built', u'first'), 0.0002994011976047904),\n",
       " ((u'built', u'high'), 0.0002994011976047904),\n",
       " ((u'built', u'keep'), 0.0002994011976047904),\n",
       " ((u'built', u'support'), 0.0002994011976047904),\n",
       " ((u'business', u'environment'), 0.0002994011976047904),\n",
       " ((u'business', u'reporting'), 0.0002994011976047904),\n",
       " ((u'butter', u'data'), 0.0002994011976047904),\n",
       " ((u'buttons', u'sliders'), 0.0002994011976047904),\n",
       " ((u'buyers', u'ils'), 0.0002994011976047904),\n",
       " ((u'c', u'++.'), 0.0002994011976047904),\n",
       " ((u'c', u'copies'), 0.0002994011976047904),\n",
       " ((u'c', u'gaining'), 0.0002994011976047904),\n",
       " ((u'caffe', u'available'), 0.0002994011976047904),\n",
       " ((u'calculate', u'exceedance'), 0.0002994011976047904),\n",
       " ((u'call', u'julia'), 0.0002994011976047904),\n",
       " ((u'call', u'python'), 0.0002994011976047904),\n",
       " ((u'called', u'autotranslation'), 0.0002994011976047904),\n",
       " ((u'called', u'george'), 0.0002994011976047904),\n",
       " ((u'cancer', u'center'), 0.0002994011976047904),\n",
       " ((u'capability', u'straight'), 0.0002994011976047904),\n",
       " ((u'care', u'setting'), 0.0002994011976047904),\n",
       " ((u'carlo', u'error'), 0.0002994011976047904),\n",
       " ((u'case', u'flotilla'), 0.0002994011976047904),\n",
       " ((u'cases', u'illustrating'), 0.0002994011976047904),\n",
       " ((u'cassandra', u'gained'), 0.0002994011976047904),\n",
       " ((u'cassandra', u'make'), 0.0002994011976047904),\n",
       " ((u'cassandra', u'open'), 0.0002994011976047904),\n",
       " ((u'cassandra', u'windows'), 0.0002994011976047904),\n",
       " ((u'catastrophic', u'bond'), 0.0002994011976047904),\n",
       " ((u'catch', u'errors'), 0.0002994011976047904),\n",
       " ((u'cdo', u'managers'), 0.0002994011976047904),\n",
       " ((u'cell', u'everything'), 0.0002994011976047904),\n",
       " ((u'cell', u'read'), 0.0002994011976047904),\n",
       " ((u'cell', u'rna'), 0.0002994011976047904),\n",
       " ((u'cells', u'multiple'), 0.0002994011976047904),\n",
       " ((u'cells', u'reducing'), 0.0002994011976047904),\n",
       " ((u'cells', u'response'), 0.0002994011976047904),\n",
       " ((u'cells', u'text'), 0.0002994011976047904),\n",
       " ((u'cement', u'insight'), 0.0002994011976047904),\n",
       " ((u'center', u'data'), 0.0002994011976047904),\n",
       " ((u'centric', u'approach'), 0.0002994011976047904),\n",
       " ((u'centric', u'software'), 0.0002994011976047904),\n",
       " ((u'challenge', u'end'), 0.0002994011976047904),\n",
       " ((u'challenge', u'young'), 0.0002994011976047904),\n",
       " ((u'challenges', u'building'), 0.0002994011976047904),\n",
       " ((u'challenging', u'beginners'), 0.0002994011976047904),\n",
       " ((u'change', u'database'), 0.0002994011976047904),\n",
       " ((u'changes', u'widget'), 0.0002994011976047904),\n",
       " ((u'changing', u'new'), 0.0002994011976047904),\n",
       " ((u'check', u'\\u2014'), 0.0002994011976047904),\n",
       " ((u'checking', u'small'), 0.0002994011976047904),\n",
       " ((u'cherrypy', u'handle'), 0.0002994011976047904),\n",
       " ((u'cherrypy', u'production'), 0.0002994011976047904),\n",
       " ((u'choice', u'regardless'), 0.0002994011976047904),\n",
       " ((u'choices', u'involved'), 0.0002994011976047904),\n",
       " ((u'choose', u'best'), 0.0002994011976047904),\n",
       " ((u'choose', u'threshold'), 0.0002994011976047904),\n",
       " ((u'choosing', u'analytics'), 0.0002994011976047904),\n",
       " ((u'choosing', u'hardware'), 0.0002994011976047904),\n",
       " ((u'chosen', u'employers'), 0.0002994011976047904),\n",
       " ((u'chrome', u'detects'), 0.0002994011976047904),\n",
       " ((u'classification', u'optimization'), 0.0002994011976047904),\n",
       " ((u'classification', u'regression'), 0.0002994011976047904),\n",
       " ((u'classification', u'topic'), 0.0002994011976047904),\n",
       " ((u'classifier', u'box'), 0.0002994011976047904),\n",
       " ((u'classifier', u'trained'), 0.0002994011976047904),\n",
       " ((u'classifier', u'using'), 0.0002994011976047904),\n",
       " ((u'classifier', u'worked'), 0.0002994011976047904),\n",
       " ((u'classifiers', u'open'), 0.0002994011976047904),\n",
       " ((u'classify', u'streetnames'), 0.0002994011976047904),\n",
       " ((u'classroom', u'environment'), 0.0002994011976047904),\n",
       " ((u'classroom', u'like'), 0.0002994011976047904),\n",
       " ((u'clean', u'extract'), 0.0002994011976047904),\n",
       " ((u'cleaning', u'exploration'), 0.0002994011976047904),\n",
       " ((u'client', u'side'), 0.0002994011976047904),\n",
       " ((u'client', u'working'), 0.0002994011976047904),\n",
       " ((u'clients', u'investment'), 0.0002994011976047904),\n",
       " ((u'cloud', u'based'), 0.0002994011976047904),\n",
       " ((u'cloud', u'platform'), 0.0002994011976047904),\n",
       " ((u'cluster', u'\"'), 0.0002994011976047904),\n",
       " ((u'cluster', u'analysis'), 0.0002994011976047904),\n",
       " ((u'cluster', u'big'), 0.0002994011976047904),\n",
       " ((u'cluster', u'interacted'), 0.0002994011976047904),\n",
       " ((u'cluster', u'running'), 0.0002994011976047904),\n",
       " ((u'clustered', u'architecture'), 0.0002994011976047904),\n",
       " ((u'clustering', u'classification'), 0.0002994011976047904),\n",
       " ((u'code', u'also'), 0.0002994011976047904),\n",
       " ((u'code', u'available'), 0.0002994011976047904),\n",
       " ((u'code', u'beyond'), 0.0002994011976047904),\n",
       " ((u'code', u'compute'), 0.0002994011976047904),\n",
       " ((u'code', u'finally'), 0.0002994011976047904),\n",
       " ((u'code', u'framework'), 0.0002994011976047904),\n",
       " ((u'code', u'locally'), 0.0002994011976047904),\n",
       " ((u'code', u'run'), 0.0002994011976047904),\n",
       " ((u'code', u'selective'), 0.0002994011976047904),\n",
       " ((u'coded', u'streetmap'), 0.0002994011976047904),\n",
       " ((u'coding', u'fun'), 0.0002994011976047904),\n",
       " ((u'cohort', u'school'), 0.0002994011976047904),\n",
       " ((u'collaboration', u'applied'), 0.0002994011976047904),\n",
       " ((u'collaboration', u'multiple'), 0.0002994011976047904),\n",
       " ((u'collaborative', u'data'), 0.0002994011976047904),\n",
       " ((u'collect', u'data'), 0.0002994011976047904),\n",
       " ((u'collect', u'explore'), 0.0002994011976047904),\n",
       " ((u'collected', u'one'), 0.0002994011976047904),\n",
       " ((u'collecting', u'preparing'), 0.0002994011976047904),\n",
       " ((u'collection', u'data'), 0.0002994011976047904),\n",
       " ((u'collection', u'interactive'), 0.0002994011976047904),\n",
       " ((u'collectively', u'figure'), 0.0002994011976047904),\n",
       " ((u'colour', u'coded'), 0.0002994011976047904),\n",
       " ((u'com', u'jasongrout'), 0.0002994011976047904),\n",
       " ((u'com', u'yeolab'), 0.0002994011976047904),\n",
       " ((u'combination', u'allows'), 0.0002994011976047904),\n",
       " ((u'combination', u'c'), 0.0002994011976047904),\n",
       " ((u'combination', u'mongodb'), 0.0002994011976047904),\n",
       " ((u'combinations', u'two'), 0.0002994011976047904),\n",
       " ((u'combinatory', u'play'), 0.0002994011976047904),\n",
       " ((u'combined', u'classroom'), 0.0002994011976047904),\n",
       " ((u'combines', u'high'), 0.0002994011976047904),\n",
       " ((u'come', u'norming'), 0.0002994011976047904),\n",
       " ((u'comes', u'built'), 0.0002994011976047904),\n",
       " ((u'coming', u'database'), 0.0002994011976047904),\n",
       " ((u'commerce', u'space'), 0.0002994011976047904),\n",
       " ((u'commission', u'sec'), 0.0002994011976047904),\n",
       " ((u'committees', u'approve'), 0.0002994011976047904),\n",
       " ((u'common', u'machine'), 0.0002994011976047904),\n",
       " ((u'common', u'recipes'), 0.0002994011976047904),\n",
       " ((u'common', u'settings'), 0.0002994011976047904),\n",
       " ((u'common', u'story'), 0.0002994011976047904),\n",
       " ((u'communicate', u'one'), 0.0002994011976047904),\n",
       " ((u'community', u'driven'), 0.0002994011976047904),\n",
       " ((u'community', u'extensive'), 0.0002994011976047904),\n",
       " ((u'community', u'help'), 0.0002994011976047904),\n",
       " ((u'community', u'tackling'), 0.0002994011976047904),\n",
       " ((u'companies', u'filing'), 0.0002994011976047904),\n",
       " ((u'company', u'analyze'), 0.0002994011976047904),\n",
       " ((u'comparatively', u'new'), 0.0002994011976047904),\n",
       " ((u'compare', u'equivalent'), 0.0002994011976047904),\n",
       " ((u'compare', u'gp'), 0.0002994011976047904),\n",
       " ((u'compare', u'models'), 0.0002994011976047904),\n",
       " ((u'compared', u'relatively'), 0.0002994011976047904),\n",
       " ((u'comparing', u'examinees'), 0.0002994011976047904),\n",
       " ((u'comparison', u'core'), 0.0002994011976047904),\n",
       " ((u'competency', u'mechanical'), 0.0002994011976047904),\n",
       " ((u'complete', u'approach'), 0.0002994011976047904),\n",
       " ((u'complete', u'correct'), 0.0002994011976047904),\n",
       " ((u'completely', u'free'), 0.0002994011976047904),\n",
       " ((u'complex', u'data'), 0.0002994011976047904),\n",
       " ((u'complicated', u'need'), 0.0002994011976047904),\n",
       " ((u'complicated', u'widget'), 0.0002994011976047904),\n",
       " ((u'complicated', u'widgets'), 0.0002994011976047904),\n",
       " ((u'components', u'specified'), 0.0002994011976047904),\n",
       " ((u'components', u'using'), 0.0002994011976047904),\n",
       " ((u'comprehensions', u'assemble'), 0.0002994011976047904),\n",
       " ((u'comprehensive', u'toolkit'), 0.0002994011976047904),\n",
       " ((u'compustat', u'aggregate'), 0.0002994011976047904),\n",
       " ((u'computational', u'power'), 0.0002994011976047904),\n",
       " ((u'compute', u'gps'), 0.0002994011976047904),\n",
       " ((u'computing', u'combines'), 0.0002994011976047904),\n",
       " ((u'computing', u'interpreting'), 0.0002994011976047904),\n",
       " ((u'computing', u'project'), 0.0002994011976047904),\n",
       " ((u'concepts', u'advanced'), 0.0002994011976047904),\n",
       " ((u'conceptual', u'analytical'), 0.0002994011976047904),\n",
       " ((u'conceptual', u'browsing'), 0.0002994011976047904),\n",
       " ((u'concise', u'list'), 0.0002994011976047904),\n",
       " ((u'conda', u'package'), 0.0002994011976047904),\n",
       " ((u'conda', u'packaging'), 0.0002994011976047904),\n",
       " ((u'connect', u'databases'), 0.0002994011976047904),\n",
       " ((u'connect', u'experience'), 0.0002994011976047904),\n",
       " ((u'connectable', u'also'), 0.0002994011976047904),\n",
       " ((u'connecting', u'pycassa'), 0.0002994011976047904),\n",
       " ((u'connecting', u'reference'), 0.0002994011976047904),\n",
       " ((u'connections', u'enable'), 0.0002994011976047904),\n",
       " ((u'connections', u'novel'), 0.0002994011976047904),\n",
       " ((u'connectivity', u'data'), 0.0002994011976047904),\n",
       " ((u'consistent', u'robust'), 0.0002994011976047904),\n",
       " ((u'consistent', u'way'), 0.0002994011976047904),\n",
       " ((u'consortium', u'---'), 0.0002994011976047904),\n",
       " ((u'constantly', u'adapt'), 0.0002994011976047904),\n",
       " ((u'constantly', u'changing'), 0.0002994011976047904),\n",
       " ((u'constantly', u'evolving'), 0.0002994011976047904),\n",
       " ((u'constraint', u'programming'), 0.0002994011976047904),\n",
       " ((u'construct', u'components'), 0.0002994011976047904),\n",
       " ((u'construct', u'connections'), 0.0002994011976047904),\n",
       " ((u'constructed', u'python'), 0.0002994011976047904),\n",
       " ((u'contain', u'cells'), 0.0002994011976047904),\n",
       " ((u'contained', u'image'), 0.0002994011976047904),\n",
       " ((u'contains', u'whole'), 0.0002994011976047904),\n",
       " ((u'content', u'creating'), 0.0002994011976047904),\n",
       " ((u'content', u'text'), 0.0002994011976047904),\n",
       " ((u'contents', u'talk'), 0.0002994011976047904),\n",
       " ((u'context', u'talk'), 0.0002994011976047904),\n",
       " ((u'controls', u'outputs'), 0.0002994011976047904),\n",
       " ((u'convenient', u'completely'), 0.0002994011976047904),\n",
       " ((u'conventional', u'unconventional'), 0.0002994011976047904),\n",
       " ((u'convolutional', u'neural'), 0.0002994011976047904),\n",
       " ((u'copies', u'data'), 0.0002994011976047904),\n",
       " ((u'core', u'algorithm'), 0.0002994011976047904),\n",
       " ((u'core', u'memory'), 0.0002994011976047904),\n",
       " ((u'core', u'pandas'), 0.0002994011976047904),\n",
       " ((u'correct', u'connectable'), 0.0002994011976047904),\n",
       " ((u'cost', u'tradeoffs'), 0.0002994011976047904),\n",
       " ((u'costs', u'quickly'), 0.0002994011976047904),\n",
       " ((u'could', u'due'), 0.0002994011976047904),\n",
       " ((u'could', u'saved'), 0.0002994011976047904),\n",
       " ((u'courses', u'developed'), 0.0002994011976047904),\n",
       " ((u'covariance', u'analysis'), 0.0002994011976047904),\n",
       " ((u'cover', u'basic'), 0.0002994011976047904),\n",
       " ((u'cover', u'common'), 0.0002994011976047904),\n",
       " ((u'cover', u'data'), 0.0002994011976047904),\n",
       " ((u'cover', u'processor'), 0.0002994011976047904),\n",
       " ((u'coverage', u'number'), 0.0002994011976047904),\n",
       " ((u'covered', u'bond'), 0.0002994011976047904),\n",
       " ((u'covers', u'rapid'), 0.0002994011976047904),\n",
       " ((u'cpython', u'isn'), 0.0002994011976047904),\n",
       " ((u'create', u'financial'), 0.0002994011976047904),\n",
       " ((u'create', u'interact'), 0.0002994011976047904),\n",
       " ((u'create', u'web'), 0.0002994011976047904),\n",
       " ((u'created', u'heroku'), 0.0002994011976047904),\n",
       " ((u'creating', u'beautiful'), 0.0002994011976047904),\n",
       " ((u'creating', u'language'), 0.0002994011976047904),\n",
       " ((u'creating', u'ui'), 0.0002994011976047904),\n",
       " ((u'credit', u'answering'), 0.0002994011976047904),\n",
       " ((u'credit', u'rating'), 0.0002994011976047904),\n",
       " ((u'cropped', u'shown'), 0.0002994011976047904),\n",
       " ((u'cropping', u'identify'), 0.0002994011976047904),\n",
       " ((u'cropping', u'producing'), 0.0002994011976047904),\n",
       " ((u'crucial', u'information'), 0.0002994011976047904),\n",
       " ((u'crunch', u'data'), 0.0002994011976047904),\n",
       " ((u'cry', u'tears'), 0.0002994011976047904),\n",
       " ((u'css', u'javascript'), 0.0002994011976047904),\n",
       " ((u'csv', u'file'), 0.0002994011976047904),\n",
       " ((u'csvs', u'hero'), 0.0002994011976047904),\n",
       " ((u'cubes', u'analytical'), 0.0002994011976047904),\n",
       " ((u'cubes', u'brings'), 0.0002994011976047904),\n",
       " ((u'cubes', u'light'), 0.0002994011976047904),\n",
       " ((u'cuda', u'opencl'), 0.0002994011976047904),\n",
       " ((u'current', u'state'), 0.0002994011976047904),\n",
       " ((u'currently', u'runs'), 0.0002994011976047904),\n",
       " ((u'curricula', u'based'), 0.0002994011976047904),\n",
       " ((u'curriculum', u'like'), 0.0002994011976047904),\n",
       " ((u'curve', u'remember'), 0.0002994011976047904),\n",
       " ((u'custom', u'...)'), 0.0002994011976047904),\n",
       " ((u'custom', u'built'), 0.0002994011976047904),\n",
       " ((u'customers', u'talk'), 0.0002994011976047904),\n",
       " ((u'cutting', u'edge'), 0.0002994011976047904),\n",
       " ((u'cycles', u'modeling'), 0.0002994011976047904),\n",
       " ((u'cython', u'c'), 0.0002994011976047904),\n",
       " ((u'data', u'\"'), 0.0002994011976047904),\n",
       " ((u'data', u'*'), 0.0002994011976047904),\n",
       " ((u'data', u'adding'), 0.0002994011976047904),\n",
       " ((u'data', u'americans'), 0.0002994011976047904),\n",
       " ((u'data', u'analysts'), 0.0002994011976047904),\n",
       " ((u'data', u'apis'), 0.0002994011976047904),\n",
       " ((u'data', u'broken'), 0.0002994011976047904),\n",
       " ((u'data', u'centric'), 0.0002994011976047904),\n",
       " ((u'data', u'cleaning'), 0.0002994011976047904),\n",
       " ((u'data', u'collection'), 0.0002994011976047904),\n",
       " ((u'data', u'complete'), 0.0002994011976047904),\n",
       " ((u'data', u'cry'), 0.0002994011976047904),\n",
       " ((u'data', u'exploration'), 0.0002994011976047904),\n",
       " ((u'data', u'first'), 0.0002994011976047904),\n",
       " ((u'data', u'generate'), 0.0002994011976047904),\n",
       " ((u'data', u'geopandas'), 0.0002994011976047904),\n",
       " ((u'data', u'get'), 0.0002994011976047904),\n",
       " ((u'data', u'go'), 0.0002994011976047904),\n",
       " ((u'data', u'infrastructure'), 0.0002994011976047904),\n",
       " ((u'data', u'm1'), 0.0002994011976047904),\n",
       " ((u'data', u'millions'), 0.0002994011976047904),\n",
       " ((u'data', u'mining'), 0.0002994011976047904),\n",
       " ((u'data', u'modeling'), 0.0002994011976047904),\n",
       " ((u'data', u'mongodb'), 0.0002994011976047904),\n",
       " ((u'data', u'monte'), 0.0002994011976047904),\n",
       " ((u'data', u'nasa'), 0.0002994011976047904),\n",
       " ((u'data', u'numerous'), 0.0002994011976047904),\n",
       " ((u'data', u'numpy'), 0.0002994011976047904),\n",
       " ((u'data', u'objects'), 0.0002994011976047904),\n",
       " ((u'data', u'often'), 0.0002994011976047904),\n",
       " ((u'data', u'openness'), 0.0002994011976047904),\n",
       " ((u'data', u'parameters'), 0.0002994011976047904),\n",
       " ((u'data', u'part'), 0.0002994011976047904),\n",
       " ((u'data', u'persistent'), 0.0002994011976047904),\n",
       " ((u'data', u'pipelines'), 0.0002994011976047904),\n",
       " ((u'data', u'problems'), 0.0002994011976047904),\n",
       " ((u'data', u'properly'), 0.0002994011976047904),\n",
       " ((u'data', u'providers'), 0.0002994011976047904),\n",
       " ((u'data', u'retrieval'), 0.0002994011976047904),\n",
       " ((u'data', u'scikit'), 0.0002994011976047904),\n",
       " ((u'data', u'show'), 0.0002994011976047904),\n",
       " ((u'data', u'storage'), 0.0002994011976047904),\n",
       " ((u'data', u'stores'), 0.0002994011976047904),\n",
       " ((u'data', u'systems'), 0.0002994011976047904),\n",
       " ((u'data', u'team'), 0.0002994011976047904),\n",
       " ((u'data', u'transformation'), 0.0002994011976047904),\n",
       " ((u'data', u'two'), 0.0002994011976047904),\n",
       " ((u'data', u'usgs'), 0.0002994011976047904),\n",
       " ((u'data', u'visualizations'), 0.0002994011976047904),\n",
       " ((u'data', u'warehouse'), 0.0002994011976047904),\n",
       " ((u'data', u'wikipedia'), 0.0002994011976047904),\n",
       " ((u'data', u'world'), 0.0002994011976047904),\n",
       " ((u'database', u'background'), 0.0002994011976047904),\n",
       " ((u'database', u'cassandra'), 0.0002994011976047904),\n",
       " ((u'database', u'schema'), 0.0002994011976047904),\n",
       " ((u'databases', u'applying'), 0.0002994011976047904),\n",
       " ((u'databases', u'compustat'), 0.0002994011976047904),\n",
       " ((u'databases', u'data'), 0.0002994011976047904),\n",
       " ((u'databases', u'first'), 0.0002994011976047904),\n",
       " ((u'dataset', u'forget'), 0.0002994011976047904),\n",
       " ((u'dataset', u'includes'), 0.0002994011976047904),\n",
       " ((u'dataset', u'nasa'), 0.0002994011976047904),\n",
       " ((u'datasets', u'dimensionality'), 0.0002994011976047904),\n",
       " ((u'datasets', u'help'), 0.0002994011976047904),\n",
       " ((u'datasets', u'individual'), 0.0002994011976047904),\n",
       " ((u'datasets', u'previously'), 0.0002994011976047904),\n",
       " ((u'datasets', u'users'), 0.0002994011976047904),\n",
       " ((u'day', u'traditionally'), 0.0002994011976047904),\n",
       " ((u'days', u'front'), 0.0002994011976047904),\n",
       " ((u'decision', u'makers'), 0.0002994011976047904),\n",
       " ((u'decision', u'trees'), 0.0002994011976047904),\n",
       " ((u'default', u'allocation'), 0.0002994011976047904),\n",
       " ((u'define', u'dictionary'), 0.0002994011976047904),\n",
       " ((u'defining', u'\"'), 0.0002994011976047904),\n",
       " ((u'deliver', u'data'), 0.0002994011976047904),\n",
       " ((u'delivering', u'types'), 0.0002994011976047904),\n",
       " ((u'demo', u'beaker'), 0.0002994011976047904),\n",
       " ((u'demonstrate', u'complicated'), 0.0002994011976047904),\n",
       " ((u'demonstrate', u'package'), 0.0002994011976047904),\n",
       " ((u'demonstrate', u'python'), 0.0002994011976047904),\n",
       " ((u'demonstrate', u'typical'), 0.0002994011976047904),\n",
       " ((u'demonstrations', u'monary'), 0.0002994011976047904),\n",
       " ((u'deploy', u'data'), 0.0002994011976047904),\n",
       " ((u'deploy', u'first'), 0.0002994011976047904),\n",
       " ((u'deploy', u'model'), 0.0002994011976047904),\n",
       " ((u'deployment', u'scaling'), 0.0002994011976047904),\n",
       " ((u'depths', u'different'), 0.0002994011976047904),\n",
       " ((u'describe', u'building'), 0.0002994011976047904),\n",
       " ((u'describe', u'tools'), 0.0002994011976047904),\n",
       " ((u'design', u'drugs'), 0.0002994011976047904),\n",
       " ((u'design', u'parametric'), 0.0002994011976047904),\n",
       " ((u'design', u'review'), 0.0002994011976047904),\n",
       " ((u'designed', u'polyglot'), 0.0002994011976047904),\n",
       " ((u'designer', u'front'), 0.0002994011976047904),\n",
       " ((u'designing', u'new'), 0.0002994011976047904),\n",
       " ((u'detect', u'block'), 0.0002994011976047904),\n",
       " ((u'detect', u'outliers'), 0.0002994011976047904),\n",
       " ((u'detecting', u'myriad'), 0.0002994011976047904),\n",
       " ((u'detection', u'case'), 0.0002994011976047904),\n",
       " ((u'detects', u'language'), 0.0002994011976047904),\n",
       " ((u'deux', u'blaze'), 0.0002994011976047904),\n",
       " ((u'developed', u'author'), 0.0002994011976047904),\n",
       " ((u'developed', u'collaboration'), 0.0002994011976047904),\n",
       " ((u'developed', u'nlp'), 0.0002994011976047904),\n",
       " ((u'developed', u'order'), 0.0002994011976047904),\n",
       " ((u'developed', u'scale'), 0.0002994011976047904),\n",
       " ((u'developed', u'using'), 0.0002994011976047904),\n",
       " ((u'developer', u'need'), 0.0002994011976047904),\n",
       " ((u'developers', u'access'), 0.0002994011976047904),\n",
       " ((u'developers', u'construct'), 0.0002994011976047904),\n",
       " ((u'developers', u'might'), 0.0002994011976047904),\n",
       " ((u'developers', u'need'), 0.0002994011976047904),\n",
       " ((u'developers', u'quickly'), 0.0002994011976047904),\n",
       " ((u'developers', u'working'), 0.0002994011976047904),\n",
       " ((u'developing', u'educational'), 0.0002994011976047904),\n",
       " ((u'development', u'path'), 0.0002994011976047904),\n",
       " ((u'development', u'python'), 0.0002994011976047904),\n",
       " ((u'dfm', u'io'), 0.0002994011976047904),\n",
       " ((u'dictionary', u'developer'), 0.0002994011976047904),\n",
       " ((u'dictionary', u'override'), 0.0002994011976047904),\n",
       " ((u'dictionary', u'python'), 0.0002994011976047904),\n",
       " ((u'differ', u'much'), 0.0002994011976047904),\n",
       " ((u'differ', u'similar'), 0.0002994011976047904),\n",
       " ((u'different', u'features'), 0.0002994011976047904),\n",
       " ((u'different', u'input'), 0.0002994011976047904),\n",
       " ((u'different', u'languages'), 0.0002994011976047904),\n",
       " ((u'different', u'maximum'), 0.0002994011976047904),\n",
       " ((u'different', u'options'), 0.0002994011976047904),\n",
       " ((u'different', u'scalings'), 0.0002994011976047904),\n",
       " ((u'different', u'sets'), 0.0002994011976047904),\n",
       " ((u'different', u'upstream'), 0.0002994011976047904),\n",
       " ((u'difficult', u'many'), 0.0002994011976047904),\n",
       " ((u'difficult', u'questions'), 0.0002994011976047904),\n",
       " ((u'difficult', u'traditional'), 0.0002994011976047904),\n",
       " ((u'difficulties', u'computing'), 0.0002994011976047904),\n",
       " ((u'difficulty', u'julia'), 0.0002994011976047904),\n",
       " ((u'digital', u'data'), 0.0002994011976047904),\n",
       " ((u'dimension', u'space'), 0.0002994011976047904),\n",
       " ((u'dining', u'experience'), 0.0002994011976047904),\n",
       " ((u'dining', u'expert'), 0.0002994011976047904),\n",
       " ((u'directly', u'company'), 0.0002994011976047904),\n",
       " ((u'directly', u'employs'), 0.0002994011976047904),\n",
       " ((u'directly', u'mongodb'), 0.0002994011976047904),\n",
       " ((u'disadvantages', u'insurance'), 0.0002994011976047904),\n",
       " ((u'disappointed', u'results'), 0.0002994011976047904),\n",
       " ((u'disaster', u'entails'), 0.0002994011976047904),\n",
       " ((u'disco', u'distributed'), 0.0002994011976047904),\n",
       " ((u'disco', u'ecosystem'), 0.0002994011976047904),\n",
       " ((u'discover', u'earth'), 0.0002994011976047904),\n",
       " ((u'discover', u'trends'), 0.0002994011976047904),\n",
       " ((u'discovered', u'using'), 0.0002994011976047904),\n",
       " ((u'discovery', u'---'), 0.0002994011976047904),\n",
       " ((u'discuss', u'challenges'), 0.0002994011976047904),\n",
       " ((u'discuss', u'changes'), 0.0002994011976047904),\n",
       " ((u'discuss', u'give'), 0.0002994011976047904),\n",
       " ((u'discuss', u'limitations'), 0.0002994011976047904),\n",
       " ((u'discuss', u'plans'), 0.0002994011976047904),\n",
       " ((u'discuss', u'tradeoffs'), 0.0002994011976047904),\n",
       " ((u'discuss', u'typical'), 0.0002994011976047904),\n",
       " ((u'discussed', u'many'), 0.0002994011976047904),\n",
       " ((u'discussion', u'interested'), 0.0002994011976047904),\n",
       " ((u'discussion', u'talk'), 0.0002994011976047904),\n",
       " ((u'disease', u'potential'), 0.0002994011976047904),\n",
       " ((u'disease', u'python'), 0.0002994011976047904),\n",
       " ((u'disk', u'subsystems'), 0.0002994011976047904),\n",
       " ((u'dispatched', u'functions'), 0.0002994011976047904),\n",
       " ((u'displayed', u'obtaining'), 0.0002994011976047904),\n",
       " ((u'distributed', u'computing'), 0.0002994011976047904),\n",
       " ((u'distributed', u'data'), 0.0002994011976047904),\n",
       " ((u'distributed', u'key'), 0.0002994011976047904),\n",
       " ((u'distribution', u'domain'), 0.0002994011976047904),\n",
       " ((u'distributions', u'former'), 0.0002994011976047904),\n",
       " ((u'dna', u'sequencing'), 0.0002994011976047904),\n",
       " ((u'documents', u'numpy'), 0.0002994011976047904),\n",
       " ((u'documents', u'simplified'), 0.0002994011976047904),\n",
       " ((u'documents', u'style'), 0.0002994011976047904),\n",
       " ((u'doesn', u'tools'), 0.0002994011976047904),\n",
       " ((u'dollars', u'unnecessary'), 0.0002994011976047904),\n",
       " ((u'domain', u'specific'), 0.0002994011976047904),\n",
       " ((u'domains', u'show'), 0.0002994011976047904),\n",
       " ((u'draft', u'policies'), 0.0002994011976047904),\n",
       " ((u'drawn', u'engineer'), 0.0002994011976047904),\n",
       " ((u'driven', u'exploration'), 0.0002994011976047904),\n",
       " ((u'driven', u'software'), 0.0002994011976047904),\n",
       " ((u'driven', u'web'), 0.0002994011976047904),\n",
       " ((u'driver', u'mongodb'), 0.0002994011976047904),\n",
       " ((u'driver', u'written'), 0.0002994011976047904),\n",
       " ((u'drivers', u'combination'), 0.0002994011976047904),\n",
       " ((u'drop', u'dna'), 0.0002994011976047904),\n",
       " ((u'drug', u'discovery'), 0.0002994011976047904),\n",
       " ((u'drug', u'resistance'), 0.0002994011976047904),\n",
       " ((u'drugs', u'necessary'), 0.0002994011976047904),\n",
       " ((u'drugs', u'proteins'), 0.0002994011976047904),\n",
       " ((u'due', u'errors'), 0.0002994011976047904),\n",
       " ((u'dynamic', u'interactivity'), 0.0002994011976047904),\n",
       " ((u'earth', u'twin'), 0.0002994011976047904),\n",
       " ((u'earth', u'using'), 0.0002994011976047904),\n",
       " ((u'earthquake', u'data'), 0.0002994011976047904),\n",
       " ((u'earthquake', u'historical'), 0.0002994011976047904),\n",
       " ((u'earthquakes', u'calculate'), 0.0002994011976047904),\n",
       " ((u'easier', u'ever'), 0.0002994011976047904),\n",
       " ((u'easily', u'create'), 0.0002994011976047904),\n",
       " ((u'easily', u'executing'), 0.0002994011976047904),\n",
       " ((u'easily', u'sec'), 0.0002994011976047904),\n",
       " ((u'easily', u'using'), 0.0002994011976047904),\n",
       " ((u'easy', u'call'), 0.0002994011976047904),\n",
       " ((u'easy', u'execute'), 0.0002994011976047904),\n",
       " ((u'easy', u'users'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'beyond'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'blaze'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'making'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'mature'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'numpy'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'offer'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'open'), 0.0002994011976047904),\n",
       " ((u'ecosystem', u'tools'), 0.0002994011976047904),\n",
       " ((u'edge', u'performance'), 0.0002994011976047904),\n",
       " ((u'edge', u'research'), 0.0002994011976047904),\n",
       " ((u'education', u'taken'), 0.0002994011976047904),\n",
       " ((u'education', u'zipfian'), 0.0002994011976047904),\n",
       " ((u'educational', u'assessment'), 0.0002994011976047904),\n",
       " ((u'educational', u'resources'), 0.0002994011976047904),\n",
       " ((u'educational', u'testing'), 0.0002994011976047904),\n",
       " ((u'effective', u'thumbnails'), 0.0002994011976047904),\n",
       " ((u'effectively', u'defining'), 0.0002994011976047904),\n",
       " ((u'effectiveness', u'selective'), 0.0002994011976047904),\n",
       " ((u'effects', u'sampling'), 0.0002994011976047904),\n",
       " ((u'efficient', u'data'), 0.0002994011976047904),\n",
       " ((u'efforts', u'3'), 0.0002994011976047904),\n",
       " ((u'either', u'sql'), 0.0002994011976047904),\n",
       " ((u'elements', u'design'), 0.0002994011976047904),\n",
       " ((u'embody', u'many'), 0.0002994011976047904),\n",
       " ((u'emerge', u'every'), 0.0002994011976047904),\n",
       " ((u'employers', u'brokerages'), 0.0002994011976047904),\n",
       " ((u'employs', u'python'), 0.0002994011976047904),\n",
       " ((u'enable', u'users'), 0.0002994011976047904),\n",
       " ((u'enables', u'biologists'), 0.0002994011976047904),\n",
       " ((u'enables', u'simple'), 0.0002994011976047904),\n",
       " ((u'encapsulating', u'variables'), 0.0002994011976047904),\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
